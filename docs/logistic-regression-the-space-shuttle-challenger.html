<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 7 Logistic Regression: The Space Shuttle Challenger | A Book Chapter Example</title>
<meta name="author" content="Your Name">
<meta name="generator" content="bookdown 0.43 with bs4_book()">
<meta property="og:title" content="Chapter 7 Logistic Regression: The Space Shuttle Challenger | A Book Chapter Example">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 7 Logistic Regression: The Space Shuttle Challenger | A Book Chapter Example">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
          margin-bottom: 0em;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
<meta name="description" content="The best thing about being a statistician is that you get to play in everyone’s backyard. —John Tukey24 There are many investigations where a researcher is interested in developing a regression...">
<meta property="og:description" content="The best thing about being a statistician is that you get to play in everyone’s backyard. —John Tukey24 There are many investigations where a researcher is interested in developing a regression...">
<meta name="twitter:description" content="The best thing about being a statistician is that you get to play in everyone’s backyard. —John Tukey24 There are many investigations where a researcher is interested in developing a regression...">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Book Chapter Example</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> An Introduction to Nonparametric Methods: Schistosomiasis</a></li>
<li><a class="" href="making-connections-the-two-sample-t-test-regression-and-anova.html"><span class="header-section-number">2</span> Making Connections: The Two-Sample t-Test, Regression, and ANOVA</a></li>
<li><a class="" href="multiple-regression-how-much-is-your-car-worth.html"><span class="header-section-number">3</span> Multiple Regression: How Much Is Your Car Worth?</a></li>
<li><a class="" href="the-design-and-analysis-of-factorial-experiments-microwave-popcorn.html"><span class="header-section-number">4</span> The Design and Analysis of Factorial Experiments: Microwave Popcorn</a></li>
<li><a class="" href="footnotes-and-citations.html"><span class="header-section-number">5</span> Footnotes and citations</a></li>
<li><a class="" href="categorical-data-analysis-is-a-tumor-malignant-or-benign.html"><span class="header-section-number">6</span> Categorical Data Analysis: Is a Tumor Malignant or Benign?</a></li>
<li><a class="active" href="logistic-regression-the-space-shuttle-challenger.html"><span class="header-section-number">7</span> Logistic Regression: The Space Shuttle Challenger</a></li>
<li><a class="" href="references.html">References</a></li>
<li><a class="" href="survival-analysis-melting-chocolate-chips.html"><span class="header-section-number">8</span> Survival Analysis: Melting Chocolate Chips</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/rstudio/bookdown-demo">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="logistic-regression-the-space-shuttle-challenger" class="section level1" number="7">
<h1>
<span class="header-section-number">7</span> Logistic Regression: The Space Shuttle Challenger<a class="anchor" aria-label="anchor" href="#logistic-regression-the-space-shuttle-challenger"><i class="fas fa-link"></i></a>
</h1>
<p><span style="float:right;"> <em>The best thing about being a statistician is that you get to play in everyone’s backyard.</em></span><br><span style="float:right;"> —John Tukey<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;D. Leonhardt, “John Tukey, 85, Statistician, Coined the Word ‘Software’,” New York Times Archives on the Web, 7/28/2000, stat.bell-labs.com/who/tukey/nytimes.html. John Tukey (1915–2000) had a formal background in chemistry and mathematics. Conducting data analysis during World War II peaked his interest in statistics, and he became one of the most influential statisticians of the 20th century.&lt;/p&gt;"><sup>24</sup></a></span></p>
<p><br><br></p>
<p>There are many investigations where a researcher is interested in developing a regression model when the response variable is dichotomous (has only two categories). Dichotomous responses can be represented with binary data (data with values of only zero or one). Logistic regression is used to examine the relationship between one or more explanatory variables and a binary response variable.</p>
<p>In this chapter, we will look at several studies, including O-ring failure data provided by the National Aeronautics and Space Administration (NASA) after the space shuttle Challenger disaster, in order to introduce the following logistic regression techniques:</p>
<ul>
<li>Calculating and interpreting the logistic regression model<br>
</li>
<li>Using the Wald statistic and likelihood ratio tests to determine the significance of individual explanatory variables<br>
</li>
<li>Calculating the log-odds function and maximum likelihood estimates<br>
</li>
<li>Conducting goodness-of-fit tests to evaluate model appropriateness<br>
</li>
<li>Assessing regression model performance by looking at a classification table, showing correct and incorrect classification of the response variable<br>
</li>
<li>Extending logistic regression to cases with multiple explanatory variables</li>
</ul>
<div id="investigation-did-temperature-influence-the-likelihood-of-an-o-ring-failure" class="section level2" number="7.1">
<h2>
<span class="header-section-number">7.1</span> <strong>Investigation: Did Temperature Influence the Likelihood of an O-Ring Failure?</strong><a class="anchor" aria-label="anchor" href="#investigation-did-temperature-influence-the-likelihood-of-an-o-ring-failure"><i class="fas fa-link"></i></a>
</h2>
<p>On January 28, 1986, the NASA space shuttle program launched its 25th shuttle flight from Kennedy Space Center in Florida. Seventy-three seconds into the flight, the external fuel tank collapsed and spilled liquid oxygen and hydrogen. These chemicals ignited, destroying the shuttle and killing all seven crew members on board. Reports to President Reagan and videos of the event are available at the Kennedy Space Center website.*</p>
<p>Investigations showed that an O-ring seal in the right solid rocket booster failed to isolate the fuel supply. Figure 7.1 shows the space shuttle Challenger just after ignition with the fuel tank and two 149.16-foot-long solid rocket boosters. Figure 7.2 shows a diagram of a solid rocket booster. Because of its size, the rocket boosters were built and shipped in separate sections. A forward, center and aft field joint connected the sections. Two O-rings (one primary and one secondary), which resemble giant rubber bands 0.28 inch thick but 37 feet in diameter, were used to seal the field joints between each of the sections.</p>
<p>An O-ring seal was used to stop the gases inside the solid rocket booster from escaping. However, the cold outside air temperature caused the O-rings to become brittle and fail to seal properly. Gases at 5800 °F escaped and burned a hole through the side of the rocket booster.</p>
<div class="figure" style="text-align: center">
<img src="docs/Fig7_1Rocket.png" alt="Picture of the space shuttle Challenger just after ignition. Each solid rocket booster had six O-rings, two at each field joint. The O-rings at the right aft field joint failed." width="100%"><p class="caption">
(#fig:fig7.1)Picture of the space shuttle Challenger just after ignition. Each solid rocket booster had six O-rings, two at each field joint. The O-rings at the right aft field joint failed.
</p>
</div>
<p><em>The Report of the Presidential Commission on the Space Shuttle Challenger Accident</em>, also known as the Rogers’ Commission Report, states:</p>
<blockquote>
<p>““O-ring resiliency is directly related to its temperature. . . . A warm O-ring that has been compressed
will return to its original shape much quicker than will a cold O-ring when compression is relieved.
. . . A compressed O-ring at 75 degrees Fahrenheit is five times more responsive in returning to its
uncompressed shape than a cold O-ring at 30 degrees Fahrenheit. . . . At the cold launch temperature
experienced, the O-ring would be very slow in returning to its normal rounded shape. . . . It would
remain in its compressed position in the O-ring channel and not provide a space between itself and the
upstream channel wall. Thus, it is probable the O-ring would not . . . seal the gap in time to preclude
joint failure due to blow-by and erosion from hot combustion gases. . . . Of 21 launches with ambient
temperatures of 61 degrees Fahrenheit or greater, only four showed signs of O-ring thermal distress:
i.e., erosion or blow-by and soot. Each of the launches below 61 degrees Fahrenheit resulted in one
or more O-rings showing signs of thermal distress.”<span class="math inline">\(^2\)</span></p>
</blockquote>
<div class="figure" style="text-align: center">
<img src="docs/Fig7_2Rocket2.png" alt="Diargam of a solid rocket booster." width="100%"><p class="caption">
(#fig:fig7.2)Diargam of a solid rocket booster.
</p>
</div>
<p>A lamentable aspect of this disaster is that the problem with the O-rings was already understood by
some engineers prior to the Challenger launch. In February 1984, the Marshall Configuration Control
Board sent a memo about the O-ring erosion that occurred on STS 41-B (the 10th space shuttle flight and
the 4th mission for the Challenger shuttle). Messages continued to increase in intensity, as evidenced by
a 1985 internal memo from Thiokol Corporation, the company that designed the O-ring. Employees from
Thiokol wrote the following to their Vice President of Engineering: “This letter is written to ensure that
management is fully aware of the seriousness of the current O-Ring erosion problem in the SRM joints
from an engineering standpoint.”<span class="math inline">\(^3\)</span></p>
<p>With the temperature on January 28, 1986, expected to be 31°F, Thiokol Corporation recommended
against the Challenger launch. However, this flight was getting significant publicity because a high school
teacher, Christa McAuliffe, was on the flight. The flight had already been delayed several times, and there
was no quick solution to the O-ring concern. The engineers were overruled, and the decision was made to go
ahead with the launch. The eventual presidential investigation stated,
&gt;“The decision to launch the Challenger was flawed. Those who made that decision were unaware of
the recent history of problems concerning the O-rings and the joint and were unaware of the initial
written recommendation of the contractor advising against the launch at temperatures below 53
degrees Fahrenheit and the continuing opposition of the engineers at Thiokol after the management
reversed its position. They did not have a clear understanding of Rockwell’s concern that it was not
safe to launch because of ice on the pad. If the decision makers had known all of the facts, it is highly
unlikely that they would have decided to launch 51-L on January 28, 1986.”<span class="math inline">\(^4\)</span></p>
<p>It seems that even though some engineers did comprehend the severity of the problem, they were unable
to properly communicate the results. Prior to the ill-fated Challenger flight, the solid rocket boosters for 24
shuttle launches were recovered and inspected for damage. Even though O-ring damage was present in some
of the flights, the O-rings were not damaged enough to allow any gas to escape. Since damage was very
minimal, all 24 prior flights were considered a success by NASA.</p>
<ul>
<li>Flight 4 is a missing data point because the rockets were lost at sea.</li>
</ul>
<p>Table 7.1 shows the temperature at the time of each launch and whether any damage was visible in any of the O‑rings. In this chapter, we will define a successful launch as one with no evidence of any O‑ring damage. In Table 7.1, <strong>Successful Launch</strong> is a categorical variable, with 0 representing a launch where O‑ring damage occurred and 1 indicating a successful launch with no O‑ring damage. Throughout the rest of this investigation, the relatively small data set in Table 7.1 will be used to demonstrate techniques that can be used to determine if the likelihood of O‑ring damage is related to temperature.</p>
</div>
<div id="activity-describing-the-data-1" class="section level2 unnumbered">
<h2>Activity: Describing the Data<a class="anchor" aria-label="anchor" href="#activity-describing-the-data-1"><i class="fas fa-link"></i></a>
</h2>
<blockquote>
<ol style="list-style-type: decimal">
<li>Based on the description of the Challenger disaster O‑ring concerns, identify which variable in the space shuttle data set in Table 7.1 should be the explanatory variable and which should be the response variable.<br>
</li>
<li>Imagine you were an engineer working for Thiokol Corporation prior to January 1986. Create a few graphs of the data in Table 7.1. Is it obvious that temperature is related to the success of the O‑rings? Submit any charts or graphs you have created that show a potential relationship between temperature and O‑ring damage.</li>
</ol>
</blockquote>
<p>In this chapter, we will develop a regression model using a binary response variable, Successful Launch. For
the space shuttle data set, y = 1 represents a successful flight with no O-ring damage and y = 0 represents a
flight with some O-ring damage. Binary response data occur in many fields; for example, we may want to know</p>
<ul>
<li>whether a disease is present or absent</li>
<li>whether or not a person is a good credit risk for a loan</li>
<li>whether or not a high school student should be admitted to a particular college</li>
<li>whether or not an individual is involved in substance abuse</li>
</ul>
<p>The next section describes why the least squares regression model is not appropriate when the response is
binary. Logistic regression is used to examine the relationship between one or more explanatory variables
and a binary response variable. Like other regression models, logistic regression models often have explana-
tory variables that are quantitative, but they can be categorical as well</p>
</div>
<div id="review-of-the-least-squares-regression-model" class="section level2" number="7.2">
<h2>
<span class="header-section-number">7.2</span> <strong>Review of the Least Squares Regression Model</strong><a class="anchor" aria-label="anchor" href="#review-of-the-least-squares-regression-model"><i class="fas fa-link"></i></a>
</h2>
<p>In Chapters 2 and 3, you saw that the ordinary least squares regression model has the form
<span class="math display">\[\begin{align}
y_i &amp;= \beta_0 + \beta_1 x_i + \epsilon_i
\quad\text{for } i = 1, 2, 3, \dots, n
\tag{7.1}
\end{align}\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of observations, <span class="math inline">\(y_i\)</span> is the <span class="math inline">\(i\)</span>th value of a , <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are regression coefficients, <span class="math inline">\(x_i\)</span> is the <span class="math inline">\(i\)</span>th value of the explanatory variable, and <span class="math inline">\(\epsilon_i\)</span> represents normally distributed errors with a constant variance. Equation (7.2) states that the mean response (the expected response at each particular <span class="math inline">\(x_i\)</span>) is equal to the linear predictor <span class="math inline">\(\beta_0 + \beta_1 x_i\)</span> for each observed value <span class="math inline">\(x_i\)</span>:</p>
<p><span class="math display">\[\begin{align}
E(Y_i \mid x_i) &amp;= \beta_0 + \beta_1 x_i
\quad\text{for } i = 1, 2, 3, \dots, n
\tag{7.2}
\end{align}\]</span></p>
<p>where <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are parameters that can be estimated with sample data. In addition to assuming that the regression model has a linear predictor, we assume that the error terms in the least squares regression model are independent and follow the normal distribution with a zero mean and a fixed standard deviation:</p>
<p><span class="math display">\[\begin{align}
\epsilon_i &amp;\overset{\mathrm{iid}}{\sim} N(0, \sigma^2)
\quad\text{for } i = 1, 2, 3, \dots, n
\tag{7.3}
\end{align}\]</span></p>
<p>Equation (7.3) states that each independent and identically distributed error term follows a normal probability
distribution that is centered at zero and has a constant variance.</p>
<p>
When there is only one explanatory variable, as in Equation (7.1), ordinary least squares regression is often called simple linear regression. As shown in Chapter 3, the model is called least squares regression because the line minimizes the sum of the squared residuals (the difference between an observed value and the expected response). Least squares estimates for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> (represented as <span class="math inline">\(b_0 = \hat\beta_0\)</span> and <span class="math inline">\(b_1 = \hat\beta_1\)</span>) can be calculated even when the normality and equal variance assumptions are violated. However, these assumptions about the error terms are needed to conduct hypothesis tests and construct confidence intervals for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.
</p>
</div>
<div id="activity-building-a-least-squares-regression-model" class="section level2 unnumbered">
<h2>Activity: Building a Least Squares Regression Model<a class="anchor" aria-label="anchor" href="#activity-building-a-least-squares-regression-model"><i class="fas fa-link"></i></a>
</h2>
<blockquote>
<ol start="3" style="list-style-type: decimal">
<li>Use the data in Table 7.1 to create a scatterplot with a least squares regression line for the space shuttle data. Calculate the predicted response values <span class="math inline">\(\hat y = b_0 + b_1 x\)</span> when the temperature is 60°F and when the temperature is 85°F.</li>
</ol>
</blockquote>
</div>
<div id="the-logistic-regression-model" class="section level2" number="7.3">
<h2>
<span class="header-section-number">7.3</span> <strong>The Logistic Regression Model</strong><a class="anchor" aria-label="anchor" href="#the-logistic-regression-model"><i class="fas fa-link"></i></a>
</h2>
<p>When the response variable is binary, the response is typically defined as a probability of success, instead of 0 or 1. For example, in Question 3, when the temperature is 60°F, the least squares regression line estimates that the probability of a successful launch is 0.338. The expected response at each particular <span class="math inline">\(x_i\)</span> is defined as</p>
<p><span class="math display">\[\begin{align}
\pi_i &amp;= P(Y_i = 1) = \text{probability that a launch has no O-ring damage at temperature } x_i \notag \\
      &amp;= E(Y_i \mid x_i) \notag \\
      &amp;= \beta_0 + \beta_1 x_i \quad \text{for } i = 1,2,3,\dots,n \tag{7.4}
\end{align}\]</span></p>
<p>While the linear model (<span class="math inline">\(\beta_0 + \beta_1 x_i\)</span>) in Equation (7.4) is simple, it is not appropriate to use, since probabilities must be between 0 and 1. For example, with a temperature value <span class="math inline">\(x_i = 50\)</span>, the least squares regression model in Question 3 would predict a probability of <span class="math inline">\(-0.036\)</span>. In order to restrict the predictions to values between 0 and 1, an S-shaped function called the log-odds function will be used.</p>
<p>Logistic regression uses the following model to fit an S-shaped relationship between <span class="math inline">\(\pi\)</span> and <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[\begin{align}
\ln\bigl(\frac{\pi_i}{1 - \pi_i}\bigr) &amp;= \beta_0 + \beta_1 x_i
\tag{7.5}
\end{align}\]</span></p>
<p>where ln represents the natural log, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are regression parameters, and <span class="math inline">\(\pi_i\)</span> is the probability of a successful launch for a given temperature (<span class="math inline">\(x_i\)</span>). The ratio <span class="math inline">\(\pi/(1 - \pi)\)</span> is called the odds, the probability of success over the probability of failure. Thus, the function ln[<span class="math inline">\(\pi/(1 - \pi)\)</span>] is called the log-odds of <span class="math inline">\(\pi\)</span> or the logistic or logit transformation of <span class="math inline">\(\pi\)</span>.* Figure 7.3 shows both the least squares regression model and the logistic regres-
sion model for the space shuttle data.</p>
<p>
In Chapter 6, the odds of an outcome are defined as <span class="math inline">\(\pi/(1 - \pi)\)</span>, the probability of a success (no O-ring damage) over the probability of a failure (O-ring damage). For example, if a computer randomly selects a day of the week, the odds of selecting Saturday (Saturday is considered a success) are 1 to 6, since</p>
<p><span class="math display">\[\begin{align}
\text{odds} = \frac{\pi}{1 - \pi} = \frac{1/7}{(1 - (1/7))} = \frac{1}{6}.
\notag
\end{align}\]</span></p>
<p>Similarly, the odds are 6 to 1 against Saturday being selected (any day but Saturday is a success).
</p>
<div class="figure">
<img src="Chap7_files/figure-html/fig7.3-1.png" alt="Figure 7.3 Space shuttle data with a simple linear regression model and a logistic regression model." width="576"><p class="caption">
(#fig:fig7.3)Figure 7.3 Space shuttle data with a simple linear regression model and a logistic regression model.
</p>
</div>
<p>*?Throughout this chapter, we will use terms such as or , but we actually use natural logs (ln) in
our calculations.</p>
<p>Equation (7.5) can be solved for <span class="math inline">\(\pi_i\)</span> to show that</p>
<p><span class="math display">\[\begin{align}
\pi_i &amp;= \frac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}} \tag{7.6}
\end{align}\]</span></p>
<p>
Binary logistic regression assumes that for each <span class="math inline">\(x_i\)</span> value, the response variable <span class="math inline">\(Y_i\)</span> follows a Bernoulli distribution (described in the extended activities). This means we assume that (1) each <span class="math inline">\(Y_i\)</span> is independent, (2) each <span class="math inline">\(Y_i\)</span> falls into exactly one of two categories represented by either a zero or a one, and (3) for each <span class="math inline">\(x_i\)</span>, <span class="math inline">\(P(Y_i = 1) = \pi_i\)</span> and <span class="math inline">\(P(Y_i = 0) = 1 - \pi_i\)</span> (more specifically written as <span class="math inline">\(P(Y_i = 1 \mid x_i) = \pi_i\)</span> and <span class="math inline">\(P(Y_i = 0 \mid x_i) = 1 - \pi_i\)</span>). This third assumption states that for any given explanatory variable (a specific temperature value), the probability of success (no O-ring failures) is constant.
</p>
<p>It is possible to use least squares regression techniques to estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> in logistic regression models. However, the assumptions needed for hypothesis tests and confidence intervals using the ordinary least squares regression model are not met. Specifically, even after the log-odds transformation, Figure 7.3 demonstrates that the residuals are not normally distributed and the variability of the residuals depends on the explanatory variable.</p>
<p>Residuals (observed values minus expected values) are used to estimate the error terms. Visual inspection of residual plots is often used to check for normality. Recall from previous work in regression that if the residuals are normally distributed, the scatterplot of the residuals versus the explanatory variable should resemble a randomly scattered oval of points. For example, it should resemble the random scatter you would see if you happened to drop 23 coins (one for each residual value).</p>
<p>In logistic regression, the residuals are <span class="math inline">\(y_i - \hat\pi_i\)</span>. If the observed response <span class="math inline">\(y_i = 0\)</span>, then the residual value is <span class="math inline">\(-\hat\pi_i\)</span>. If the observed response <span class="math inline">\(y_i = 1\)</span>, then the residual value is <span class="math inline">\(1 - \hat\pi_i\)</span>. This leads to the two curves shown in Figure 7.4. When the temperature is low in the space shuttle data (around 55°F, as seen in Figure 7.3), the observed responses tend to be zero and the predicted responses (<span class="math inline">\(\hat\pi_i\)</span>’s) are small positive numbers. Thus, the residual values (<span class="math inline">\(-\hat\pi_i\)</span>’s) are negative and close to zero. When the temperature is high, the observed responses tend to be one, the predicted responses are close to one, and the residual values are positive and close to zero.</p>
<div class="figure">
<img src="Chap7_files/figure-html/fig7.4-1.png" alt="Figure 7.4 A scatterplot of the residuals from the space shuttle logistic regression model and a sample of what a scatterplot of normally distributed residuals might look like." width="576"><p class="caption">
(#fig:fig7.4)Figure 7.4 A scatterplot of the residuals from the space shuttle logistic regression model and a sample of what a scatterplot of normally distributed residuals might look like.
</p>
</div>
<p>
When the response in a regression model is binomial, <span class="math inline">\(\pi_i = P(Y_i = 1)\)</span> is the probability of a success (a launch has no O-ring damage at temperature <span class="math inline">\(x_i\)</span>). In simple linear regression models with a binomial response,</p>
<p><span class="math display">\[\begin{align}
y_i &amp;= \pi_i + \epsilon_i = \beta_0 + \beta_1 x_i + \epsilon_i \quad \text{for } i = 1,2,3,\dots,n \tag{7.7}
\end{align}\]</span></p>
<p>With the logit transformation, logistic regression models with a binomial response have the following form:</p>
<p><span class="math display">\[\begin{align}
y_i &amp;= \pi_i + \epsilon_i = \frac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}} + \epsilon_i \quad \text{for } i = 1,2,3,\dots,n \tag{7.8}
\end{align}\]</span></p>
<p>While the logit transformation results in a nice S-shaped curve, the error terms in Equations (7.7) and (7.8) are not constant and are not normally distributed. Thus, hypothesis tests and confidence intervals cannot be calculated using least squares regression.
</p>
</div>
<div id="the-logistic-regression-model-using-maximum-likelihood-estimates" class="section level2" number="7.4">
<h2>
<span class="header-section-number">7.4</span> <strong>The Logistic Regression Model Using Maximum Likelihood Estimates</strong><a class="anchor" aria-label="anchor" href="#the-logistic-regression-model-using-maximum-likelihood-estimates"><i class="fas fa-link"></i></a>
</h2>
<p>Logistic regression is a special case of what is known as a generalized linear model. Generalized linear models expand linear regression models to cases where the normal assumptions do not hold. All generalized linear models have three components:</p>
<p>Generalized linear models can also be used when the response variable follows other distributions. For example, <span class="math inline">\(y\)</span> may follow a Poisson or gamma distribution. Textbooks on generalized linear models derive link functions for each of these types of response variables. In least squares regression where the response has a normal distribution, as in Equation (7.1), the link function is simply the identity function. In other words, the response needs no transformation in simple linear regression models.</p>
<p>Clearly the logistic regression model in Figure 7.3 is nonlinear. So it may seem somewhat surprising to consider logistic regression as a generalized linear model. The reason we still call this model linear is that the link function, the log-odds transformation, is modeled with a linear predictor, <span class="math inline">\(\beta_0 + \beta_1 x\)</span>.</p>
<p>Instead of using least squares estimates, generalized linear models use the method of maximum likelihood to estimate the coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. The extended activities provide more detail on calculating maximum likelihood estimates in logistic regression. In the space shuttle example, we will simply use a computer software package to find maximum likelihood estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<p> In least squares regression, we often transform the response variable (<span class="math inline">\(y\)</span>) so that the data fit model assumptions. In addition to linearizing data, transforming <span class="math inline">\(y\)</span> impacts the variability and the distribution of the error terms. In generalized linear models, the link function transforms the expected response (<span class="math inline">\(\pi\)</span>) to fit a linear predictor. For those who have had calculus, link functions are also differentiable and invertible.
</p>
</div>
<div id="activity-using-software-to-calculate-maximum-likelihood-estimates" class="section level2 unnumbered">
<h2>Activity: Using Software to Calculate Maximum Likelihood Estimates<a class="anchor" aria-label="anchor" href="#activity-using-software-to-calculate-maximum-likelihood-estimates"><i class="fas fa-link"></i></a>
</h2>
<blockquote>
<ol start="4" style="list-style-type: decimal">
<li>Solve Equation (7.5) for <span class="math inline">\(\pi_i\)</span> to show that Equation (7.6) is true.<br>
</li>
<li>Use Equation (7.6) to create six graphs. In each graph, plot the explanatory variable (<span class="math inline">\(x\)</span>) versus the expected probability of success (<span class="math inline">\(\pi\)</span>) using <span class="math inline">\(\beta_0 = -10\)</span> and <span class="math inline">\(-5\)</span> and <span class="math inline">\(\beta_1 = 0.5\)</span>, <span class="math inline">\(1\)</span>, and <span class="math inline">\(1.5\)</span>. Repeat the process for <span class="math inline">\(\beta_0 = 10\)</span> and <span class="math inline">\(5\)</span> and <span class="math inline">\(\beta_1 = -0.5\)</span>, <span class="math inline">\(-1\)</span>, and <span class="math inline">\(-1.5\)</span>.<br>
</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Do not submit the graphs, but explain the impact of changing <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.<br>
</li>
<li>For all of these graphs, what value of <span class="math inline">\(\pi\)</span> appears to have the steepest slope?<br>
</li>
</ol>
<ol start="6" style="list-style-type: decimal">
<li>Use statistical software to calculate the maximum likelihood estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. Compare the maximum likelihood estimates to the least squares estimates in Question 3.</li>
</ol>
</blockquote>
<p>Figure 7.3 shows a logistic regression model using maximum likelihood estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. Using Equation (7.6) and the maximum likelihood estimates from Question 6, we can estimate the probability that a launch has no O-ring damage at temperature <span class="math inline">\(x_i\)</span>:</p>
<p><span class="math display">\[\begin{align}
\hat\pi_i
&amp;= \frac{e^{b_0 + b_1 x_i}}{1 + e^{b_0 + b_1 x_i}} \\
&amp;= \frac{e^{-15.043 + 0.232 x_i}}{1 + e^{-15.043 + 0.232 x_i}} \\
&amp;\quad \text{for } i = 1,2,3,\dots,n \tag{7.9}
\end{align}\]</span></p>
<p>Notice that <span class="math inline">\(\pi\)</span> in Equation (7.6) has been replaced by <span class="math inline">\(\hat\pi\)</span> in Equation (7.9) because the parameters in the linear regression model (<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>) have been estimated with our sample data; <span class="math inline">\(b_0 = \hat\beta_0 = -15.043\)</span> and <span class="math inline">\(b_1 = \hat\beta_1 = 0.232\)</span>.</p>
</div>
<div id="activity-estimating-the-probability-of-success-with-maximum-likelihood-estimates" class="section level2 unnumbered">
<h2>Activity: Estimating the Probability of Success with Maximum Likelihood Estimates<a class="anchor" aria-label="anchor" href="#activity-estimating-the-probability-of-success-with-maximum-likelihood-estimates"><i class="fas fa-link"></i></a>
</h2>
<blockquote>
<ol start="7" style="list-style-type: decimal">
<li>Use Equation (7.9) to predict the probability that a launch has no O-ring damage when the temperature is 31°F, 50°F, and 75°F.</li>
</ol>
</blockquote>
<p>At this point, it seems reasonable to question why the O-rings were not considered a higher risk at the time of the 1986 Challenger launch. After all, the odds of a successful launch (no O-ring damage) at the expected temperature of 31°F are about 1 to 2555 and the predicted odds change dramatically based on temperature. It is important to recognize that the previous launches did not result in the same disaster as the Challenger launch because the O-rings showed only “minor” damage. This wasn’t enough for gas to escape—only an indicator that the O-rings might not be as resilient as expected.</p>
<p> Estimating a value for a temperature of 31°F is extrapolating beyond our data set. Just as in least squares regression, caution should be used when making predictions outside the range of explanatory variables that are available.
</p>
</div>
<div id="interpreting-the-logistic-regression-model" class="section level2" number="7.5">
<h2>
<span class="header-section-number">7.5</span> <strong>Interpreting the Logistic Regression Model</strong><a class="anchor" aria-label="anchor" href="#interpreting-the-logistic-regression-model"><i class="fas fa-link"></i></a>
</h2>
<p>Interpretation of logistic regression models is often done in terms of the odds of success (odds of a launch with no O-ring damage). When the temperature is 59°F, the odds of a successful launch with no O-ring damage are <span class="math inline">\(\hat\pi/(1 - \hat\pi) = 0.2066/(1 - 0.2066) = 0.2605 \approx 0.25 = 1/4\)</span>. Thus, at 59°F, we state that the odds of a successful launch are about 1 to 4. When the temperature is 60°F, the odds of a successful launch are <span class="math inline">\(\hat\pi/(1 - \hat\pi) = 0.3285 \approx 0.333 \approx 1/3\)</span>. At 60°F, we state that the odds of a successful launch are about 1 to 3.</p>
<p>The slope is not as easy to interpret for a logistic regression model as for a simple linear regression model. While ordinary least squares regression focuses on <span class="math inline">\(\beta_1\)</span>, logistic regression measures the change in the odds of success by the term <span class="math inline">\(e^{b_1}\)</span>, which is called the odds ratio. If we increase <span class="math inline">\(x_i\)</span> by 1 unit in a logistic regression model, the predicted odds that <span class="math inline">\(y = 1\)</span> (i.e., the launch will not have any O-ring damage) will be multiplied by <span class="math inline">\(e^{b_1}\)</span>. For example, when the temperature changes from 59°F to 60°F, the odds increase by a multiplicative factor of <span class="math inline">\(e^{b_1} = e^{0.232} = 1.2613\)</span>. In other words,</p>
<p><span class="math display">\[\begin{align}
\text{odds of success at 59°F} \times e^{b_1}
&amp;= 0.2605(1.2613) \notag \\
&amp;= 0.3285 \notag \\
&amp;= \text{odds of success at 60°F} \notag
\end{align}\]</span></p>
<p>For any temperature value <span class="math inline">\(x_i\)</span>, this relationship can also be stated as</p>
<p><span class="math display">\[\begin{align}
\text{odds ratio} = e^{b_1} &amp;= \frac{\text{odds}(x_i + 1)}{\text{odds}(x_i)} \tag{7.10}
\end{align}\]</span></p>
<p>
Taking the exponent of Equation (7.5), we can write the odds of success as
<span class="math display">\[\begin{align}
\text{odds} = \biggl(\frac{\pi_i}{1 - \pi_i}\biggr) &amp;= e^{\beta_0 + \beta_1 x_i} = e^{\beta_0}(e^{\beta_1})^{x_i} \tag{7.11}
\end{align}\]</span></p>
<p>Thus, as <span class="math inline">\(x_i\)</span> increases by 1,
<span class="math display">\[\begin{align}
e^{\beta_0}(e^{\beta_1})^{x_i + 1} &amp;= e^{\beta_0}(e^{\beta_1})^{x_i}(e^{\beta_1}) \tag{7.12}
\end{align}\]</span></p>
<p>
The slope in a logistic regression model is typically described in terms of the odds ratio <span class="math inline">\(e^{b_1}\)</span>. If we increase <span class="math inline">\(x_i\)</span> by 1 unit, the predicted odds will be multiplied by <span class="math inline">\(e^{b_1}\)</span>. In our example, if the temperature increases by one degree, we increase the odds of a successful launch by <span class="math inline">\(e^{b_1} = e^{0.232} = 1.2613\)</span> times. Similarly, if we decrease <span class="math inline">\(x_i\)</span> by 1 unit, the predicted odds will be multiplied by <span class="math inline">\(e^{-b_1} = 1/e^{b_1}\)</span>.
</p>
</div>
<div id="activity-interpreting-a-logistic-regression-model" class="section level2 unnumbered">
<h2>Activity: Interpreting a Logistic Regression Model<a class="anchor" aria-label="anchor" href="#activity-interpreting-a-logistic-regression-model"><i class="fas fa-link"></i></a>
</h2>
<blockquote>
<ol start="8" style="list-style-type: decimal">
<li>Calculate the odds of a launch with no O-ring damage when the temperature is 60°F and when the temperature is 70°F.<br>
</li>
<li>When <span class="math inline">\(x_i\)</span> increases by 10, state in terms of <span class="math inline">\(e^{b_1}\)</span> how much you would expect the odds to change.<br>
</li>
<li>The difference between the odds of success at 60°F and 59°F is about 0.3285 – 0.2605 = 0.068. Would you expect the difference between the odds at 52°F and 51°F to also be about 0.068? Explain why or why not.<br>
</li>
<li>Create a plot of two logistic regression models. Plot temperature versus the estimated probability using maximum likelihood estimates from Question 6, and plot temperature versus the estimated probability using least squares estimates from Question 3.</li>
</ol>
</blockquote>
<p>Thus far, we have developed a model to estimate the odds of a successful launch with no O-ring failures. However, we have not yet discussed the variability of the estimates or how confident we can be of the results. In the next section, we will discuss two hypothesis tests that can be used to determine if the odds of a successful launch are related to temperature. In other words, can we conclude that the logistic regression coefficient <span class="math inline">\(b_1\)</span> is not equal to zero?</p>
<p>
The probability, the odds, and the log-odds are three closely related calculations. Even though any of
the three could be used to express the concepts of interest, the log-odds are often used to estimate the
coefficients, while interpretation of logistic regression models typically relies on expected probabilities
and odds because they are easier to interpret.
</p>
</div>
<div id="inference-for-the-logistic-regression-model" class="section level2" number="7.6">
<h2>
<span class="header-section-number">7.6</span> <strong>Inference for the Logistic Regression Model</strong><a class="anchor" aria-label="anchor" href="#inference-for-the-logistic-regression-model"><i class="fas fa-link"></i></a>
</h2>
</div>
<div id="assumptions-for-logistic-regression-models" class="section level2 unnumbered">
<h2>Assumptions for Logistic Regression Models<a class="anchor" aria-label="anchor" href="#assumptions-for-logistic-regression-models"><i class="fas fa-link"></i></a>
</h2>
<p>Inference for logistic regression uses statistical theory that is based on limits as the sample size approaches infinity. The techniques, based on what is called asymptotic theory, work well with large sample sizes, but are only approximate with outliers or small sample sizes. It is common for logistic regression models to be developed for data sets of any size, but savvy statisticians will always use caution when interpreting the results for data sets with small sample sizes (such as the space shuttle example).</p>
</div>
<div id="the-wald-statistic" class="section level2 unnumbered">
<h2>The Wald Statistic<a class="anchor" aria-label="anchor" href="#the-wald-statistic"><i class="fas fa-link"></i></a>
</h2>
<p>Wald’s test is often used to test the significance of logistic regression coefficients. Just as in least squares regression, we set up a hypothesis test to determine if there is a relationship between the explanatory and response variables:</p>
<p><span class="math display">\[\begin{align}
H_0: b_1 = 0 \quad \text{vs.}\quad H_a: b_1 \neq 0
\end{align}\]</span></p>
<p>Wald’s test is similar to the one-sample Z-test seen in introductory statistics courses. The Wald statistic is calculated as</p>
<p><span class="math display">\[\begin{align}
Z = \frac{b_1 - 0}{\text{se}(b_1)} = \frac{0.232}{0.108} = 2.14 \tag{7.13}
\end{align}\]</span></p>
<p>
Some texts use a chi-square statistic instead of the Z-statistic given in Equation (7.13). Most probability textbooks explain that the square of the test statistic in Equation (7.13) follows a chi-square distribution with 1 degree of freedom. Both techniques provide identical p-values.
</p>
<p>where <span class="math inline">\(b_1\)</span> is the maximum likelihood estimate of <span class="math inline">\(\beta_1\)</span> and se(<span class="math inline">\(b_1\)</span>) is the standard error of <span class="math inline">\(b_1\)</span>. The maximum likelihood estimate, <span class="math inline">\(b_1\)</span>, is asymptotically normally distributed (i.e., <span class="math inline">\(b_1\)</span> is normally distributed when the sample size is large). Thus, the Z-statistic in Equation (7.13) will follow a standard normal distribution when the null hypothesis is true and the sample size is large. In this model, we see that the estimated slope coefficient <span class="math inline">\(b_1 = 0.232\)</span> has a p-value of <span class="math inline">\(P(|Z| \geq 2.14) = 0.032\)</span>.</p>
<p>Wald confidence intervals can also be created. In logistic regression, the confidence interval is often discussed in terms of the odds ratio. For example, a 95% confidence interval for <span class="math inline">\(\beta_1\)</span> is given as</p>
<p><span class="math display">\[\begin{align}
(e^{b_1 - Z^* \text{se}(b_1)}, e^{b_1 + Z^* \text{se}(b_1)}) = (e^{0.232 - 1.96(0.108)}, e^{0.232 + 1.96(0.108)}) = (e^{0.02}, e^{0.44}) = (1.02, 1.56) \tag{7.14}
\end{align}\]</span></p>
<p>where 1.96 = Z^* represents a value corresponding to a 95% confidence interval for a normal distribution with mean of 0 and standard deviation of 1. When <span class="math inline">\(b_1 = 0\)</span>, and thus the odds ratio <span class="math inline">\(e^{b_1} = 1\)</span>, the odds of success for temperature <span class="math inline">\(x_i\)</span> are the same as the odds of success for any other temperature. Thus, <span class="math inline">\(e^{b_1} = 1\)</span> tells us that there is no association between the explanatory variable and the response.</p>
<p>
When a 95% Wald confidence interval for the odds ratio does not contain 1, we reject the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span> (using an alpha-level of 0.05) and conclude that the odds of success do depend on the explanatory variable <span class="math inline">\(x_i\)</span>. If the interval does contain 1, we fail to reject <span class="math inline">\(H_0: \beta_1 = 0\)</span>.
</p>
<p>The Minitab output in Figure 7.5 shows Wald’s test and the corresponding confidence interval for the
odds ratio. In the space shuttle example, the 95% confidence interval does not include <span class="math inline">\(e^{\beta_1} = 1\)</span>; thus, we can
reject the null hypotheses and conclude that the odds of a successful launch do depend on the temperature.
Even though computer software provided a small p-value and a confidence interval that does not include 1, it
is important to note that there are only 23 observations in this study. While Wald’s test is reasonable with very
large sample sizes, with smaller sample sizes it is known to have a tendency to result in a type II error—failing
to reject the null hypothesis when it should be rejected.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;?Recall that Z in Equation (7.13) is a statistic calculated from the sample data and Z* in Equation (7.11) is called a critical value. Z* represents a value based on a desired level of confidence. Z* is known before the data are collected, while Z is based on the sample data&lt;/p&gt;"><sup>25</sup></a></p>
<p>We can also calculate the odds ratio of a successful launch between 60°F and 70°F. When <span class="math inline">\(x_i\)</span>
increases by 10°F, the odds are multiplied by <span class="math inline">\((e^{b_1})10 = 1.2613^{10} = 10.19\)</span>. Thus, we have approximately
10 times higher odds of a successful launch when the temperature is 70°F than when it is 60°F. A 95%
confidence interval for the odds ratio of a successful launch between 60°F and 70°F can be given by
<span class="math inline">\((1.02^10, 1.56^10) = (1.22, 85.40)\)</span>. This 95% confidence interval has a very wide range; the odds of success
at 70°F could be just slightly larger than the odds at 60°F or 85 times as large as the odds at 60°F. This
large range suggests that our estimate of the odds ratio, 10.19, is highly variable. More data are needed to
better understand the true odds ratio.</p>
</div>
<div id="activity-wald-confidence-intervals-and-hypothesis-tests" class="section level2 unnumbered">
<h2>Activity: Wald Confidence Intervals and Hypothesis Tests<a class="anchor" aria-label="anchor" href="#activity-wald-confidence-intervals-and-hypothesis-tests"><i class="fas fa-link"></i></a>
</h2>
<blockquote>
<ol start="12" style="list-style-type: decimal">
<li>Calculate the odds ratio of a successful launch between 31°F and 60°F. Provide a confidence interval
for this odds ratio and interpret your results.</li>
<li>The coefficients in Equation (7.9) were calculated when a successful launch was given a value of 1. Con-
duct a logistic regression analysis where 1 indicates an O-ring failure and 0 represents a successful launch.</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Explain any relationships between the model shown in Equation (7.9) and this new model.</li>
<li>How did the regression coefficients change?</li>
<li>How did the odds ratio change?</li>
<li>Create a 95% Wald confidence interval for the new odds ratio and interpret the results.</li>
</ol>
</blockquote>
<div class="figure" style="text-align: center">
<img src="docs/Fig7_5Minitab.png" alt="Minitab output for the space shuttle study." width="100%"><p class="caption">
(#fig:fig7.5)Minitab output for the space shuttle study.
</p>
</div>
</div>
<div id="the-likelihood-ratio-test" class="section level2 unnumbered">
<h2>The Likelihood Ratio Test<a class="anchor" aria-label="anchor" href="#the-likelihood-ratio-test"><i class="fas fa-link"></i></a>
</h2>
<p>The <strong>likelihood ratio test (LRT)</strong> is derived by calculating the difference between the adequacy of the full and restricted log-likelihood models. A <strong>full model</strong> (sometimes called an <strong>unrestricted model</strong>) includes all parameters under consideration in the model. In this example, there are only two parameters, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, but the full model could include more parameters if more explanatory variables were in the model. The <strong>restricted model</strong> (also called a <strong>reduced model</strong>) is a model with fewer terms than the full model. In our example, only <span class="math inline">\(\beta_0\)</span> is in the restricted model (no explanatory variables are in this model). When no explanatory variables are in the restricted model, the restricted model is also called a <strong>null model</strong>. If the full model has a significantly better fit (the expected values are closer to the observed values) than the restricted model, we reject the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span> and conclude that <span class="math inline">\(H_a: \beta_1 \neq 0\)</span>.</p>
<p>The log-likelihood (restricted) function, described in the extended activities, is a measure of the fit of the model that includes only the intercept:</p>
<p><span class="math display">\[\begin{align}
\text{Restricted Model:} \quad \pi_i = \frac{e^{\beta_0}}{1 + e^{\beta_0}}
\notag
\end{align}\]</span></p>
<p>For the restricted model, the null hypothesis is true and <span class="math inline">\(\pi_i\)</span> is constant for any x-value. The log-likelihood (full) function measures the fit of the model that includes all of the parameters of interest:</p>
<p><span class="math display">\[\begin{align}
\text{Full Model:} \quad \pi_i = \frac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}}
\notag
\end{align}\]</span></p>
<p>When the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span> is true, it can be shown that</p>
<p><span class="math display">\[\begin{align}
G = 2 \times \text{log-likelihood(full model)} - 2 \times \text{log-likelihood(restricted model)} \sim \chi^2_1
\tag{7.15}
\end{align}\]</span></p>
<p>The G-statistic measures the difference between the fits of the restricted and full models. In essence, we
are measuring how much better the fit is when an explanatory variable (temperature) is added to the logistic
model. If the p-value corresponding to the G-statistic is small, the difference in fits is so large that it is unlikely
to occur by chance, and thus we conclude that Ha: b1 0 (the fit of the full model is significantly better than
that of the restricted model).</p>
<p>Degrees of freedom for the LRT equal the number of parameters in the full model minus the number of
parameters in the restricted model. In our case, this is 2 - 1 = 1. Different software packages will present
this test in slightly different ways. In the Minitab output in Figure 7.5, the log-likelihood of the full model
(-10.158) and the G-statistic (7.952) are provided.</p>
<p>Other statistics packages may not give the G-statistic, but they will give enough information so that
the LRT can be calculated. The R output shown in Figure 7.6 gives the null deviance [K - 2 * log-
likelihood (restricted model)] and the residual deviance [K - 2 * log-likelihood (full model)], where
K is a constant value.</p>
<p>Note that
<span class="math display">\[\begin{align}
G &amp;= 2 \times \text{log-likelihood(full model)} - 2 \times \text{log-likelihood(restricted model)} \notag \\
&amp;= \text{null (restricted model) deviance} - \text{residual (full model) deviance} \notag \\
&amp;= 7.952. \notag
\end{align}\]</span></p>
</div>
<div id="activity-the-likelihood-ratio-test" class="section level2 unnumbered">
<h2>Activity: The Likelihood Ratio Test<a class="anchor" aria-label="anchor" href="#activity-the-likelihood-ratio-test"><i class="fas fa-link"></i></a>
</h2>
<blockquote>
<ol start="14" style="list-style-type: decimal">
<li>Use statistical software to calculate the LRT for the space shuttle data. Submit the p-value and state your conclusions.</li>
</ol>
</blockquote>
<p>The G-statistic is relatively large, indicating that we have some evidence to reject <span class="math inline">\(H_0: beta_1 = 0\)</span> and conclude that temperature is related to the odds of a successful launch with no O-ring damage. When sample sizes are large and there is only one explanatory variable in the model, the p-values for the LRT and Wald’s test will be approximately the same. In the space shuttle example, the LRT and Wald’s test have somewhat different p-values.</p>
<p>The likelihood ratio test is more reliable and is often preferred over Wald’s test for small sample sizes. However, unlike the LRT, Wald’s test can have one-sided alternative hypothesis tests as well as nonzero hypothesized values. It is difficult to determine the actual sample size needed for Wald’s test or the LRT to perform well. Some statisticians suggest a minimum sample size of 100 observations.<span class="math inline">\(^7\)</span> Thus, it is best to label each p-value as approximate when using these tests with a small sample size.</p>
<p>
With a large sample size, Wald’s test and the likelihood ratio test provide accurate tests for <span class="math inline">\(H_0: \beta_1 = 0\)</span> versus <span class="math inline">\(H_a: \beta_1 \neq 0\)</span>. While the LRT test tends to be more reliable with smaller sample sizes, use caution when interpreting the results for data sets with small sample sizes (such as the space shuttle example).
</p>
</div>
<div id="what-can-we-conclude-from-the-space-shuttle-study" class="section level2" number="7.7">
<h2>
<span class="header-section-number">7.7</span> <strong>What Can We Conclude from the Space Shuttle Study?</strong><a class="anchor" aria-label="anchor" href="#what-can-we-conclude-from-the-space-shuttle-study"><i class="fas fa-link"></i></a>
</h2>
<p>The space shuttle example is an observational study, since the launches were not “randomly assigned” to the temperature groups, so we cannot conclude solely from this data set that low temperatures caused O-ring damage. Wald’s test and the likelihood ratio test both provided some evidence that the odds of a successful launch are related to temperature. However, a sample size of 23 is not large enough for us to be confident that the p-values are reliable. The logistic regression model provides some indication that the probability of a successful launch is related to temperature. Other information, such as scientists understanding that cold temperatures cause O-rings to be more brittle, also strengthens the conclusion.</p>
</div>
<div id="logistic-regression-with-multiple-explanatory-variables" class="section level2" number="7.8">
<h2>
<span class="header-section-number">7.8</span> <strong>Logistic Regression with Multiple Explanatory Variables</strong><a class="anchor" aria-label="anchor" href="#logistic-regression-with-multiple-explanatory-variables"><i class="fas fa-link"></i></a>
</h2>
<p>Wolberg and Mangasarian developed a technique to accurately diagnose breast masses using only visual characteristics of the cells within the tumor. A sample is placed on a slide, and characteristics of the cellular nuclei within the tumor, such as size, shape, and texture, are examined under a microscope to determine whether the cancer cells are benign or malignant. Benign tumors are scar tissue or abnormal growths that do not spread and are typically harmless. Malignant (or invasive) cancer cells are cells that can travel, typically through the bloodstream or lymph nodes, and begin to replace normal cells in other parts of the body. If a tumor is malignant, it is essential to remove or destroy all cancerous cells in order to keep them from spreading. If a tumor is benign, surgery is typically not needed and the harmless tumor can remain.</p>
<p>In Chapter 6, we used contingency tables with only two variables, cell shape and type, to better understand how to analyze two categorical variables. This section will describe the process of variable selection in logistic regression, using the radius and the concavity of cell nuclei to estimate the probability that a tumor is malignant. In this data set, radius is actually the average radius (in micrometers, <span class="math inline">\(\mu\)</span>m) of all visible cell nuclei from a slide, but we will refer to this variable simply as the cell radius for the tumor. The concavity of the cell nuclei is an indicator of whether the visible cell nuclei from the sample have the nice round shape of typical healthy cells or whether cells appear to have grown in such a way that the perimeters of the cell nuclei tend to have concave points.</p>
</div>
<div id="extended-activity-estimating-the-probability-of-malignancy-in-cancer-cells" class="section level2 unnumbered">
<h2>Extended Activity: Estimating the Probability of Malignancy in Cancer Cells<a class="anchor" aria-label="anchor" href="#extended-activity-estimating-the-probability-of-malignancy-in-cancer-cells"><i class="fas fa-link"></i></a>
</h2>
<p>Data set: Cancer2</p>
<ol start="15" style="list-style-type: decimal">
<li>Create a logistic regression model using Radius and Concave as explanatory variables to estimate the probability that a mass is malignant.<br>
</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Using Radius as the first explanatory variable, <span class="math inline">\(x_1\)</span>, and Concave as the second explanatory variable, <span class="math inline">\(x_2\)</span>, submit the logistic regression model. In other words, find the coefficients for the model</li>
</ol>
<p><span class="math display">\[\begin{align}
   y_i &amp;= \frac{e^{\beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i}}}{1 + e^{\beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i}}} + \epsilon_i \quad \text{for } i = 1,2,\dots,n
\end{align}\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Submit the likelihood ratio test results, including the log-likelihood (or deviance) values.<br>
</li>
<li>Concave = 0 represents round cells and Concave = 1 represents concave cells. Calculate the event probability when Radius = 4 and the cells are concave. Also calculate the event probability when Radius = 4 and the cells are not concave.</li>
</ol>
<ol start="16" style="list-style-type: decimal">
<li>Create a logistic regression model using only Radius as an explanatory variable to estimate the probability that a mass is malignant.<br>
</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Submit the logistic regression model and the likelihood ratio test results, including the log-likelihood (or deviance) values.<br>
</li>
<li>Calculate the event probability when Radius = 4.</li>
</ol>
<p>When there are multiple explanatory variables in a logistic regression model, such as in the model created in Question 15, the likelihood ratio test compares the full model to the null model, which excludes the radius and concavity terms. Thus, the null hypothesis is that the coefficient corresponding to each of the explanatory variables is zero. In Question 15, the LRT is testing</p>
<p><span class="math display">\[H_0: \beta_1 = \beta_2 = 0 \quad \text{vs.}\quad H_a: \text{at least one of the coefficients is not zero}\]</span></p>
<p>The G-statistic for this hypothesis test is 527.42 with 3 - 1 = 2 degrees of freedom (the number of parameters in the full model minus the number of parameters in the restricted model) and a corresponding p-value &lt; 0.001. Thus, we can reject <span class="math inline">\(H_0\)</span> and conclude that at least one of the explanatory variables is significantly related to the probability that the cells are malignant.</p>
<p>The coefficients in multiple logistic regression models are discussed in terms of the odds of success. Any coefficient (<span class="math inline">\(b_j\)</span>) indicates how the response will change corresponding to the <span class="math inline">\(j\)</span>th explanatory variable, conditional on all other explanatory variables in the model. When the <span class="math inline">\(j\)</span>th explanatory variable is increased by one unit, the odds of success will be multiplied by <span class="math inline">\(e^{b_j}\)</span>. When an explanatory variable (<span class="math inline">\(x_j\)</span>) is binary, as Concave is, <span class="math inline">\(e^{b_j}\)</span> represents the odds ratio between the two groups. However, just as in ordinary least squares regression with multiple explanatory variables, these coefficients are conditional on the other terms in the model.</p>
</div>
<div id="the-drop-in-deviance-test" class="section level2" number="7.9">
<h2>
<span class="header-section-number">7.9</span> <strong>The Drop-in-Deviance Test</strong><a class="anchor" aria-label="anchor" href="#the-drop-in-deviance-test"><i class="fas fa-link"></i></a>
</h2>
<p>Figure 7.7 provides the expected probabilities for the model created in Question 15. The probability of a cell being malignant appears to depend on Radius. In addition, it appears that concavity is an important variable in the model, since concave cells tend to have a higher estimated probability of being malignant. In Chapter 3, variable selection is described as a process of determining which explanatory variables should be included in a regression model. Ideally, we would like the simplest model (i.e., the model with the fewest terms) that best explains the response (i.e., the model that has the smallest residuals). In this example, we want to determine if the model in Question 16 can estimate the probability of malignancy just as accurately as the slightly more complex model in Question 15. If the models have similar abilities to estimate the probability of malignancy, then we will prefer the simpler model.</p>
<p>The logic for determining whether additional terms should be in a logistic regression model is essentially the same as for the LRT discussed in connection with the space shuttle study. The log-likelihood (or deviance) can be used to measure how well any model fits the data. If a full model with two explanatory variables, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, has a much better fit than a restricted model with just one explanatory variable, <span class="math inline">\(x_1\)</span>, we can conclude that <span class="math inline">\(x_2\)</span> is significant and should be included in the model.</p>
<p>If the LRT shows no significant difference between the full and the restricted model, the coefficient of
the second variable, <span class="math inline">\(x_2\)</span>, can be set to zero and we can conclude that including the additional variable in the
model does not improve our ability to estimate the probability of success (in this case, success represents a
malignant cell).</p>
<p>In the cancer study, we can compare the full log-likelihood (or deviance) from the two-term model in
Question 15 to the restricted log-likelihood (or deviance) from the one-term model in Question 16.</p>
<p><span class="math display">\[\begin{align}
G &amp;= 2 \times \text{log-likelihood (full model)} - 2 * \text{log-likelihood (restricted model)} \notag \\
&amp;= 2(-112.008) - 2(-165.005) \notag \\
&amp;= \text{null (restricted model) deviance} - \text{residual (full model) deviance} \notag \\
&amp;= 105.994 \tag{7.16}
\end{align}\]</span></p>
<p>Just as in the LRT described in connection with the shuttle example, degrees of freedom are calculated as the
number of parameters in the full model minus the number of parameters in the restricted model. Thus, we can
test whether <span class="math inline">\(Concave\)</span> (<span class="math inline">\(x_2\)</span>) should be included in the model by finding the <span class="math inline">\(p\)</span>-value, which is the percentage of
the <span class="math inline">\(\chi^2\)</span> distribution with 3 - 2 = 1 degree of freedom that exceeds <span class="math inline">\(G\)</span>. The <span class="math inline">\(p\)</span>-value corresponding to Equation
(7.16) is less than 0.0001. We have strong evidence that the explanatory variable, <span class="math inline">\(Concave\)</span>, is important in
the model when <span class="math inline">\(Radius\)</span> is already included. Thus, the logistic regression model in Question 15 is preferred
over the model in Question 16.</p>
<div class="figure">
<img src="Chap7_files/figure-html/fig7.7-1.png" alt="Figure 7.7 A scatterplot of the observed data and estimated probabilities for both round cells (Concave= 0) and concave cells (Concave= 1)." width="576"><p class="caption">
(#fig:fig7.7)Figure 7.7 A scatterplot of the observed data and estimated probabilities for both round cells (Concave= 0) and concave cells (Concave= 1).
</p>
</div>
<p>When the LRT is used for variable selection, it is often called the <strong>change-in-deviance test</strong> or <strong>drop-in-deviance test</strong>. This test is valid only when the restricted model is nested within the full model. A
restricted model is <strong>nested</strong> in a full model when every explanatory variable in the restricted model is also
in the full model.</p>
<p>
</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p>
</p>
<p>
The drop-in-deviance can also be used to simultaneously test multiple variables. For example, let’s assume
that there were four Shape measurements (round, slightly concave, moderately concave, and severely
concave). These four levels would be used to create three indicator variables. If we are interested in
testing whether <span class="math inline">\(Shape\)</span> is important, we should test all three coefficients simultaneously. The steps in
testing three coefficients simultaneously are identical to the steps listed above except that instead of
just testing for <span class="math inline">\(x_j\)</span>, we are simultaneously testing for <span class="math inline">\(x_2\)</span>, <span class="math inline">\(x_3\)</span>, and <span class="math inline">\(x_4\)</span>. Then the null hypothesis would be
<span class="math inline">\(H0: \beta_2 = \beta_3 = \beta_4 = 0\)</span>, where each of these coefficients corresponds to one <span class="math inline">\(Shape\)</span> indicator variable
in the full model.
</p>
<p>Drop-in-deviance tests are often used in combination with stepwise regression techniques in order to
identify the best model for prediction. In the end-of-chapter exercises, backward elimination procedures
analogous to those used in multiple least squares regression will be used to find an appropriate model. The
procedure starts with all explanatory variables of interest in the model. Variables that do not appear to be
significant (or do not significantly reduce the size of the residuals) are removed in a stepwise process. The
process is continued until all variables in the model are significant (or the model consists of only variables
that are important in reducing the size of the residuals).</p>
<p>Just as in least squares regression, there is often no “best” multivariate logistic regression model. Differ-
ent stepwise procedures, sampling variability, the desired balance between the accuracy and the simplicity
of the model, and choice of terms tested all can influence the final model that is selected. While there should
be careful justification for selecting a final model, caution should be used before stating that any selected
model is “the best.”</p>
<p>
Recall that stepwise techniques are useful for developing models if the goal is to estimate or predict a
response. However, they are not appropriate if the goal of developing a regression model is theory test-
ing. Stepwise procedures involve multiple testing on the same variables. This leads to unreliable p-values
when testing the coefficients of individual explanatory variables. Thus, it is inappropriate to use stepwise
procedures to find a good model and then test each coefficient on your final model to determine if the
individual explanatory variables are significant.
</p>
<p>
Some software packages have programs that will automatically suggest a model for logistic regression
(just as is done for stepwise procedures in least squares regression). While this chapter focuses only on
the drop-in-deviance test, there are other techniques that can be used in variable selection.
<span class="math display">\[\begin{align}
\textbf{Akaike’s Information Criterion (AIC)} &amp;= -2 \times \text{log-likelihood} + 2 \times p \notag \\
\textbf{Bayesian Information Criterion (BIC)} &amp;= -2 \times \text{log-likelihood} + p \times ln(n) \notag
\end{align}\]</span>
where p is the number of estimated parameters (the number of explanatory variables plus 1) and n is the
sample size. Both the AIC and the BIC adjust for the number of parameters in the model and are more
likely to select models with fewer variables than the drop-in-deviance test. Both techniques suggest choos-
ing a model with the smallest AIC or BIC value
</p>
</div>
<div id="measures-of-association" class="section level2" number="7.10">
<h2>
<span class="header-section-number">7.10</span> <strong>Measures of Association</strong><a class="anchor" aria-label="anchor" href="#measures-of-association"><i class="fas fa-link"></i></a>
</h2>
<p>When sample sizes are small, a model may have a strong association (a clear pattern is visible) but not have enough evidence to show that the independent variable is significant. Conversely, if there were thousands of observations in a data set, a hypothesis test might conclude that an independent variable was significant even though there was only a very weak association. Thus, researchers typically report both significance tests and a measure of association when discussing results. While there is no widely accepted equivalent to <span class="math inline">\(R^2\)</span> in logistic regression, this section will describe calculations that can be used to measure the strength of association.</p>
<p>To measure the strength of association in the space shuttle logistic regression model, pair each observed success with every observed failure. In the shuttle example, there are 16 successes and 7 failures; thus, there are <span class="math inline">\(16 \times 7 = 112\)</span> pairs. For each pair, use the logistic regression model to estimate the probability of success for both the observed success and the observed failure. If the observation corresponding to a success has a higher estimated probability, the pair is called a <strong>concordant pair</strong>. If the observation corresponding to a success has a lower estimated probability, the pair is called a <strong>discordant pair</strong>. <strong>Tied pairs</strong> occur when the observed success has the same estimated probability as the observed failure.</p>
<p>If all possible pairs were concordant, then Somers’ <span class="math inline">\(D\)</span> would equal 1. If the model had no predictive power, we would expect half the pairs to be concordant and half to be discordant. This would correspond to Somers’ <span class="math inline">\(D = 0\)</span>. Thus, a value of 0 for Somers’ <span class="math inline">\(D\)</span> (as well as Goodman and Kruskal’s gamma) indicates no effect of the explanatory variable on the response variable.</p>
</div>
<div id="extended-activity-measures-of-association" class="section level2 unnumbered">
<h2>Extended Activity: Measures of Association<a class="anchor" aria-label="anchor" href="#extended-activity-measures-of-association"><i class="fas fa-link"></i></a>
</h2>
<p>Data set: <span class="math inline">\(Shuttle\)</span></p>
<ol start="19" style="list-style-type: decimal">
<li>The first and eleventh launches form a pair, since at 63°F there was an O-ring failure and at 66°F there was a success (no O-ring failure). This is a concordant pair, since the probability of success is higher when there was an observed success. Estimate the probability of success for each temperature.</li>
<li>Calculate the expected probabilities of the first (66°F) and 22nd (75°F) observations. Is this a concordant or discordant pair?</li>
<li>Identify two launches in the space shuttle data that represent a tied pair.</li>
<li>Various statistical software packages tend to provide different measures of association. Use statistical software to calculate the Goodman-Kruskal gamma, Somers’ <span class="math inline">\(D\)</span>, or Kendall’s tau-a for the space shuttle data.</li>
</ol>
<p>Minitab output for the space shuttle data is provided in Figure 7.8. The output shows that 75.9% of the pairs
were concordant, while 19.6% of the pairs were discordant. This provides some evidence of association
between temperature and probability of a successful launch.</p>
<p><span class="math display">\[\begin{align}
\text{Somers’ D} &amp;= (85 - 22)/112 = 0.56 \notag \\
\text{Goodman-Kruskal gamma} &amp;= (85 - 22)/(112 - 5) = 0.59 \notag \\
\text{Kendall’s Tau-a} &amp;= (85 - 22)/(253) = 0.25 \notag \\
\text{where } 253 &amp;= 23 \times 22/2, \text{the total number of pairs} \notag
\end{align}\]</span></p>
<p>Somers’ D and the Goodman-Kruskal gamma are very close to each for other because there are very
few tied pairs. There are no <span class="math inline">\(p\)</span>-values corresponding to these measures, but they are useful for compar-
ing different models with different explanatory variables or comparing models based on different link
functions.</p>
<div class="figure" style="text-align: center">
<img src="docs/Fig7_8MinitabPairs.png" alt="Minitab output showing measures of association for the space shuttle data." width="100%"><p class="caption">
(#fig:fig7.8)Minitab output showing measures of association for the space shuttle data.
</p>
</div>
</div>
<div id="review-of-means-and-variances-of-binary-and-binomial-data" class="section level2" number="7.11">
<h2>
<span class="header-section-number">7.11</span> <strong>Review of Means and Variances of Binary and Binomial Data</strong><a class="anchor" aria-label="anchor" href="#review-of-means-and-variances-of-binary-and-binomial-data"><i class="fas fa-link"></i></a>
</h2>
<p>If you have worked with discrete probability models, you will recognize that binary data follow a Bernoulli distribution if the following conditions are true:</p>
<ul>
<li>Each observation, <span class="math inline">\(y_i\)</span>, is independent.</li>
<li>Each <span class="math inline">\(y_i\)</span> falls into exactly one of two categories represented by either a zero or a one.</li>
<li>The probability of success, <span class="math inline">\(P(Y_i = 1) = \pi_i\)</span>, is constant for each observation.</li>
</ul>
<p>The Bernoulli distribution can be displayed as in Table 7.2.
Table 7.2 is often represented with the following mathematical function to model the Bernoulli distribution:</p>
<p><span class="math display">\[\begin{align}
P(Y = k) &amp;= \pi^k (1 - \pi)^{1-k} \quad \text{for } k = 0, 1 \tag{7.17}
\end{align}\]</span></p>
<p>The expected value (mean) of <span class="math inline">\(y\)</span> is the average outcome:</p>
<p><span class="math display">\[\begin{align}
E(Y) &amp;= 0 \times P(Y = 0) + 1 \times P(Y = 1) = 0 \times (1 - \pi) + 1 \times \pi = \pi \tag{7.18}
\end{align}\]</span></p>
<p>Note that each outcome is not equally likely. Thus, each outcome is weighted by its probability. The variance of <span class="math inline">\(y\)</span> is the average value of the squared deviation of the observed value, <span class="math inline">\(y\)</span>, and the expected value, <span class="math inline">\(\pi\)</span>:</p>
<p><span class="math display">\[\begin{align}
\mathrm{Var}(Y) &amp;= E[(Y - \pi)^2] = (0 - \pi)^2 \times P(Y = 0) + (1 - \pi)^2 \times P(Y = 1) = \pi (1 - \pi) \tag{7.19}
\end{align}\]</span></p>
<p>In the above equations, <span class="math inline">\(\pi\)</span> is the same for every observational unit and the results for each observational unit are independent of each other. However, in regression we focus on the expected value of <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span>, <span class="math inline">\(E(Y \mid x_i)\)</span>. This represents the expectation that the probability of success depends on an explanatory variable. In the space shuttle example, the expected probability of a successful launch will change depending on the temperature. For any given temperature value, <span class="math inline">\(x_i\)</span>, the probability of success is constant, <span class="math inline">\(\pi_i\)</span>, and the observations are independent. Thus, for a particular <span class="math inline">\(x_i\)</span>, the corresponding mean and variance are</p>
<p><span class="math display">\[\begin{align}
E(Y \mid x_i) &amp;= \pi_i \quad \text{and} \quad \mathrm{Var}(Y \mid x_i) = \pi_i (1 - \pi_i)
\tag{7.20}
\end{align}\]</span></p>
<p>Thus, in logistic regression with Bernoulli response variables, the variance of <span class="math inline">\(Y\)</span> will depend on <span class="math inline">\(x\)</span>. This violates
the key assumption of constant variance in least squares regression models.</p>
<p>Logistic regression is also appropriate when the response is a count of the number of successes. A count follows a binomial distribution if the following conditions are true:</p>
<ul>
<li>There are <span class="math inline">\(n_i\)</span> independent observations at a given level of <span class="math inline">\(x\)</span> (<span class="math inline">\(x_i\)</span>).</li>
<li>
<span class="math inline">\(\pi_i = P(Y_i = 1 \mid x_i)\)</span> is the probability of success, and this probability is constant for any given <span class="math inline">\(x_i\)</span> (<span class="math inline">\(0 \le \pi_i \le 1\)</span>).</li>
<li>Each response has only two possible outcomes. However, instead of recording a 0 or a 1 value for each outcome, we typically record <span class="math inline">\(Y\)</span> as the count of successes (or proportion of successes) at a particular <span class="math inline">\(x_i\)</span> value.</li>
</ul>
<p>Many introductory textbooks show that if the data follow a binomial distribution, the probability that there are <span class="math inline">\(k\)</span> successes in <span class="math inline">\(n_i\)</span> independent observations is:</p>
<p><span class="math display">\[\begin{align}
P(Y = k \mid x_i) &amp;= \binom{n_i}{k} \,\pi_i^k (1 - \pi_i)^{n_i - k} \quad \text{for } k = 0, 1, \dots, n_i \tag{7.21}
\end{align}\]</span></p>
<p>where <span class="math inline">\(\displaystyle \binom{n_i}{k} = \frac{n_i!}{k!\,(n_i - k)!}\)</span> is called the binomial coefficient and is read as “<span class="math inline">\(n_i\)</span> choose <span class="math inline">\(k\)</span>.”</p>
<p>Using a technique similar to that for the Bernoulli distribution, it can be shown that the conditional mean (probability of success) and variance of binomial response variables are</p>
<p><span class="math display">\[\begin{align}
E(Y \mid x_i) &amp;= n_i \times \pi_i \quad \text{and} \quad \mathrm{Var}(Y \mid x_i) = n_i \times \pi_i (1 - \pi_i)
\tag{7.22}
\end{align}\]</span></p>
</div>
<div id="extended-activity-understanding-the-binomial-distribution" class="section level2 unnumbered">
<h2>Extended Activity: Understanding the Binomial Distribution<a class="anchor" aria-label="anchor" href="#extended-activity-understanding-the-binomial-distribution"><i class="fas fa-link"></i></a>
</h2>
<ol start="23" style="list-style-type: decimal">
<li><p>Assume that for a particular temperature <span class="math inline">\(x_i = 70^\circ\mathrm{F}\)</span> the true probability of success is <span class="math inline">\(\pi_i = 0.75\)</span>. If there are four launches made at <span class="math inline">\(x_i = 70^\circ\mathrm{F}\)</span>, use Equation (7.21) to find the probability that all four launches are successful, <span class="math inline">\(P(Y_i = 4 \mid x_i = 70)\)</span>. Also find the probability that one of the four launches is successful, <span class="math inline">\(P(Y_i = 1 \mid x_i = 70)\)</span>.</p></li>
<li><p>When there are four observations (<span class="math inline">\(n_i = 4\)</span>) and <span class="math inline">\(\pi_i = 0.75\)</span>, use the basic formula for calculating means<br>
to find<br></p></li>
<li><p>When <span class="math inline">\(n_i = 4\)</span> and <span class="math inline">\(\pi_i = 0.75\)</span>, find <span class="math inline">\(\mathrm{Var}(Y_i \mid x_i)\)</span>.</p></li>
</ol>
</div>
<div id="calculating-logistic-regression-models-for-binomial-counts" class="section level2" number="7.12">
<h2>
<span class="header-section-number">7.12</span> Calculating Logistic Regression Models for Binomial Counts<a class="anchor" aria-label="anchor" href="#calculating-logistic-regression-models-for-binomial-counts"><i class="fas fa-link"></i></a>
</h2>
<p>In the previous examples, <span class="math inline">\(y\)</span> was considered a binary random variable (either 0 or 1). In this example, logistic regression will be used when the response is a count of the number of successes (i.e., the response is binomial). Table 7.3 shows the cancer cell data with the <span class="math inline">\(Radius\)</span> variable grouped into five levels. Clearly grouping is not necessary for logistic regression, but this grouping is done to demonstrate how to conduct logistic regression when the response is binomial.</p>
</div>
<div id="extended-activity-binomial-logistic-regression" class="section level2 unnumbered">
<h2>Extended Activity: Binomial Logistic Regression<a class="anchor" aria-label="anchor" href="#extended-activity-binomial-logistic-regression"><i class="fas fa-link"></i></a>
</h2>
<p>Data set: Table 7.3</p>
<ol start="26" style="list-style-type: decimal">
<li>Create a logistic regression model based on Equation (7.8) to predict the probability of a malignant cell from the grouped radius data in Table 7.3.<br>
</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>What are the maximum likelihood estimates <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>?<br>
</li>
<li>Substitute <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> into Equation (7.9) to estimate the probability of malignancy when the radius is 4.5.<br>
</li>
<li>Use Wald’s test and the <span class="math inline">\(G\)</span>-statistic to determine if this model provides evidence that the probability of malignancy is related to cell radius.</li>
</ol>
<p>Figure 7.9 plots the observed percentages of malignant cells and the corresponding logistic regression model.
Notice that the observed and expected probabilities are fairly close. However, the observed percentage of
malignant cells was higher than expected when the cell radius was 4.5.</p>
<div class="figure">
<img src="Chap7_files/figure-html/fig7.9-1.png" alt="Figure 7.9 A logistic regression model, with $\hat \pi_i$ estimated from Equation (7.8) using maximum likelihood estimates, plotted with the observed probability of malignancy for the grouped data in Table 7.3." width="576"><p class="caption">
(#fig:fig7.9)Figure 7.9 A logistic regression model, with <span class="math inline">\(\hat \pi_i\)</span> estimated from Equation (7.8) using maximum likelihood estimates, plotted with the observed probability of malignancy for the grouped data in Table 7.3.
</p>
</div>
</div>
<div id="calculating-residuals-for-logistic-models-with-binomial-counts" class="section level2" number="7.13">
<h2>
<span class="header-section-number">7.13</span> Calculating Residuals for Logistic Models with Binomial Counts<a class="anchor" aria-label="anchor" href="#calculating-residuals-for-logistic-models-with-binomial-counts"><i class="fas fa-link"></i></a>
</h2>
<p>When the response variable follows a binomial distribution as in Table 7.3, Pearson residuals and
deviance residuals are often calculated. These residuals are calculated to evaluate how well a model
fits the data.</p>
<p>Using our estimate <span class="math inline">\(\hat\pi_i = \frac{e^{b_0 + b_1 x_i}}{1 + e^{b_0 + b_1 x_i}}\)</span> for <span class="math inline">\(\pi_i\)</span>, we find the Pearson residual by taking the number of observed successes (<span class="math inline">\(y_i\)</span>) minus the estimated number of expected successes (<span class="math inline">\(n_i \times \hat\pi_i\)</span>) and then dividing by the estimated standard deviation:</p>
<p><span class="math display">\[\begin{align}
\text{Pearson residual for the $i$th radius value}
= \frac{y_i - n_i \,\hat\pi_i}{\sqrt{n_i \,\hat\pi_i\,(1 - \hat\pi_i)}}
\tag{7.23}
\end{align}\]</span></p>
<p>Since the variance is not constant, each residual is weighted by its own estimated standard deviation.</p>
<p>
We can calculate the Pearson residual using count data because we know that for any individual radius, <span class="math inline">\(x_i\)</span>, the response variable, <span class="math inline">\(y_i\)</span>, follows a binomial distribution with an estimated mean <span class="math inline">\(n_i \times \hat\pi_i\)</span> and variance <span class="math inline">\(n_i \times \hat\pi_i \times (1 - \hat\pi_i)\)</span>.<br></p>
</div>
<div id="extended-activity-evaluating-residuals-in-binomial-logistic-regression" class="section level2 unnumbered">
<h2>Extended Activity: Evaluating Residuals in Binomial Logistic Regression<a class="anchor" aria-label="anchor" href="#extended-activity-evaluating-residuals-in-binomial-logistic-regression"><i class="fas fa-link"></i></a>
</h2>
<p>Data set: Table 7.3</p>
<ol start="27" style="list-style-type: decimal">
<li>Create a logistic regression model to predict the probability of a malignant cell from the grouped radius data in Table 7.3.<br>
</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Use software to calculate the Pearson residuals.<br>
</li>
<li>Create a histogram and/or a normal probability plot of these residuals. Are the residuals normally distributed?<br>
</li>
<li>Create a scatterplot of the Pearson residuals versus radius. How accurate are the estimates when radius is 2.5 and when radius is 4.5?</li>
</ol>
<ol start="28" style="list-style-type: decimal">
<li>For each observation, the deviance residual measures the change in deviance from the full to the null model. Each deviance residual is the square root of the deviance goodness–of–fit statistic for each cell (or distinct radius value). The deviance residual is
<span class="math display">\[\begin{align}
D_i \,\pm\, \sqrt{2 \Bigl[
y_i \ln\bigl(\tfrac{y_i}{n_i\,\hat\pi_i}\bigr)
\;+\;(n_i - y_i)\ln\bigl(\tfrac{n_i - y_i}{n_i - n_i\,\hat\pi_i}\bigr)
\Bigr]}
\tag{7.24}
\end{align}\]</span>
where the sign is positive if the observed <span class="math inline">\(y_i\)</span> is higher than the estimated mean and negative if the
observed <span class="math inline">\(y_i\)</span> is less than the estimated mean. Repeat Question 27 but use the deviance residual instead
of the Pearson residual.</li>
</ol>
</div>
<div id="assessing-the-fit-of-a-logistic-regression-model-with-binomial-counts" class="section level2" number="7.14">
<h2>
<span class="header-section-number">7.14</span> <strong>Assessing the Fit of a Logistic Regression Model with Binomial Counts</strong><a class="anchor" aria-label="anchor" href="#assessing-the-fit-of-a-logistic-regression-model-with-binomial-counts"><i class="fas fa-link"></i></a>
</h2>
<p>A goodness‐of‐fit test measures how well a model fits the observed data. We will discuss three goodness‐of‐fit tests for logistic regression based on the chi‐square distribution. As discussed in Chapter 6, a chi‐square goodness‐of‐fit test is an asymptotic test that measures the accumulated distance between observed values and expected values (i.e., values predicted by our model). In each of the three tests, the null and alternative hypotheses are</p>
<p><span class="math display">\[\begin{align}
H_0:\;&amp;\text{the logistic regression model provides an adequate fit to the data} \notag \\
H_a:\;&amp;\text{the model does not adequately fit the data} \notag
\end{align}\]</span></p>
<p>If the predicted values in the model are relatively close to the observed data values (i.e., the model is a good fit for the data), then the test statistic will be small and the <span class="math inline">\(p\)</span>-value will be large. If the test statistic is large, it suggests “lack of fit”; <span class="math inline">\(H_0\)</span> should be rejected and other models should be tried to better fit the data.</p>
<p><br>
Goodness‐of‐fit tests assess the overall fit of a logistic regression model. If <span class="math inline">\(H_0\)</span> is rejected, the model is not a good fit. Failing to reject <span class="math inline">\(H_0\)</span> does not guarantee that the model is a “best fit” or even a good fit, but rather that we simply don’t have enough evidence to prove that it’s a poor fit.<br></p>
<p>Test 1: The <strong>Pearson chi‐square goodness‐of‐fit test</strong> is the traditional chi‐square goodness‐of‐fit test seen in many introductory statistics courses. Degrees of freedom are calculated as the number of groups (classes) minus the number of parameters being estimated. In our case, groups are classified by median radius; we have five groups and are estimating two parameters (<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>), so there are <span class="math inline">\(5 - 2 = 3\)</span> degrees of freedom.</p>
<p>Test 2: The <strong>deviance goodness‐of‐fit test</strong> (also called the likelihood ratio chi‐square test) is based on the sum of squared deviance residuals. The test statistic follows a chi‐square probability distribution where the degrees of freedom are calculated as the number of groups minus the number of parameters being estimated. The Pearson test statistic and the deviance test statistic tend to be similar. For determining a model, the deviance goodness‐of‐fit test is preferred over the Pearson test.<span class="math inline">\(^9\)</span></p>
<p><br>
The sum of squared Pearson residuals is equal to the Pearson chi‐square test statistic. When there is one explanatory variable, the likelihood ratio test is equivalent to the deviance goodness‐of‐fit test. The residual deviance and the degrees of freedom given in R are identical to those given in the deviance goodness‐of‐fit test statistic in Minitab.<br></p>
<p>Test 3: Hosmer and Lemeshow developed a test similar to the Pearson chi‐square goodness‐of‐fit test. The key distinction is that the groups are formed differently. While the Pearson test uses the <em>explanatory variable</em> to form groups, the <strong>Hosmer‐Lemeshow test</strong> uses the <em>predicted values</em> to sort the data and form groups (the default is 10 groups). In Table 7.3, we predetermined the five groups, so the Pearson and Hosmer‐Lemeshow tests are identical in this example. However, when the explanatory variable is continuous (not grouped), the Hosmer‐Lemeshow test is more reliable than the Pearson chi‐square goodness‐of‐fit test.</p>
</div>
<div id="extended-activity-calculating-residuals-by-hand" class="section level2 unnumbered">
<h2>Extended Activity: Calculating Residuals by Hand<a class="anchor" aria-label="anchor" href="#extended-activity-calculating-residuals-by-hand"><i class="fas fa-link"></i></a>
</h2>
<p>Data set: <span class="math inline">\(Cancercells\)</span></p>
<ol start="29" style="list-style-type: decimal">
<li>Calculate the Pearson chi‐square goodness‐of‐fit test by hand for the Cancercells data.<br>
</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Complete Table 7.4 by using the logistic regression model from Question 26 to calculate the expected (predicted) cell counts.<br>
</li>
<li>Calculate the Pearson chi‐square test statistic:<br><span class="math display">\[\begin{align}
\chi^2 &amp;= \sum \frac{(\text{observed count} - \text{expected count})^2}{\text{expected count}}
\tag{7.25}
\end{align}\]</span>
using the observed count and expected count from Table 7.4. Note that there are 10 observed and 10 expected cells.<br>
</li>
<li>Find the <span class="math inline">\(p\)</span>-value by using software or looking up the statistic in a chi‐square probability distribution table.<br>
</li>
<li>Interpret the results and clearly state how your conclusions are impacted by random sampling and random allocation to treatments in the original study design. Remember that a small <span class="math inline">\(p\)</span>-value provides evidence that the model is <em>not</em> a good fit.</li>
</ol>
<ol start="30" style="list-style-type: decimal">
<li>Calculate the deviance chi‐square goodness‐of‐fit test by hand for the Cancercells data given in Table 7.3. Expected counts, degrees of freedom, and <span class="math inline">\(p\)</span>-values are found the same way as in the Pearson test. The only difference is in the calculation of the test statistic:<br><span class="math display">\[\begin{align}
   \text{Deviance chi-square test statistic} = D^2 = 2 \times \sum \biggl[\text{observed count} \times \ln\bigl(\frac{\text{observed count}}{\text{expected count}}\bigr)\biggr]
   \tag{7.26}
   \end{align}\]</span>
Describe differences (if any) between your conclusions here and those from the Pearson test in Question 29.</li>
</ol>
<p>Figure 7.10 shows the Minitab output for the logistic regression model fit and the three goodness-of-fit tests.
Note that all three goodness-of-fit tests show small p-values, indicating that we can reject <span class="math inline">\(H_0\)</span> and con-
clude that the model is <em>not a good fit</em>. In addition, the cell counts shown in Question 29 reveal that the sample
size is large enough for us to believe the tests are reliable. Thus, other models should be tried. The logistic
regression model shown in Figure 7.10 looks reasonable, but there are many possible explanations as to why
the data are not considered a good fit:</p>
<ul>
<li><p>The groups based on median radius may not be appropriate. For example, radii of 1.51 and 2.49 were
considered part of the same group. We could create more groups with smaller class sizes. Clearly we lose
information whenever data are grouped into categories, so using the original data is likely the best option.</p></li>
<li><p>Additional explanatory variables may need to be included in the model. Just as in ordinary regres-
sion, it may be appropriate to include interaction terms or transformations (such as the square, cube,
or log) of the explanatory variable(s).</p></li>
<li><p>The binomial model may not appropriately model the response variable or a transformation other
than the logit transformation may need to be tried. <em>Notice that the logit model assumes symmetry; the curves in the S are the same shape and symmetric around the midpoint of the data.</em></p></li>
</ul>
<div class="figure" style="text-align: center">
<img src="docs/Fig7_10MinitabLog.png" alt="Minitab output for the Cancercells study" width="100%"><p class="caption">
(#fig:fig7.10)Minitab output for the Cancercells study
</p>
</div>
<ul>
<li>A few outliers may be significantly influencing the results. For example, when the median radius is
4.5, the observed probability is not very close to the expected value. This point greatly contributes to
the large chi-square statistics in Questions 29 and 30.</li>
</ul>
<p>Since chi-square tests are based on asymptotic theory, the p-values are not reliable unless there is a large
enough sample size. A general rule for analyzing a 2 <span class="math inline">\(\times\)</span> 2 table of counts is that the expected count for each
cell must be at least 5. For larger tables, the expected counts for all cells must be at least 1, and the average
of expected cell counts should be greater than 5.</p>
<p>When the data are sparse (the observed, and therefore the expected, cell counts are too small), chi-square
tests may tend to fail to reject the null hypothesis which states that the model is a good fit for the data.<span class="math inline">\(^10\)</span> In other
words, these tests may not have good power for detecting particular types of lack of fit. The Hosmer-Lemeshow
test is designed to correct for this problem (when there are continuous explanatory variables) by grouping
the data. Hosmer and Lemeshow suggest that you have a sample size of at least 400 before using their test.<span class="math inline">\(^11\)</span></p>
</div>
<div id="extended-activity-goodnessoffit-tests-for-continuous-explanatory-variables" class="section level2 unnumbered">
<h2>Extended Activity: Goodness‐of‐Fit Tests for Continuous Explanatory Variables<a class="anchor" aria-label="anchor" href="#extended-activity-goodnessoffit-tests-for-continuous-explanatory-variables"><i class="fas fa-link"></i></a>
</h2>
<p>Data set: <span class="math inline">\(Cancer2\)</span></p>
<ol start="31" style="list-style-type: decimal">
<li>Use computer software to create a logistic regression model to predict the probability of a malignant cell using the continuous variable <span class="math inline">\(Radius\)</span> as the explanatory variable. Conduct the three goodness‐of‐fit tests for this model.</li>
</ol>
<p>The goodness-of-fit tests in Question 31 look very different from those calculated with the grouped data
in Figure 7.10. Remember that for goodness-of-fit tests, the degrees of freedom are calculated as the number of
groups minus the number of parameters being estimated. There are two parameters estimated, and the stated 454
degrees of freedom for the Pearson and deviance tests indicate that 456 groups were formed (based on each distinct
radius value given in the data). Since there are 569 observations in this data set, the Pearson and deviance goodness-
of-fit tests do not meet the sample size requirements for a reliable test (most of the cells have only one observation).
In Question 31, the Pearson and deviance goodness-of-fit tests fail to reject <span class="math inline">\(H_0\)</span>: the logistic regression
model provides an adequate fit to the data. However, the violation of the sample size requirement makes these
tests inappropriate to use.</p>
<p>The Hosmer-Lemeshow test in Question 31 has only 8 degrees of freedom, since the default is to form 10
groups based on the fitted values. To form the groups, statistical software sorts the estimated probabilities and
then attempts to create 10 groups of equal size. The observed and expected values are given in the software
output. Notice that there are still two cells with expected values less than 1. Recent studies have shown that
the Hosmer-Lemshow test is somewhat sensitive to the way groups are formed, and other more specific tests
have been developed.<span class="math inline">\(^12\)</span> However, since the p-value is much bigger than <span class="math inline">\(\alpha = 0.05\)</span>, there does not appear to
be strong evidence that the model is not a good fit.</p>
<p>Hosmer and Lemeshow state that slight violations of the sample size requirements are acceptable.<span class="math inline">\(^13\)</span> They
suggest that if there is a sample size concern, you simply group a few columns. For example, in Question
31, grouping columns 1 and 2 and grouping columns 9 and 10 would satisfy the sample size requirements.</p>
</div>
<div id="extended-activity-goodnessoffit-tests-for-continuous-explanatory-variables-1" class="section level2 unnumbered">
<h2>Extended Activity: Goodness‐of‐Fit Tests for Continuous Explanatory Variables<a class="anchor" aria-label="anchor" href="#extended-activity-goodnessoffit-tests-for-continuous-explanatory-variables-1"><i class="fas fa-link"></i></a>
</h2>
<p>Data set: <span class="math inline">\(Cancer2\)</span></p>
<ol start="32" style="list-style-type: decimal">
<li>Conduct the Hosmer‐Lemeshow goodness‐of‐fit test again, but this time adjust the group size (or number of groups) so that the sample size requirement is not violated. Did the <span class="math inline">\(p\)</span>-value of this new Hosmer‐Lemeshow test change your conclusions?</li>
</ol>
<p><br>
The Pearson, deviance, and Hosmer‐Lemeshow tests assess model fit with chi‐square tests. When one or more explanatory variables are continuous (which is often the case), the deviance and Pearson chi‐square tests are not useful because there are not enough observations in each observed and expected cell (the number of distinct values of the explanatory variable is nearly equal to the number of observations). The Hosmer‐Lemeshow test can be used with continuous explanatory variables because it uses the predicted values to group the data.
</p>
</div>
<div id="diagnostic-plots" class="section level2" number="7.15">
<h2>
<span class="header-section-number">7.15</span> <strong>Diagnostic Plots</strong><a class="anchor" aria-label="anchor" href="#diagnostic-plots"><i class="fas fa-link"></i></a>
</h2>
<p>Just as in least squares regression, residual plots are useful in understanding logistic models. Goodness‐of‐fit tests are useful, but, as stated earlier, they tend to fail to reject the null hypothesis even when the model is not appropriate. When these chi‐square tests fail to conclude that a model is inappropriate, residual plots can be used to verify that the model fit is appropriate. In addition, if the goodness‐of‐fit tests do reject the null hypothesis (conclude that the model is not appropriate), residual plots can help identify where the issues of model fit occur.</p>
<p>Large residual values are useful in identifying observations that are not explained well by the model. In addition to residuals, several scatterplots can be used to identify outliers and influential observations. Before creating these plots, we will need to define a few additional terms.</p>
<p>A <strong>covariate pattern</strong> is a set of all observations with identical explanatory variables. In the space shuttle example, there are four observations where the temperature is 70°F. These four observations form a covariate pattern. In the Cancer2 data set, a covariate pattern is a group of observations with both the same <em>Radius</em> value and the same <em>Concave</em> value.</p>
<p><strong>Delta chi-square</strong> (<span class="math inline">\(\Delta \chi^2\)</span>) is a measure of the change in the Pearson goodness-of-fit statistic (x2) when a
particular observation (or covariate pattern if there is more than one observation with the same explanatory
values) is eliminated. In other words, for a particular xi value, delta chi-square is <span class="math inline">\(\Delta \chi^2_i = \chi^2 \;-\;\chi^2_{(i)}\)</span>, where <span class="math inline">\(\chi^2_{(i)}\)</span> is the Pearson goodness‐of‐fit statistic with the <span class="math inline">\(i\)</span>th observation (or covariate pattern) eliminated.</p>
<p><strong>Delta deviance</strong> (<span class="math inline">\(\Delta D^2\)</span>) is a measure of the change in the deviance goodness-of-fit statistic (D2) when a
particular observation (or covariate pattern) is eliminated. In other words, for a particular <span class="math inline">\(x_i\)</span> value, delta chi-
square is <span class="math inline">\(\Delta D^2_i = D^2 \;-\; D^2_{(i)}\)</span>, where <span class="math inline">\(D^2_{(i)}\)</span> is the deviance goodness‐of‐fit statistic with the <span class="math inline">\(i\)</span>th observation (or covariate pattern) eliminated.</p>
<p><strong>Delta beta</strong> measures the difference in the regression coefficient when a particular observation (or covariate pattern) is removed.</p>
<p>Figure 7.11 shows the delta chi‐square, delta deviance, and delta beta values plotted against the expected probabilities (<span class="math inline">\(\hat\pi_i\)</span>). The determination as to whether or not an observation (covariate pattern) is an outlier or overly influential is somewhat subjective. Outliers typically appear as extreme values in the upper corners of the scatterplot. As a rough estimate, <span class="math inline">\(\Delta\chi^2\)</span> or <span class="math inline">\(\Delta D^2\)</span> greater than 4 and delta beta values greater than 1 may be considered unusual observations. With large sample sizes, <span class="math inline">\(\Delta\chi^2\)</span> and <span class="math inline">\(\Delta D^2\)</span> approximately follow the chi‐square distribution, and the 95th percentile of the chi‐square distribution with 1 degree of freedom equals 3.84. Figure 7.11 has one covariate pattern that has a fairly large <span class="math inline">\(\Delta\chi^2\)</span> and <span class="math inline">\(\Delta D^2\)</span> values. It corresponds to the two launches that occurred at 75°F. Notice in Figure 7.11 that a failure at 75°F appears to be somewhat unusual. In this example, the <span class="math inline">\(\Delta\chi^2\)</span> and <span class="math inline">\(\Delta D^2\)</span> values are not extreme enough to be of major concern (all values are close to or less than 4).</p>
<div class="figure">
<img src="Chap7_files/figure-html/fig7.11-1.png" alt="Figure 7.11 Scatterplots of delta deviance, delta chi-square, and delta beta values versus the expected probabilities from the space shuttle data. Circled values represent launches at 75°F." width="576"><p class="caption">
(#fig:fig7.11)Figure 7.11 Scatterplots of delta deviance, delta chi-square, and delta beta values versus the expected probabilities from the space shuttle data. Circled values represent launches at 75°F.
</p>
</div>
<p>Figure 7.12 shows the delta chi‐square and delta beta values plotted against the <strong>leverage</strong> values. Leverages are values between 0 and 1 that depend only on the explanatory variables (not the response). Large leverage values indicate that the observation (covariate pattern) has extreme explanatory values and may have a large influence on the regression coefficients.</p>
<div class="figure">
<img src="Chap7_files/figure-html/fig7.12-1.png" alt="Figure 7.12 Scatterplots of delta chi-square and delta beta values versus leverage from the space shuttle data. The extreme value along the y-axis represents the launches at 75°F. There do not appear to be any extreme leverage values." width="576"><p class="caption">
(#fig:fig7.12)Figure 7.12 Scatterplots of delta chi-square and delta beta values versus leverage from the space shuttle data. The extreme value along the y-axis represents the launches at 75°F. There do not appear to be any extreme leverage values.
</p>
</div>
<p><br>
The contribution of a single observation depends on both its residual and its leverage. The delta chi‐square and delta deviance values can be used to detect observations that have a strong influence on the goodness‐of‐fit statistics. A large delta beta value indicates a covariate pattern with large leverage and/or large residual values.<br></p>
</div>
<div id="extended-activity-identifying-outliers-and-influential-observations" class="section level2 unnumbered">
<h2>Extended Activity: Identifying Outliers and Influential Observations<a class="anchor" aria-label="anchor" href="#extended-activity-identifying-outliers-and-influential-observations"><i class="fas fa-link"></i></a>
</h2>
<p>Data set: <span class="math inline">\(Cancer2\)</span></p>
<ol start="33" style="list-style-type: decimal">
<li>Run a logistic regression model with both explanatory variables <em>Radius</em> and <em>Concave</em>.<br>
</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Create histograms of the standardized Pearson residuals and deviance residuals.<br>
</li>
<li>Create scatterplots of delta deviance, delta chi‐square, and delta beta (standardized) versus the expected probabilities.<br>
</li>
<li>Create scatterplots of delta deviance, delta chi‐square, and delta beta (standardized) versus the leverage.<br>
</li>
<li>Identify any observations (covariate patterns) that appear to be outliers or influential observations.</li>
</ol>
</div>
<div id="maximum-likelihood-estimation-in-logistic-regression" class="section level2" number="7.16">
<h2>
<span class="header-section-number">7.16</span> <strong>Maximum Likelihood Estimation in Logistic Regression</strong>*<a class="anchor" aria-label="anchor" href="#maximum-likelihood-estimation-in-logistic-regression"><i class="fas fa-link"></i></a>
</h2>
<p>Maximum likelihood estimation is a fairly complex topic. The goal of this section is simply to provide an example of how to calculate maximum likelihood estimates with binomial data. After completing this section, you should have a better understanding of how the LRT and the deviance test are calculated. However, as stated earlier, maximum likelihood estimates are very computationally intensive and are best left to computer algorithms.</p>
<p><br>
When the assumptions about the residuals in the least squares regression model described in Section 7.2 are satisfied, the maximum likelihood estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are identical to the least squares estimates.<br></p>
</div>
<div id="maximum-likelihood-estimator-for-binary-data" class="section level2 unnumbered">
<h2>Maximum Likelihood Estimator for Binary Data<a class="anchor" aria-label="anchor" href="#maximum-likelihood-estimator-for-binary-data"><i class="fas fa-link"></i></a>
</h2>
<p>Consider a set of independent binary responses <span class="math inline">\(y_1, y_2, \dots, y_n\)</span>. Since each observed response is independent and follows the Bernoulli distribution shown in Equation (7.17), the probability of a particular outcome can be found as</p>
<p><span class="math display">\[\begin{align}
P(Y_1 = k_1,\,Y_2 = k_2,\,\dots,\,Y_n = k_n)
&amp;= P(Y_1 = k_1)\,P(Y_2 = k_2)\,\cdots\,P(Y_n = k_n) \notag \\
&amp;= \pi^{k_1}(1 - \pi)^{1-k_1}\,\pi^{k_2}(1 - \pi)^{1-k_2}\,\cdots\,\pi^{k_n}(1 - \pi)^{1-k_n} \notag \\
&amp;= \pi^{\sum_{i=1}^n k_i}\,(1 - \pi)^{\sum_{i=1}^n (1 - k_i)}
\tag{7.27}
\end{align}\]</span></p>
<p>where <span class="math inline">\(k_1, k_2, \dots, k_n\)</span> represent a particular observed series of 0 or 1 outcomes and <span class="math inline">\(\pi\)</span> is a probability, <span class="math inline">\(0 \le \pi \le 1\)</span>. Once <span class="math inline">\(k_1, k_2, \dots, k_n\)</span> have been observed, they are fixed values. Maximum likelihood estimates are functions of sample data that are derived by finding the value of <span class="math inline">\(\pi\)</span> that maximizes the likelihood function. For a given observed data set <span class="math inline">\(y_1, y_2, \dots, y_n\)</span>, when Equation (7.27) is a function of <span class="math inline">\(\pi\)</span>, it is called the likelihood function and denoted <span class="math inline">\(L(\pi)\)</span>.</p>
<p><br>
Equation (7.27) is often called a joint probability function when considered as a function of the data. However, when the data are assumed to be fixed and Equation (7.27) is considered a function of <span class="math inline">\(\pi\)</span>, it is called a likelihood function.<br></p>
<p>The <strong>maximum likelihood estimate</strong> is the value of <span class="math inline">\(\pi = P(Y = 1)\)</span> that maximizes Equation (7.27). For simplicity, it is common to find the value of <span class="math inline">\(\pi\)</span> that maximizes the log of the likelihood function. Recall that the value of <span class="math inline">\(\pi\)</span> that maximizes the likelihood function, <span class="math inline">\(L(\pi)\)</span>, will also maximize the log-likelihood function, <span class="math inline">\(\ln L(\pi)\)</span>.</p>
<p><span class="math display">\[\begin{align}
\ln L(\pi)
&amp;= \ln\!\bigl(\pi^{\sum_{i=1}^n k_i}\,(1 - \pi)^{\sum_{i=1}^n (1 - k_i)}\bigr) \notag \\
&amp;= \sum_{i=1}^n k_i \ln(\pi)\;+\;\bigl(n - \sum_{i=1}^n k_i\bigr)\ln(1 - \pi)
\tag{7.28}
\end{align}\]</span></p>
<p><br>
The principle of maximum likelihood estimation is to choose a value of <span class="math inline">\(\pi\)</span> such that the observed data set is most likely to occur (i.e., the likelihood function is maximized).<br></p>
<p>To find the maximum value of <span class="math inline">\(\ln L(\pi)\)</span>, we take the derivative of <span class="math inline">\(\ln L(\pi)\)</span> and set the first derivative equal to 0:</p>
<p><span class="math display">\[\begin{align}
\frac{d[\ln L(\pi)]}{d\pi}
&amp;= \sum_{i=1}^n k_i \,\frac{1}{\pi} \;+\;\bigl(n - \sum_{i=1}^n k_i\bigr)\,\frac{-1}{(1 - \pi)} = 0
\tag{7.29}
\end{align}\]</span></p>
<p>*?Calculus required</p>
<p>Then we solve the following equivalent equation in terms of <span class="math inline">\(\pi\)</span>:</p>
<p><span class="math display">\[\begin{align}
(1 - \pi)\sum_{i=1}^n k_i \;-\;\pi\bigl(n - \sum_{i=1}^n k_i\bigr) = 0
\tag{7.30}
\end{align}\]</span></p>
<p>This provides the maximum likelihood estimator of <span class="math inline">\(\pi = P(Y = 1)\)</span>:</p>
<p><span class="math display">\[\begin{align}
\displaystyle \hat{\pi} = \frac{\sum_{i=1}^n k_i}{n}
\tag{7.31}
\end{align}\]</span></p>
<p>In this example, the maximum likelihood estimator is the same as our well-known frequentist approach to estimating <span class="math inline">\(\pi = P(Y = 1)\)</span>. However, this is not always the case.</p>
<p><br>
The second derivative of <span class="math inline">\(\ln L(\pi)\)</span> can also be calculated to show that the function is concave down. Thus, <span class="math inline">\(\hat{\pi}\)</span> is a local maximum and not a local minimum. Likelihood functions with just one parameter typically have only one critical value, which is the maximum value. When more than one parameter is involved, more care needs to be taken to ensure that the critical values are actually maximum likelihood estimates.<br></p>
</div>
<div id="extended-activity-calculating-maximum-likelihood-estimates" class="section level2 unnumbered">
<h2>Extended Activity: Calculating Maximum Likelihood Estimates<a class="anchor" aria-label="anchor" href="#extended-activity-calculating-maximum-likelihood-estimates"><i class="fas fa-link"></i></a>
</h2>
<ol start="34" style="list-style-type: decimal">
<li>For binomial response data that follow the binomial distribution, the log-likelihood function for one observation (<span class="math inline">\(y_1\)</span>) is<br><span class="math display">\[\begin{align}
\ln[P(Y_1 = k)]
&amp;= \ln\!\bigl(\binom{n_1}{k}\,\pi_1^k\,(1 - \pi_1)^{n_1 - k}\bigr) \notag \\
&amp;= C \;+\;k\ln(\pi_1)\;+\;(n_1 - k)\ln(1 - \pi_1)
\tag{7.32}
\end{align}\]</span>
Where <span class="math inline">\(C\)</span> is a constant that does not influence <span class="math inline">\(\pi_1\)</span>, we can drop <span class="math inline">\(C\)</span> from the above equation without any impact on the maximum likelihood estimate. Assume that you observed <span class="math inline">\(y_1 = 5\)</span> malignant cells out of a sample of <span class="math inline">\(n_1 = 12\)</span>. Substituting these values into Equation (7.32) and dropping <span class="math inline">\(C\)</span> simplifies the log-likelihood function to <span class="math inline">\(\ln[P(Y_1 = 5)] = 5\ln(\pi_1) + 7\ln(1 - \pi_1).\)</span>
</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Plot the log-likelihood function for several values of <span class="math inline">\(\pi_1\)</span> between 0 and 1. Use the plot to estimate the value of <span class="math inline">\(\pi_1\)</span> that will maximize the log-likelihood function.<br>
</li>
<li>If you have had calculus, set the derivative of the log-likelihood function to 0 and solve for <span class="math inline">\(\pi_1\)</span>. What is the maximum likelihood estimate for <span class="math inline">\(\pi_1\)</span>?</li>
</ol>
</div>
<div id="maximum-likelihood-estimator-for-logistic-regression-models" class="section level2 unnumbered">
<h2>Maximum Likelihood Estimator for Logistic Regression Models<a class="anchor" aria-label="anchor" href="#maximum-likelihood-estimator-for-logistic-regression-models"><i class="fas fa-link"></i></a>
</h2>
<p>The previous example did not address cases in logistic regression where the observations depend on one or more explanatory variables. To use maximum likelihood estimation in logistic regression, from Equation (7.6) we see that</p>
<p><span class="math display">\[
\pi_i = \frac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}}.
\notag
\]</span></p>
<p>Replacing this value for <span class="math inline">\(\pi\)</span> in Equation (7.28) gives</p>
<p><span class="math display">\[\begin{align}
\ln L(\beta_0, \beta_1)
&amp;= \sum_{i=1}^n k_i \ln\!\bigl(\tfrac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}}\bigr)
\;+\;\bigl(n - \sum_{i=1}^n k_i\bigr)\ln\!\Bigl[1 - \bigl(\tfrac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}}\bigr)\Bigr]
\tag{7.33}
\end{align}\]</span></p>
<p>To find the maximum likelihood estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, take the derivative of Equation (7.33) with respect to <span class="math inline">\(\beta_0\)</span> and respect to <span class="math inline">\(\beta_1\)</span>. This will provide two equations and two unknowns. However, the two equations will not be linear and cannot be solved directly.</p>
<p><br>
Finding estimates for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that maximize the log-likelihood function is an iterative technique that quickly becomes complex even for small data sets and is not typically done by hand. Iterative techniques start with initial estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. An iterative technique such as the Newton–Raphson method repeatedly provides new estimates for <span class="math inline">\(\beta_0\)</span> or <span class="math inline">\(\beta_1\)</span> that increase the log-likelihood until the log-likelihood does not notably change.<span class="math inline">\(^{15}\)</span><br></p>
</div>
<div id="chapter-summary-5" class="section level2 unnumbered">
<h2>
<strong>Chapter Summary</strong><a class="anchor" aria-label="anchor" href="#chapter-summary-5"><i class="fas fa-link"></i></a>
</h2>
<p>Throughout this chapter, we have discussed how to conduct logistic regression for binary response data. When the observed response variable is binary, <span class="math inline">\(y_i = 1\)</span> typically represents a success (or outcome of interest) and <span class="math inline">\(y_i = 0\)</span> represents a failure. In the logistic regression model, the estimated response is typically defined as the log-odds of the probability of a success:
<span class="math display">\[\begin{align}
\ln\left(\frac{\pi_i}{1-\pi_i}\right) = \beta_0 + \beta_1 x_i \quad \text{for} \quad i = 1, 2, \ldots, n
\notag
\end{align}\]</span></p>
<p>where <span class="math inline">\(\pi_i = E[y_i | x_i] = P(y_i = 1)\)</span>. Solving the above equation for <span class="math inline">\(\pi_i\)</span> shows that the can be calculated with the following equation:
<span class="math display">\[\begin{align}
\hat{\pi}_i = \frac{e^{b_0 + b_1 x_i}}{1 + e^{b_0 + b_1 x_i}}
\notag
\end{align}\]</span></p>
<p>where <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are <strong>maximum likelihood estimates</strong> of the regression coefficients. While the logit transformation can create a nice S-shaped curve, the model assumptions for least squares regression are violated. For hypothesis testing, both logistic and least squares regression assume a linear predictor and independent observations. In addition, outliers and highly correlated explanatory variables can influence hypothesis test results in both logistic and least squares regression. However, logistic regression does not assume that the error terms are normally distributed or have equal variances. Logistic regression hypothesis tests are based on asymptotic tests and require large sample sizes.</p>
<p><strong>Wald’s test</strong> and the <strong>likelihood ratio test</strong> can be used to test the significance of individual explanatory variables. If Wald’s test and the likelihood ratio test provide different results, the results of the likelihood ratio test should be used. However, the likelihood ratio test can only test whether the slope is equal to zero (<span class="math inline">\(H_0: \beta_1 = 0\)</span> versus <span class="math inline">\(H_a: \beta_1 \neq 0\)</span>) while Wald’s test allows us to test for any value of the slope and also allows for one-sided hypothesis tests and confidence intervals.</p>
<p>Goodness-of-fit tests, such as the <strong>Pearson chi-square test</strong>, the <strong>deviance test</strong>, and the <strong>Hosmer-Lemeshow test</strong>, can be used to assess how well the model fits the data. Only the Hosmer-Lemeshow test should be used when an explanatory variable is continuous. Even though there is no widely accepted equivalent to <span class="math inline">\(R^2\)</span> in logistic regression, <strong>Somers’ <span class="math inline">\(D\)</span></strong>, <strong>Goodman-Kruskal gamma</strong>, and <strong>Kendall’s tau-a</strong> are used to measure the strength of association by means of a classification table showing correct and incorrect classifications of the response variable.</p>
<p>The slope in logistic regression is typically described using the <strong>odds ratio</strong>, <span class="math inline">\(e^{b_1}\)</span>. If we increase the explanatory variable by one unit, the predicted odds will be multiplied by <span class="math inline">\(e^{b_1}\)</span>. If the explanatory variable is also categorical, <span class="math inline">\(e^{b_1}\)</span> is interpreted as the odds ratio between the two groups.</p>
<p><strong>Variable selection</strong> is a process of determining which explanatory variables should be included in a regression model. We prefer to select the model with the fewest number of terms that still best explains the response. The <strong>drop-in-deviance</strong> test compares the log-likelihood (or deviance) of a full model (a model with several terms) and a restricted model (a model with a smaller subset of the terms from the full model). If the test shows no significant difference between the full and the restricted model, we conclude that the additional terms in the full model can be eliminated. The drop-in-deviance test can be used sequentially until all terms that do not improve the model have been removed from the model.</p>
[[[ This part not working. All the text under the heading is not showing up
## <strong>Exercises</strong>{-}

<p>[[[ This part not working. All the text under the heading is not showing up
## <strong>Endnotes</strong>{-}</p>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="categorical-data-analysis-is-a-tumor-malignant-or-benign.html"><span class="header-section-number">6</span> Categorical Data Analysis: Is a Tumor Malignant or Benign?</a></div>
<div class="next"><a href="references.html">References</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#logistic-regression-the-space-shuttle-challenger"><span class="header-section-number">7</span> Logistic Regression: The Space Shuttle Challenger</a></li>
<li><a class="nav-link" href="#investigation-did-temperature-influence-the-likelihood-of-an-o-ring-failure"><span class="header-section-number">7.1</span> Investigation: Did Temperature Influence the Likelihood of an O-Ring Failure?</a></li>
<li><a class="nav-link" href="#activity-describing-the-data-1">Activity: Describing the Data</a></li>
<li><a class="nav-link" href="#review-of-the-least-squares-regression-model"><span class="header-section-number">7.2</span> Review of the Least Squares Regression Model</a></li>
<li><a class="nav-link" href="#activity-building-a-least-squares-regression-model">Activity: Building a Least Squares Regression Model</a></li>
<li><a class="nav-link" href="#the-logistic-regression-model"><span class="header-section-number">7.3</span> The Logistic Regression Model</a></li>
<li><a class="nav-link" href="#the-logistic-regression-model-using-maximum-likelihood-estimates"><span class="header-section-number">7.4</span> The Logistic Regression Model Using Maximum Likelihood Estimates</a></li>
<li><a class="nav-link" href="#activity-using-software-to-calculate-maximum-likelihood-estimates">Activity: Using Software to Calculate Maximum Likelihood Estimates</a></li>
<li><a class="nav-link" href="#activity-estimating-the-probability-of-success-with-maximum-likelihood-estimates">Activity: Estimating the Probability of Success with Maximum Likelihood Estimates</a></li>
<li><a class="nav-link" href="#interpreting-the-logistic-regression-model"><span class="header-section-number">7.5</span> Interpreting the Logistic Regression Model</a></li>
<li><a class="nav-link" href="#activity-interpreting-a-logistic-regression-model">Activity: Interpreting a Logistic Regression Model</a></li>
<li><a class="nav-link" href="#inference-for-the-logistic-regression-model"><span class="header-section-number">7.6</span> Inference for the Logistic Regression Model</a></li>
<li><a class="nav-link" href="#assumptions-for-logistic-regression-models">Assumptions for Logistic Regression Models</a></li>
<li><a class="nav-link" href="#the-wald-statistic">The Wald Statistic</a></li>
<li><a class="nav-link" href="#activity-wald-confidence-intervals-and-hypothesis-tests">Activity: Wald Confidence Intervals and Hypothesis Tests</a></li>
<li><a class="nav-link" href="#the-likelihood-ratio-test">The Likelihood Ratio Test</a></li>
<li><a class="nav-link" href="#activity-the-likelihood-ratio-test">Activity: The Likelihood Ratio Test</a></li>
<li><a class="nav-link" href="#what-can-we-conclude-from-the-space-shuttle-study"><span class="header-section-number">7.7</span> What Can We Conclude from the Space Shuttle Study?</a></li>
<li><a class="nav-link" href="#logistic-regression-with-multiple-explanatory-variables"><span class="header-section-number">7.8</span> Logistic Regression with Multiple Explanatory Variables</a></li>
<li><a class="nav-link" href="#extended-activity-estimating-the-probability-of-malignancy-in-cancer-cells">Extended Activity: Estimating the Probability of Malignancy in Cancer Cells</a></li>
<li><a class="nav-link" href="#the-drop-in-deviance-test"><span class="header-section-number">7.9</span> The Drop-in-Deviance Test</a></li>
<li><a class="nav-link" href="#measures-of-association"><span class="header-section-number">7.10</span> Measures of Association</a></li>
<li><a class="nav-link" href="#extended-activity-measures-of-association">Extended Activity: Measures of Association</a></li>
<li><a class="nav-link" href="#review-of-means-and-variances-of-binary-and-binomial-data"><span class="header-section-number">7.11</span> Review of Means and Variances of Binary and Binomial Data</a></li>
<li><a class="nav-link" href="#extended-activity-understanding-the-binomial-distribution">Extended Activity: Understanding the Binomial Distribution</a></li>
<li><a class="nav-link" href="#calculating-logistic-regression-models-for-binomial-counts"><span class="header-section-number">7.12</span> Calculating Logistic Regression Models for Binomial Counts</a></li>
<li><a class="nav-link" href="#extended-activity-binomial-logistic-regression">Extended Activity: Binomial Logistic Regression</a></li>
<li><a class="nav-link" href="#calculating-residuals-for-logistic-models-with-binomial-counts"><span class="header-section-number">7.13</span> Calculating Residuals for Logistic Models with Binomial Counts</a></li>
<li><a class="nav-link" href="#extended-activity-evaluating-residuals-in-binomial-logistic-regression">Extended Activity: Evaluating Residuals in Binomial Logistic Regression</a></li>
<li><a class="nav-link" href="#assessing-the-fit-of-a-logistic-regression-model-with-binomial-counts"><span class="header-section-number">7.14</span> Assessing the Fit of a Logistic Regression Model with Binomial Counts</a></li>
<li><a class="nav-link" href="#extended-activity-calculating-residuals-by-hand">Extended Activity: Calculating Residuals by Hand</a></li>
<li><a class="nav-link" href="#extended-activity-goodnessoffit-tests-for-continuous-explanatory-variables">Extended Activity: Goodness‐of‐Fit Tests for Continuous Explanatory Variables</a></li>
<li><a class="nav-link" href="#extended-activity-goodnessoffit-tests-for-continuous-explanatory-variables-1">Extended Activity: Goodness‐of‐Fit Tests for Continuous Explanatory Variables</a></li>
<li><a class="nav-link" href="#diagnostic-plots"><span class="header-section-number">7.15</span> Diagnostic Plots</a></li>
<li><a class="nav-link" href="#extended-activity-identifying-outliers-and-influential-observations">Extended Activity: Identifying Outliers and Influential Observations</a></li>
<li><a class="nav-link" href="#maximum-likelihood-estimation-in-logistic-regression"><span class="header-section-number">7.16</span> Maximum Likelihood Estimation in Logistic Regression*</a></li>
<li><a class="nav-link" href="#maximum-likelihood-estimator-for-binary-data">Maximum Likelihood Estimator for Binary Data</a></li>
<li><a class="nav-link" href="#extended-activity-calculating-maximum-likelihood-estimates">Extended Activity: Calculating Maximum Likelihood Estimates</a></li>
<li><a class="nav-link" href="#maximum-likelihood-estimator-for-logistic-regression-models">Maximum Likelihood Estimator for Logistic Regression Models</a></li>
<li><a class="nav-link" href="#chapter-summary-5">Chapter Summary</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/rstudio/bookdown-demo/blob/master/Chap7.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/rstudio/bookdown-demo/edit/master/Chap7.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Book Chapter Example</strong>" was written by Your Name. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
