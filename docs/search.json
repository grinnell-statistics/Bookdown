[{"path":"index.html","id":"an-introduction-to-nonparametric-methods-schistosomiasis","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"1 An Introduction to Nonparametric Methods: Schistosomiasis","text":"Using statistics substitute thinking problem\n-Douglas Montgomery1Randomization tests, permutation tests, bootstrap methods quickly gaining popularity methods conduct statistical inference. ? nonparametric methods require fewer assumptions provide results often accurate traditional techniques using well-known distributions (normal, t, F distribution). methods based computer simulations instead distributional assumptions thus particularly useful sample data skewed sample size small. addition, nonparametric methods can extended parameters interest, median standard deviation, well known parametric methods described introductory statistics courses often restricted just inference population mean.    begin chapter comparing two treatments potentially deadly disease called Schistosomiasis (shis-tuh-soh-mahy-uh-sis). illustrate basic concepts behind nonparametric methods using randomization tests :Provide intuitive description statistical inference.Conduct randomization test handUse software conduct randomization testCompare one-sided two-sided hypothesis testsMaking connections randomization tests conventional terminologyAfter working schistosomiasis investigation, opportunity \nanalyze several data sets using randomization tests, permutation tests, bootstrap methods,\nrank-based nonparametric tests.","code":""},{"path":"index.html","id":"investigation-can-a-new-drug-reduce-the-spread-of-schistosomiasis","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"1.1 Investigation: Can a New Drug Reduce the Spread of Schistosomiasis?","text":"Schistosomiasis disease occurring humans caused parasitic flatworms called schistosomes (skis’-tuhsohms).\nSchistosomiasis affects 200 million people worldwide serious problem sub-Saharan\nAfrica, South America, China, Southeast Asia. disease can cause death, commonly results\nchronic debilitating symptoms, arising primarily body’s immune reaction parasite eggs\nlodged liver, spleen, intestines. Currently one drug, praziquantel (prā’zĭ-kwän’těl’), common use treatment schistosomiasis; cheap effective. However many organizations worried relying single drug treat serious disease affects many people worldwide. Drug resistance may prompted 1990s outbreak Senegal, cure rates low. 2007, several researchers published work involving promising drug called K11777 , theory, might also treat schistosomiasis. chapter, analyze data study researchers wanted find whether K11777 helps stop schistosome worms growing. one phase study, 10 female laboratory mice 10 male laboratory mice deliberately infected schistosome parasite. Seven days infected schistosomiasis, mouse given injections every day 28 days. Within sex, 5 mice randomly assigned treatment K11777 whereas 5 mice formed control group injected equal volume plain water. day 49, researchers euthanized mice measured number eggs numbers worms mice livers. numbers expected lower drug effective.Table 1.1 gives worm count mouse. individual value plot data shown Figure 1.1. Notice treatment group fewer worms control group females males.\nFigure 1.1: Individual value plot worm count data\nNOTE\ndifference individual value plots dotplots. dotplots (Figures 1.3 \n1.4 shown later chapter), observation represented dot along number line (x-axis).\nvalues close , dots stacked. Dotplots can used place histograms\nsample size small. Individual value plots, shown Figure 1.1, used simultaneously\ndisplay observation multiple groups. can used instead boxplots identify outliers distribution shape, especially relatively observations.","code":""},{"path":"index.html","id":"activity-describing-the-data","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"Activity: Describing the Data","text":"Use Figure 1.1 visually compare number worms treatment control groups \nmale female mice. four groups appear similar center similar\nspread? outliers (extreme observations don’t seem fit rest data)?Calculate appropriate summary statistics (e.g., median, mean, standard deviation, range) \nfour groups. female mice, calculate difference treatment control\ngroup means. male mice.descriptive analysis Questions 1 2 points positive treatment effect: K11777 appears havereduced number parasitic worms sample. descriptive analysis usually first step\nascertaining whether effect real; often conduct significance test create confidence interval\ndetermine chance alone explain effect.introductory statistics courses focus hypothesis tests involve using normal, t-, chi-square F-distribution calculate p-value. tests often based central limit theorem. theschistosomiasis study, five observations group. much smaller sample size\nrecommended central limit theorem, especially given Figure 1.1 indicates data\nmay normally distributed. Since confident sample averages normally distributed,\nuse distribution-free test, also called nonparametric test. tests require\ndistribution sample statistic specific form often useful studies \nsmall sample sizes.\npopulation mean m finite standard deviation s, central limit theorem states \nsample mean x independent identically distributed sample tends follow normal\ndistribution sample size large enough. mean x population mean, m, \nstandard deviation x s/1n, n sample size.use form nonparametric statistical inference known randomization hypothesis test analyze data schistosomiasis study. Randomization hypothesis tests significance tests simulate random allocation units treatments many times order determine likelihood observing outcome least extreme one found actual study.\nintroduce basic concepts randomization tests setting units (mice example) randomly allocated treatment control group. Using significance test, decide observed treatment effect (observed difference mean responses treatment control) “real” “random chance alone” plausibly explain observed effect. null hypothesis states “random chance alone” reason observed effect. initial discussion, alternative hypothesis onesided want show true treatment mean (\\(\\mu\\)treatment) less true control mean (\\(\\mu\\)control). Later, expand discussion consider modifications needed deal two-sided alternatives.","code":""},{"path":"index.html","id":"statistical-inference-through-a-randomization-test","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"1.2 Statistical Inference Through a Randomization Test","text":"Whether take form significance tests confidence intervals, inferential procedures rest fundamental question inference: “happen many times?” Let’s unpack \nquestion context female mice schistosomiasis study. observed difference means\n7.6 = 12.00 - 4.40 worms control treatment groups. expect large difference\nreflects effectiveness drug, possible chance alone explain difference. \n“chance alone” position usually called null hypothesis includes following assumptions:number parasitic worms found liver naturally varies mouse mouse.Whether drug effective, clearly variability responses mice infestation\nschistosomes.group exhibits variability, even drug effective, mice better \nothers.explanation observed difference 7.6 worms means random\nallocation randomly placed mice larger numbers worms control group mice \nsmaller numbers worms treatment group.study, null hypothesis treatment effect average worm count, itis denoted \n| \\(H_0\\): \\(\\mu\\)control = \\(\\mu\\)treatment\nAnother way write null hypothesis \n\\(H_0\\): treatment effect average worm countThe research hypothesis (treatment causes reduction average worm count) called alternative\nhypothesis denoted \\(H_a\\) (\\(H_1\\)). example,\n\\(H_a\\): mcontrol 7 mtreatment\nAnother way write alternative hypothesis \nHa: treatment reduces average worm count\nAlternative hypotheses can “one-sided, greater ” (investigation), “one-sided, less-”\n(treatment causes increase worm count), “two-sided” (treatment mean different, one\ndirection , control mean). chose test one-sided hypothesis \nclear research interest one direction. words, take action (start using drug) \ncan show K11777 reduces worm count.\n","code":""},{"path":"index.html","id":"activity-conducting-a-randomization-test-by-hand","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"Activity: Conducting a Randomization Test by Hand","text":"get feel concept p-value, write female worm counts index card.\nShuffle 10 index cards, draw five cards random (without replacement). Call five\ncards treatment group five remaining cards control group. null hypothesis\n(.e. treatment effect worm counts), allocation mimics precisely actually happened\nexperiment, since cause group differences random allocation.\n| Calculate mean five cards representing treatment group mean five\ncards representing control group. find difference control treatment group means obtained allocation. consistent, take control group mean minus \ntreatment group mean. work look similar following simulation:[[[Fig_CT]]]another random allocation, get difference means? Explain.another random allocation, get difference means? Explain.Now, perform nine random allocations, time computing writing difference \nmean worm count control group treatment group. Make dotplot 10 differences.\nproportion differences 7.6 larger?Now, perform nine random allocations, time computing writing difference \nmean worm count control group treatment group. Make dotplot 10 differences.\nproportion differences 7.6 larger?performed simulation many times, expect large percentage simulations \nresult mean difference greater 7.6? Explain.performed simulation many times, expect large percentage simulations \nresult mean difference greater 7.6? Explain.reasoning previous activity leads us randomization test interpretation thefundamental question inference. fundamental question context follows: “null\nhypothesis actually true randomly allocated 10 mice treatment control groups many\ntimes, proportion time observed difference means big bigger 7.6?”\nlong-run proportion probability statisticians call p-value randomization test. \np-values randomization tests found simulations. Despite fact simulations \ngive exact p-values, usually preferred tedious time-consuming process listing\npossible outcomes. Researchers usually pick round number 10,000 repetitions simulation\napproximate p-value accordingly. Since p-value approximation, often referred \nempirical p-value.\n\nMany researchers include observed value one possible outcomes. case, N = 9999\niterations typically used p-value calculated (X + 1)/(9999 + 1). results \nsimilar whether X/10,000 (X + 1)/(9999 + 1) used. Including observed value one \npossible allocations conservative approach protects getting p-value 0. \nobservation actual experiment provides evidence true p-value greater zero.","code":""},{"path":"index.html","id":"performing-a-randomization-test-using-a-computer-simulation","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"1.3 Performing a Randomization Test Using a Computer Simulation","text":"physical simulations (index cards activity) help us understand process computing \nempirical p-value, using computer software much efficient way producing empirical p-value\nbased large number iterations. simulating 10 random allocations, just easy use index cards computer. However, advantage computer simulation 10,000 random allocations\ncan conducted almost amount time takes simulate 10 allocations. following\nsteps, develop program calculate empirical p-value.","code":""},{"path":"index.html","id":"activity-using-computer-simulations-to-conduct-a-hypothesis-test","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"Activity: Using Computer Simulations to Conduct a Hypothesis Test","text":"Use technology instructions provided CD insert schistosomiasis data statistical\nsoftware package randomly allocate 10 female worm counts either treatment \ncontrol group.Use technology instructions provided CD insert schistosomiasis data statistical\nsoftware package randomly allocate 10 female worm counts either treatment \ncontrol group.Take control group average minus K11777 treatment group average.Take control group average minus K11777 treatment group average.Use instructions write program, function, macro repeat process 10,000 times. Count\nnumber simulations difference group averages (control minus K11777) \ngreater equal 7.6, divide count 10,000, report resulting empirical p-value.Use instructions write program, function, macro repeat process 10,000 times. Count\nnumber simulations difference group averages (control minus K11777) \ngreater equal 7.6, divide count 10,000, report resulting empirical p-value.Create histogram 10,000 simulated differences group means comment \nshape histogram. histogram, created simulations randomization test, called \nempirical randomization distribution. distribution describes frequency observed\ndifference (control treatment means) null hypothesis true.Create histogram 10,000 simulated differences group means comment \nshape histogram. histogram, created simulations randomization test, called \nempirical randomization distribution. distribution describes frequency observed\ndifference (control treatment means) null hypothesis true.Based results Questions 9 10 assuming null hypothesis true, frequently\nthink obtain mean difference large larger 7.6 random allocation alone?Based results Questions 9 10 assuming null hypothesis true, frequently\nthink obtain mean difference large larger 7.6 random allocation alone?answer Question 11 lead believe “chance alone” position (.e., null hypothesis\nmean worm count treatment control), lead \nbelieve K11777 positive inhibitory effect schistosome worm female mice? Explain.answer Question 11 lead believe “chance alone” position (.e., null hypothesis\nmean worm count treatment control), lead \nbelieve K11777 positive inhibitory effect schistosome worm female mice? Explain.Figure 1.2 shows histogram resulting previous activity. computer simulation Question 9resulted p-value 281/10,000 = 0.0281. result shows random allocation alone produce\nmean group difference large larger 7.6 3% time, suggesting something\nchance needed explain difference group means. Since distinction \ngroups presence absence treatment, can conclude treatment causes reduction \nworm counts.conducted four simulations, 10,000 iterations, resulted p-values 0.0272,0.0282, 0.0268, 0.0285. number iterations large, empirical randomization distribution\n(histogram created Question 10) provides precise estimate likelihood possible values difference control treatment means. Thus, number iterations large,\nwell-designed simulation studies result empirical p-values fairly accurate. larger number\niterations (.e., randomizations) within simulation study, precise p-value .\nFigure 1.2: Histogram showing results schistosomiasis simulation study. simulation, 281 10,000 resulted difference greater equal 7.6.\n sample sizes schistosomiasis study small, possible apply mathematicalmethods obtain exact p-value randomization test. exact p-value can calculated writing\nset possibilities (assuming possible outcome equally likely null hypothesis)\ncalculating proportion set difference least large observed difference.\nschistosomiasis study, requires listing every possible combination five 10\nfemale mice can allocated treatment (five assigned control). 252 possible\ncombinations. combinations, difference treatment control means\ncalculated. exact p-value proportion times difference means least\nlarge observed difference 7.6 worms. 252 combinations, six mean difference \n7.6 one mean difference greater 7.6 (namely 8.8). Since 252 random allocations \nequally likely, exact p-value example 7/252 = 0.0278. However, real studies large\nlist possible samples. Randomization tests almost always adequate, providing approximate p-values\nclose enough true p-value.\nConducting two-sample t-test female mice provides p-value 0.011. p-value 0.011 \naccurate observed test statistic (.e., difference means) follows appropriate assumptions\ndistribution. Figure 1.2 demonstrates distributional assumptions violated. \nrandomization test provides approximate p-value “close 0.0278,” provides much better estimate\nexact p-value two-sample t-test. Note five simulations listed gave \np-value closer exact p-value one given two-sample t-test. \nSometimes threshold p-value reject null hypothesis andconclude favor alternative. threshold value called significance level usually denoted\nGreek letter alpha (\\(\\alpha\\)). Common values \\(\\alpha\\) = 0.05 \\(\\alpha\\) = 0.01, value depend heavily\ncontext researcher’s assessment acceptable risk stating incorrect conclusion. \nstudy’s p-value less equal significance level, state results statistically\nsignificant level . see phrase “statistically significant” without specification \\(\\alpha\\) writer\nlikely assuming \\(\\alpha\\) = 0.05, reasons history convention alone. However, best show\np-value instead simply stating result significant particular \\(\\alpha\\)-level.","code":""},{"path":"index.html","id":"two-sided-tests","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"1.4 Two-Sided Tests","text":"direction alternative hypothesis derived research hypothesis. K11777 study, \nenter study expecting reduction worm counts hoping data bear expectation. \nexpectation, hope, interest drives alternative hypothesis randomization calculation. Occasionally,\nenter study without firm direction mind alternative, case use two-sided\nalternative. Furthermore, even hope new treatment better old treatment better\ncontrol, might wrong—may new treatment actually worse old treatment\neven harmful (worse control). statisticians argue conservative objective approach \nalways consider two-sided alternative. two-sided test, p-value must take account extreme\nvalues test statistic either direction (matter direction actually observe sample data)\nnow make definition p-value general allow wider variety significancetesting situations. p-value probability observing group difference extreme extreme\ngroup difference actually observed sample data, assuming nothing creating group\ndifferences except random allocation process.","code":""},{"path":"index.html","id":"activity-a-two-sided-hypothesis-test","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"Activity: A Two-Sided Hypothesis Test","text":"Run simulation study find empirical p-value two-sided hypothesis test determine\ndifference treatment control group means female mice.number simulations resulting difference greater equal 7.6 identical number\nsimulations resulting difference less equal -7.6? Explain two values\nlikely close identical.Explain expect p-value two-sided alternative double onesided\nalternative. Hint: may want look Figure 1.2Using two-sided alternative hypothesis, two-sample t-test provides p-value 0.022.2 \np-value provide strong evidence rejecting assumption difference \ntreatment control (null hypothesis). However, p-value used draw\nconclusions study. Explain .study, simulation involving 100,000 iterations provided empirical p-value 0.0554., particular data set small, 252 possible random allocations can listed find \nexact two-sided p-value 14/252 = 0.0556.","code":""},{"path":"index.html","id":"what-can-we-conclude-from-the-schistosomiasis-study","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"1.5 What Can We Conclude from the Schistosomiasis Study?","text":"key question study whether K11777 reduce spread common potentially deadly\ndisease. result calculated one-sided randomization hypothesis test \nclose exact p-value 0.0278. small p-value allows reject null hypothesis conclude\nworm counts lower female treatment group female control group. every study,\nimportant consider random allocation random sampling impact conclusions.Random allocation: schistosomiasis study experiment units (female mice)randomly allocated treatment control groups. best knowledge experiment\ncontrolled outside influences allows us state cause effect relationship\ntreatment response. Therefore, can conclude K11777 cause reduction \naverage number schistosome parasites female mice.Random sampling: Mice type study typically ordered facility breeds raises labmice. possible mice study biologically related exposed something \ncaused response different mice. Similarly, risks simply assuming\nmale mice response females, end--chapter exercises provide opportunity conduct separate test male mice. Since sample 10 female mice selected random\npopulation mice, question whether results study hold mice.importantly, results shown new drug impact humans\nmice. addition, even though found K11777 cause reduction worm counts,\nspecifically show reduce spread disease. disease less deadly two\nworms body instead 10? Statistical consultants aren’t typically expected know answers \ntheoretical, biological, medical types questions, ask questions ensure \nstudy conclusions match hypothesis tested. cases, drug tests require multiple levels \nstudies ensure drug safe show results consistent across entire population \ninterest. study promising, much work needed can conclude K11777\ncan reduce spread schistosomiasis humans.","code":""},{"path":"index.html","id":"a-closer-look-nonparametric-methods","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"A Closer Look: Nonparametric Methods","text":"","code":""},{"path":"index.html","id":"permutation-tests-versus-randomization-tests","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"1.6 Permutation Tests versus Randomization Tests","text":"random allocation experimental units (e.g., mice) groups provides basis statistical inference \nrandomized comparative experiment. schistosomiasis K11777 treatment study, used significance\ntest ascertain whether cause effect work. context random allocation study design,\ncalled significance test randomization test.\n| observational studies, subjects randomly allocated groups. context, apply \ninferential procedures previous experiment, commonly call significance test \npermutation test rather randomization test.3 importantly, observational studies, results\ntest typically used claim cause effect; researcher exhibit caution \ninterpretation results.\n\n","code":""},{"path":"index.html","id":"age-discrimination-study","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"Age Discrimination Study","text":"Westvaco company produces paper products. 1991, Robert Martin working engineering\ndepartment company’s envelope division laid Round 2 several rounds layoffs\ncompany.3 sued company, claiming victim age discrimination. ages 10\nworkers involved Round 2 : 25, 33, 35, 38, 48, 55, 55, 55, 56, 64. ages three people\nlaid 55, 55, 64.Figure 1.3 shows comparative dotplot age layoff category. dotplot gives impression thatRobert Martin may case: appears older workers likely laid . know\nenough variability cautious.\nFigure 1.3: Dotplot age years worker versus layoff (whether laid )\n","code":""},{"path":"index.html","id":"extended-activity-is-there-evidence-of-age-discrimination","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"Extended Activity: Is There Evidence of Age Discrimination?","text":"Data set: Age\n17. Conduct permutation test determine whether observed difference means likely \noccur just chance. Use Age response variable Layoff explanatory variable. \ninterested one-sided hypothesis test determine mean age people \nlaid higher mean age people laid .Modify program/macro created Question 17 conduct one-sided hypothesis test determine\nmedian age people laid higher median age people \nlaid . Report p-value compare results Question 17. Since random allocation (.e., people randomly assigned layoff group),statistical significance give us right assert greater age causing difference \nlaid . null hypothesis context becomes “observed difference explained \nrandom allocation alone.” , proceed practicing social scientist must working\nobservational data. “imagine” experiment workers randomly allocated \nlayoff group determine observed average difference ages laid-workers\nlaid significantly larger expected occur chance randomized\ncomparative experiment.\n| age cause difference—hence proving allegation age discrimination—\nmany possibilities (.e., extraneous variables), educational levels \nworkers, competence job, ratings past performance evaluations. Rejecting \n“random allocation” hypothesis nonrandomized context can useful step toward\nestablishing causality; however, establish causality unless extraneous variables \nproperly accounted .\n| actual court case, data three rounds layoffs statistically analyzed. analysis\nshowed evidence older people likely laid ; however, Robert Martin ended \nsettling court.","code":""},{"path":"index.html","id":"permutation-and-randomization-tests-for-matched-pairs-designs","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"1.7 Permutation and Randomization Tests for Matched Pairs Designs","text":"ideas developed chapter can extended study designs, basic two-variable design\ncalled matched pairs design. matched pairs design, experimental unit provides measurements\nstudy two treatments (one control). Conversely, completely randomized\nsituation schistosomiasis K11777 treatment study, half units assigned control half \ntreatment; mouse received treatments.","code":""},{"path":"index.html","id":"music-and-relaxation","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"Music and Relaxation","text":"Grinnell College students Anne Tillema Anna Tekippe conducted experiment study effect \nmusic person’s level relaxation. hypothesized fast songs increase pulse rate \nslow songs. file called Music contains data experiment. decided use person’s\npulse rate operational definition person’s level relaxation compare pulse rates two selections music: fast song slow song. fast song chose “Beyond” Nine Inch\nNails, slow song chose Rachmaninoff’s “Vocalise.” recruited 28 student subjects \nexperiment.Anne Anna came following experimental design. fundamental questioninvolved two treatments: (1) listening fast song (2) listening slow song. \nrandomly allocated 14 subjects hear fast song 14 subjects hear slow song, \nefficient approach subject provide measurements. , subject\nlistened songs, giving rise two data values subject, called matched pairs. Randomization\ncame play decided coin flip whether subject listen first \nfast song slow song.\n Specifically, determined coin flips, half subjects experienced following procedure:[one minute rest; measure pulse (prepulse)] \\(>\\) [listen fast song 2 minutes; measure pulse\nsecond minute (fast song pulse)] \\(>\\) [rest one minute] \\(>\\) [listen slow song 2 minutes;\nmeasure pulse second minute (slow song pulse)].half experienced procedure way except heard slow song first andthe fast song second.\n| subject gives us two measurements interest analysis: (1) fast song pulse minus prepulse\n(2) slow song pulse minus prepulse. data file, two measurements called Fastdiff \nSlowdiff, respectively.Figure 1.4 shows dotplot 28 Fastdiff-minus-Slowdiff values. Notice positive numberspredominate mean difference 1.857 beats per minute, suggesting fast song indeed\nheighten response (pulse rate) slow song. need confirm suspicion randomization\ntest.perform randomization test, mimic randomization procedure study design. ,randomization determined order subject heard songs, randomization applied\ntwo measurements interest subject. compute p-value, determine frequently\nobtain observed difference large larger 1.857.\nFigure 1.4: Dotplot difference pulse rates 28 subjects.\n","code":"#> Bin width defaults to 1/30 of the range of the data. Pick\n#> better value with `binwidth`."},{"path":"index.html","id":"extended-activity-testing-the-effect-of-music-on-relaxation","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"Extended Activity: Testing the Effect of Music on Relaxation","text":"\n","code":""},{"path":"index.html","id":"the-bootstrap-distribution","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"1.8 The Bootstrap Distribution","text":"Bootstrapping another simulation technique commonly used develop confidence intervals \nhypothesis tests. Bootstrap techniques useful generalize situations traditional methods\nbased normal distribution applied. example, can used create confidence intervals\nhypothesis tests parameter interest, median, ratio, standard deviation. Bootstrap\nmethods differ previously discussed techniques sample replacement (randomly draw\nobservation original sample put observation back drawing next observation).\n| Permutation tests, randomization tests, bootstrapping often called resampling techniques\n, instead collecting many different samples population, take repeated samples (called\nresamples) just one random sample.","code":""},{"path":"index.html","id":"extended-activity-creating-a-sampling-distribution-and-a-bootstrap-distribution","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"Extended Activity: Creating a Sampling Distribution and a Bootstrap Distribution","text":"\nmany real-world situations, process used Question 21 practical collecting morethan one simple random sample expensive time consuming. approach Question 22 \ncomputer intensive, simple convenient since uses one simple random sample. key idea\nbehind bootstrap methods assumption original sample represents population, resamples\none simple random sample can used represent samples population, done Question\n22. Thus, bootstrap distribution provides approximation sampling distribution.traditional methods statistical inference involve collecting one sample calculating samplemean. , based central limit theorem, assumptions made shape spread \nsampling distribution. Question 22 used one sample calculate sample mean used \nbootstrap distribution estimate shape spread sampling distribution.central limit theorem tells us shape spread sample mean. key advantage ofthe bootstrap distribution works parameter interest. Thus, bootstrap distribution can \nused estimate shape spread sampling distribution interest.\nFigure 1.5 shows sampling distribution bootstrap distribution sample size 10 used estimate mean ChiSq data. Notice spreads histograms areroughly equivalent. central limit theorem tells us standard deviation sampling distribution (distribution \\(\\bar{x}\\) ) \\(\\sigma\\)/\\(\\sqrt{n}\\) = 1.3153/\\(\\sqrt{10}\\) = 0.4159. standard deviation bootstrap distribution 0.4541, reasonable estimate standard deviation sampling distribution. addition, graphs similar, right-skewed shapes. strength bootstrap method provides accurate estimates shape spread sampling distribution. general, histograms bootstrap distribution similar shape spread histograms\nsampling distribution.[[[Fig1,5]]]bootstrap method improve estimate population mean. mean sampling distribution Question 21 typically close population mean. mean bootstrap distribution Question 22 typically accurate, based one simple random sample. Ideally, like know close statistic original sample population parameter. statistic biased centered value population parameter. can use bootstrap distribution estimate bias statistic. difference original sample mean bootstrap mean called bootstrap estimate bias.\n","code":"#> [1] \"1.465279886\""},{"path":"index.html","id":"using-bootstrap-methods-to-create-confidence-intervals","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"1.9 Using Bootstrap Methods to Create Confidence Intervals","text":"confidence interval gives range plausible values parameter. range values surrounding observed estimate parameter—estimate based data. range values attach level confidence true parameter lies range. alpha-level, \\(\\alpha\\), often used specify level confidence. example, \\(\\alpha\\) = 0.05, 100(1 - \\(\\alpha\\)), = 95\\(\\%\\) confidence level. Thus, 100(1 - \\(\\alpha\\)), confidence interval gives estimate think parameter precisely pinned .","code":""},{"path":"index.html","id":"bootstrap-t-confidence-intervals-","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"1.10 *Bootstrap t Confidence Intervals{-}","text":"bootstrap distribution appears approximately normal, typically safe assume \nt-distribution can used calculate 100(1 - \\(\\alpha\\)), confidence interval \\(\\mu\\), often called bootstrap\nt confidence interval:\\[\\begin{equation}\n  \\bar{x} \\pm t^*\\left(S^*\\right)\n  \\tag{1.1} \\label{eq:1_1}\n\\end{equation}\\]\\(S^*\\) standard deviation bootstrap distribution \\(t^*\\) critical value t-distribution n - 1 degrees freedom.one simple random sample size n = 10 used create bootstrap distribution Figure 1.5b mean \\(\\bar{x}\\) = 1.238 standard deviation s = 1.490. bootstrap distribution Figure 1.5b mean \\(\\bar{x}^*\\) = 1.249 standard deviation \\(S^*\\) = 0.4541. Notice Formula (1.1) uses mean original sample uses bootstrap distribution estimate spread. incorrectly assume sampling distribution Figure 1.5 normal, 95% bootstrap t confidence interval \\(\\mu\\) given \\[\\begin{equation}\n  \\bar{x} \\pm t^*\\left(S^*\\right) = 1.238 \\pm 2.262(0.4541)\n\\end{equation}\\]\\(t^*\\) = 2.262 critical value corresponding 97.5th percentile t-distribution n - 1 = 9\ndegrees freedom. Thus, 95% confidence interval \\(\\mu\\) (0.211, 2.265).\nskewed data small sample sizes (original data normally distributed), parametric\nmethods (based central limit theorem) appropriate. Figure 1.5 see \nsampling distribution skewed right. Thus, sample size 10, neither traditional onesample\nt confidence interval bootstrap t confidence interval reliable example. However,\nsample size 40, histograms Questions 21 22 tend look somewhat normally\ndistributed.","code":""},{"path":"index.html","id":"bootstrap-percentile-confidence-intervals","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"Bootstrap Percentile Confidence Intervals","text":"Bootstrap percentile confidence intervals found calculating appropriate percentiles bootstrap distribution. find 100(1 - \\(\\alpha\\)) confidence interval, take \\(\\alpha\\)/2 * 100 percentile tail bootstrap distribution. example, find 95% confidence interval \\(\\mu\\), sort observations bootstrap distribution find values represents 2.5th 97.5th percentiles bootstrap distribution. \n2.5th percentile bootstrap distribution Figure 1.5b 0.546, 97.5th percentile 2.282. Thus,\n95% confidence interval \\(\\mu\\) (0.546, 2.282).\nNotice percentile confidence interval centered sample mean. Since bootstrap\ndistribution right skewed, right side confidence interval (2.282 - 1.238 = 1.044) wider \nleft side confidence interval (1.238 - 0.546 = 0.692). lack symmetry can influence \naccuracy confidence interval.\n","code":""},{"path":"index.html","id":"when-to-use-bootstrap-confidence-intervals","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"When to Use Bootstrap Confidence Intervals","text":"Bootstrap methods extremely useful use theory, central limit theorem, \napproximate sampling distribution. Thus, bootstrap methods can used create confidence intervals\nessentially parameter interest, central limit theorem limited parameters\n(population mean).4 However, bootstrap methods always reliable.Small sample sizes still produce problems bootstrap methods. sample size small, (1) sample statistic may accurately estimate population parameter, (2) distribution sample means less likely symmetric, (3) shape spread bootstrap distribution may accurately represent true sampling distribution.addition, bootstrap methods work equally well parameters. example, end-ofchapterexercises show bootstrapping often provides unreliable bootstrap distributions median values median resample likely possible values. Thus, confidence intervals medians used large (n \\(\\geq\\) 100) sample sizes.easy determine whether bootstrap methods provide appropriate confidence intervals. bootstrap t bootstrap percentile confidence intervals often compared . percentile confidence interval tends accurate, neither two used intervals relativelyclose. bootstrap distribution skewed biased, methods used find confidence intervals. advanced bootstrap methods (BCa tilting confidence intervals) available generally accurate bias skewness exists bootstrap distribution.\\(^5\\)","code":""},{"path":"index.html","id":"extended-activityestimating-salaries-of-medical-faculty","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"Extended Activity:Estimating Salaries of Medical Faculty","text":"","code":""},{"path":"index.html","id":"relationship-between-the-randomization-test-and-the-two-sample-t-test","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"1.11 Relationship Between the Randomization Test and the Two-Sample t-Test","text":"R.. Fisher, perhaps preeminent statistician 20th century, introduced randomization test context two-group randomly allocated experiment famous 1935 book, Design Experiments.\\(^6\\) time acknowledged randomization test practical computational\nintensity calculation. Clearly, 1935 predates modern computing. Indeed, Efron Tibshirani describe permutation test “computer-intensive statistical technique predates computers.”\\(^7\\) Fisher went assert classical two-sample t-test (independent samples) approximates randomization test well. Ernst cites references several approximations randomization tests using classical computationally tractable methods published time.\\(^8\\)seen two-sample tests previously, likely context Ernst calls population model, distinguishes randomization model. population model, units selected random one populations. observational studies population models. One simple case population model involves comparing two separate population means. case, can take two independent simple random samples use classic two-sample t-test make comparison.*randomization model, fixed number experimental units randomly allocated treatments. experiments randomization models. randomization models schistosomiasis example,two samples formed collection available experimental units randomly divided two groups. Since fixed number units, groups completely independent. example,\none 10 male mice natural resistance schistosomiasis randomly placed treatment group, expect control group slightly higher worm count. Since two groups completely independent, assumptions classic two-sample t-test violated. Even sample sizes schistosomiasis study much larger, randomization test appropriate test two-sample t-test. However, empirical evidence shown two-sample t-test good approximation randomization test sample sizes large enough. fortunate , age modern computing, longer routinely compromise using t-test approximate randomization test.\n","code":""},{"path":"index.html","id":"wilcoxon-rank-sum-tests-for-two-independent-samples","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"1.12 Wilcoxon Rank Sum Tests for Two Independent Samples","text":"Wilcoxon rank sum test, also called two-sample Mann-Whitney test, makes inferences difference two populations based data two independent random samples. test ranks observations two samples arranging order smallest largest.Focusing ranks instead actual observed values allows us remove assumptions normal distribution. Rank-based tests used many years. However, rank-based methods (discussedin section next section) much less accurate methods based simulations. general, randomization tests, permutation tests, bootstrap methods used whenever possible.following example examines whether pitchers first basemen play National League baseball teams salary distribution. null alternative hypotheses written ,\\(H_0\\): distribution salaries pitchers first basemen\n\\(H_a\\): distribution salaries different pitchers first basemenTable 1.2 shows salaries five pitchers five first basemen randomly selected National League baseball players. Table 1.3 ranks players based 2005 salaries.Note two players exactly salary, standard practice average ranks tied values.Table 1.1: Randomly selected pitchers first baseman 2005 National League baseball teams.Table 1.2: Ranking 10 randomly selected 2005 National League baseball players.Wilcoxon rank sum test, define following terms:\\(n_1\\) sample size first group (5 pitcher group example)\\(n_2\\) sample size second group (5 first baseman group example)N= \\(n_1\\) + \\(n_2\\)W, Wilcoxon rank sum statistic, sum ranks first group\n(1 + 2 + 3 + 4 + 8 = 18)two groups continuous distribution, W mean,\\[\\begin{equation}\n  \\mu_W = \\frac{n_1(N+1)}{2} = \\frac{5(11)}{2}=27.5\n  \\tag{1.3} \\label{eq:1_3}\n\\end{equation}\\]standard deviation\\(^9\\)\\[\\begin{equation}\n  \\sigma_W = \\sqrt{\\frac{n_1n_2(N+1)}{12}} = \\sqrt{\\frac{(5)(5)(11)}{12}}= 4.787\n  \\tag{1.4} \\label{eq:1_4}\n\\end{equation}\\]W far \\(\\mu_W\\), Wilcoxon rank sum test rejects hypothesis two populations identical distributions—, rejects \\(H_0\\) (difference distribution salaries) favor \\(H_a\\) (salary distributions different based position). p-value probability observing sample statistic, W, least extreme one sample. Since 18 less hypothesized mean, 27.5, p-value two-sided test example found calculating 2 * P(W \\(\\leq\\) 18).\n","code":""},{"path":"index.html","id":"extended-activity-wilcoxon-rank-sum-tests","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"Extended Activity: Wilcoxon Rank Sum Tests","text":"first may seem somewhat surprising first basemen tend make pitchers. However, 2005 19 first basemen 215 pitchers National League. Many pitchers play much got paid low salary, whereas 19 first basemen considered quite valuable teams.","code":""},{"path":"index.html","id":"kruskal-wallis-test-for-two-or-more-independent-samples","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"1.13 Kruskal-Wallis Test for Two or More Independent Samples","text":"Kruskal-Wallis test another popular nonparametric test often used compare two independent samples. Like ANOVA, common parametric test discussed later chapters, Kruskal-Wallis test requires independent random samples population. data clearly deviate normal distribution, Kruskal-Wallis test likely one-way ANOVA identify true differences population. null alternative hypotheses Kruskal-Wallis test :\\(H_0\\): distribution response variable groups\n\\(H_a\\): responses systematically higher groups othersTable 1.3: Randomly selected catchers 2005 National League baseball teams.Kruskal-Wallis test also based ranks. ranks summed group, group sums far apart, evidence groups different. calculations Kruskal-Wallis test statistic provided , suggest using statistical software conduct significance test. Continuing baseball salaries example, Table 1.4 displays salaries five randomly selected catchers 2005 National League baseball teams.Kruskal-Wallis test, define following terms:\\(n_1\\) sample size first group (5 pitcher group)\\(n_2\\) sample size second group (5 first baseman group)\\(n_3\\) sample size third group (5 catcher group)N = \\(n_1\\) + \\(n_2\\) + \\(n_3\\)\\(R_i\\) sum ranks ith group (\\(R_1\\) = 35, \\(R_2\\) = 62, \\(R_3\\) = 23)Kruskal-Wallis test statistic calculated ,\\[\\begin{equation}\n  H = \\frac{12}{N(N + 1)} \\sum_{=1}^k \\frac{R_i^2}{n_i} - 3(N + 1) = \\frac{12}{(15)(16)}(\\frac{35^2}{5}+\\frac{62^2}{5}+\\frac{23^2}{5}) -3(16) =7.98\n  \\tag{1.5} \\label{eq:1_5}\n\\end{equation}\\]exact distribution H null hypothesis depends ni, complex time consuming calculate. Even statistical software packages use chi-square approximation - 1 degrees freedom obtain p-values (number groups).\n","code":""},{"path":"index.html","id":"extended-activity-kruskal-wallis-test","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"Extended Activity: Kruskal-Wallis Test","text":"Data set: NLBB SalariesUsing software package, run Kruskal-Wallis test (use three groups samples size 5 per group) determine distribution salaries differs position. Create individual value plot data. data look normally distributed group?\nNonparametric tests based rank usually less powerful (less likely reject null hypothesis) corresponding parametric tests. Thus, less likely identify differences groups really exist. reasonably certain assumptions parametric procedure satisfied, parametric procedure used instead rank-based nonparametric procedure. Many introductory texts suggest , order conduct parametric test, sample size 15 group skewed data outliers.","code":""},{"path":"index.html","id":"multiple-comparisons","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"1.14 Multiple Comparisons","text":"introductory texts, statistical inference often described terms drawing one random sample, performing one significance test, stating appropriate conclusions—analysis done, case closed. However, many situations inference simple. Performing multiple statistical tests data set can create several problems.Using significance level \\(\\alpha\\) = 0.05 (.e., rejecting \\(H_0\\) favor alternative p-value less equal 0.05) helps ensure won’t make wrong decision. words, one time 20 expect incorrectly reject null hypothesis. want 20 tests thesame data set? mean ’re sure wrong least ? , can tell findings incorrect? following activities explore researchers can protect drawing conclusions statistical findings result random chance.","code":""},{"path":"index.html","id":"extended-activitycomparing-car-prices","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"Extended Activity:Comparing Car Prices","text":"","code":""},{"path":"index.html","id":"extended-activity-the-least-significant-differences-method-and-the-bonferroni-method","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"Extended Activity: The Least-Significant Differences Method and the Bonferroni Method","text":"Data set: Car1When significance level controlled individual test, done Question 29, process often called least-significant differences method (LSD). Notice using = 0.05 tests undesirable properties, especially large number tests conducted. 100 independent tests conducted compare multiple groups (really differences), probability incorrectly rejecting least one test 1 - 0.95\\(^{100}\\) = 0.994. Thus, using \\(\\alpha\\)= 0.05 critical value 100 comparisons almost always lead us incorrectly conclude results significantly different.One technique commonly used address problem multiple comparisons called *Bonferroni method. technique protects probability false rejection using cutoff value \\(\\alpha\\)/K, K number comparisons. Question 29, three comparisons (.e.,three hypothesis tests). Thus, cutoff value 0.05/3 = 0.01667 used. words, three comparisons Question 29, Bonferroni method rejects null hypothesis p-value less equal 0.01667. Using least-significant differences method (\\(\\alpha\\) = 0.05), done Question 29, conclude prices Buicks Chevrolets significantly different, using Bonferroni method fail reject three tests.\n","code":""},{"path":"index.html","id":"choosing-a-critical-value","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"Choosing a Critical Value","text":"\\(\\alpha\\)-level represents probability type error. type error can considered false alarm: hypothesis test led us conclude found significant difference one exist. However, important recognize also possible make type II error, means hypothesis\ntest failed detect significant difference one exists. essence, type II error can thought alarm failed go .Notice Bonferroni method used six tests, critical value individual test 0.05/6 = 0.00833. Thus, method often fails detect real differences groups, leaving us open high rate type II error protecting us type errors.Neither least-significant differences Bonferroni method ideal. Caution used techniques, neither technique used numerous comparisons. key recognize benefits limitations technique properly interpret results technique tell us. researchers suggest limiting number tests, using techniques, letting reader decidewhich conclusions draw. techniques commonly used fewer 10 comparisons. However, researcher always decide comparisons test looking data.","code":""},{"path":"index.html","id":"chapter-summary","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"Chapter Summary","text":"chapter described basic concepts behind randomization tests, permutation tests, bootstrap methods, rank-based nonparametric tests. Parametric tests (z-tests, t-tests F-tests) assume data follow known probability distribution use central limit theorem make inferences population. *Nonparametric tests require assumptions distribution population central limit theorem order make inferences population.*null hypothesis, denoted \\(H_0\\), states study nothing creating group differences except random allocation process. research hypothesis called alternative hypothesis denoted \\(H_a\\) (\\(H_1\\)). p-value likelihood observing statistic least extreme one observedfrom sample data null hypothesis true. threshold value, called significance level, denoted Greek letter alpha (\\(\\alpha\\)). study’s p-value less equal significance level, state results statistically significant level \\(\\alpha\\). Exact p-values often difficult calculate, *empirical p-values can often simulated randomization permutation test. empirical p-value become precise number randomizations within simulation study\nincreases.steps randomization test follows:experiment conducted units assigned treatment observed sample statistic calculated (difference group means).Software used simulate random allocation process number times (N iterations).iteration, statistic interest (difference group means) recorded, X \nnumber times statistic iteration exceeds observed statistic \nactual experiment.X/N computed find p-value, proportion times statistic exceeds \nobserved difference.*permutation test general form randomization test. steps tests identical, except permutation tests require random allocation. Randomization tests permutation tests can provide accurate results. tests preferred parametric methods sample size small outliers data set. Since real data sets tend come exactly normal populations, important recognize even p-values parametric tests approximate (typically accurate long sample sizes large enough, data skewed, outliers, data reasonably normal). graph boxplot individual value plot always created determine parametric methods appropriate. Randomization tests gaining popularity require fewer assumptions just powerful parametric tests.Bootstrap methods take many (least 1000) resamples replacement original sample createa bootstrap distribution. bootstrap distribution symmetric unbiased, bootstrap t bootstrap\npercentile confidence intervals can used approximate 100(1 - \\(\\alpha\\))%, confidence intervals.steps creating bootstrap confidence intervals follows:One sample size n taken population statistic interest calculated.Software used take resamples (replacement) size n original sample number \ntimes (N iterations). iteration, statistic interest calculated resample.bootstrap distribution, distribution N resample statistics, used estimate\nshape spread sampling distribution.bootstrap t confidence interval found calculating \\(\\bar{x}\\) \\(\\pm t^*(S^*)\\) \\(S^*\\) standard\ndeviation bootstrap distribution \\(t^*\\) critical value t(n - 1) distribution \n100(1 - \\(\\alpha\\))%, area - t* t*.100(1 - \\(\\alpha\\)), bootstrap percentile confidence interval found taking \\(\\alpha\\) / 2 * 100\npercentile tail bootstrap distribution.Bootstrap confidence intervals based small samples can unreliable. bootstrap t percentile confidence interval may used ,bootstrap distribution appear biased,bootstrap distribution appears normal, andthe bootstrap t percentile confidence intervals similar.Simulation studies can easily extended testing terms, median variance, whereas parametric tests described introductory statistics classes (z-test t-test) restricted testing mean. Simulation studies extremely useful tool can fairly easily used calculate accurate p-values research hypotheses tests appropriate.computationally intensive techniques easily available, rank-based nonparametric tests, suchas Wilcoxon rank sum test Kruskal-Wallis test, commonly used. tests \nrequire assumptions distributions, tend less informative ranks used instead\nactual data. Mann-Whitney test Kruskal-Wallis test assume sample data \nindependent random samples whose distributions shape scale. sample \nKruskal-Wallis test consist least five measurements. Rank-based nonparametric tests tend \nless powerful (less likely identify differences groups) parametric tests (assumptions \nhold) resampling methods. sample sizes small reasons doubt normality\nassumption, rank-based nonparametric tests recommended parametric tests. Randomization tests \npermutation tests typically preferred parametric rank-based tests. p-values often \nreliable, flexible choice parameter tested.One final note caution: Even though possible analyze data variety parametricand nonparametric techniques, statisticians never search around technique provides \nresults looking . Conducting multiple tests data choosing test provides\nsmallest p-value cause results unreliable. possible, determine type analysis \nconducted data collected.","code":""},{"path":"index.html","id":"exercises","chapter":"1 An Introduction to Nonparametric Methods: Schistosomiasis","heading":"Exercises","text":"","code":""},{"path":"making-connections-the-two-sample-t-test-regression-and-anova.html","id":"making-connections-the-two-sample-t-test-regression-and-anova","chapter":"2 Making Connections: The Two-Sample t-Test, Regression, and ANOVA","heading":"2 Making Connections: The Two-Sample t-Test, Regression, and ANOVA","text":" theory, ’s difference theory practice. practice, . -Yogi Berra5\nStatistics courses often teach two-sample t-test, linear regression, analysis variance (ANOVA) distinct approaches analyzing different types data. However, chapter makes connections among three techniques focusing statistical models. Statistical software made easy calculate statistics \\(p\\)-values. without understanding underlying model assumptions, easy draw incorrect conclusions sample data. studies become complex, models become fundamental drawing appropriate conclusions. chapter, simple student experiment involving games several additional studies used following:Compare underlying statistical models two-sample t-test, linear regression, \nANOVADiscuss model assumptions three testsCreate interpret normal probability plotsTransform data order better fit model assumptionsDiscuss mathematical details hypothesis test corresponding confidence interval","code":""},{"path":"making-connections-the-two-sample-t-test-regression-and-anova.html","id":"investigation-do-distracting-colors-influence-the-time-to-complete-a-game","chapter":"2 Making Connections: The Two-Sample t-Test, Regression, and ANOVA","heading":"2.1 Investigation: Do Distracting Colors Influence the Time to Complete a Game?","text":"1935, John Stroop published paper presenting research reaction time undergraduate students identifying ink colors.2 found students took longer time identifying ink colors ink used spell different color. example, word [[[“yellow”]]] printed blue ink, students took longer identify blue ink automatically read word “yellow.” Even though students told identify ink color, automatized behavior reading interfered task slowed reaction time.6 Automatized behaviors behaviors can done automatically without carefully thinking step process. Stroop’s work, demonstrating automatized behaviors can act distracter desired behaviors, well known effect often called Stroop effect.Several students introductory statistics class wanted develop final project test impact distracters. decided conduct study determine students college perform differently distracting color incorporated computerized game. game challenges people place assortment shaped pegs appropriate spaces quickly possible. data collected, students developed clear set procedures.40 students randomly selected college.720 students assigned standard game 20 assigned game color\ndistracter. student researchers flip coin randomly assign subjects treatment. \n20 subjects assigned either group, rest automatically assigned play \ngame.Subjects see picture game rules clearly explained \nplayed game. example games shown Figure 2.1.Subjects play game area similar background noise control \npossible distractions.response variable time seconds participant pressed “start\ngame” button won game.[[[Insert Figure 2.1]]]Figure 2.1 black white image electronic Shapesplosion game\nwithout color distracters. instructions game \nclick drag peg space matching shape.NOTE\nimportant recognize subject study assigned exactly one treatment, either standard game color distracter game. researchers may point paired design (subject assigned treatments) might efficient. However, purposes chapter, study treated students originally designed : study comparing two independent samples.","code":""},{"path":"making-connections-the-two-sample-t-test-regression-and-anova.html","id":"understanding-the-study-design","chapter":"2 Making Connections: The Two-Sample t-Test, Regression, and ANOVA","heading":"2.1.1 Understanding the Study Design","text":"study, identify units, population conclusions can drawn, explanatory variable, response variable.study, identify units, population conclusions can drawn, explanatory variable, response variable.study experiment observational study? Explain.study experiment observational study? Explain.researchers hoped determine distracting colors influenced college students’ response times playing computerized game. Write words symbols appropriate null alternative hypotheses. Let \\(\\mu_1\\) represent true mean response time color group \\(\\mu_2\\) true mean response time standard group. Use two-sided alternative hypothesis question.researchers hoped determine distracting colors influenced college students’ response times playing computerized game. Write words symbols appropriate null alternative hypotheses. Let \\(\\mu_1\\) represent true mean response time color group \\(\\mu_2\\) true mean response time standard group. Use two-sided alternative hypothesis question.Create individual value plot boxplot Games1 data study. Describe graph. example, look groups equal means equal standard deviations? unusual observations data set? Calculate mean standard deviation color distracter responses, \\(\\bar{y}_1\\) \\(s_1\\) , well mean standard deviation standard game responses, \\(\\bar{y}_2\\) \\(s_2\\).Create individual value plot boxplot Games1 data study. Describe graph. example, look groups equal means equal standard deviations? unusual observations data set? Calculate mean standard deviation color distracter responses, \\(\\bar{y}_1\\) \\(s_1\\) , well mean standard deviation standard game responses, \\(\\bar{y}_2\\) \\(s_2\\).","code":""},{"path":"making-connections-the-two-sample-t-test-regression-and-anova.html","id":"the-two-sample-t-test-to-compare-population-means","chapter":"2 Making Connections: The Two-Sample t-Test, Regression, and ANOVA","heading":"2.2 The Two-Sample t-Test to Compare Population Means","text":"","code":""},{"path":"making-connections-the-two-sample-t-test-regression-and-anova.html","id":"the-statistical-model","chapter":"2 Making Connections: The Two-Sample t-Test, Regression, and ANOVA","heading":"2.2.1 The Statistical Model","text":"Generally, statistical models following form:\nobserved value = mean response + random error\nstatistical model describes observed value data set sum mean response subgroup interest (often called group mean) random error term. mean response fixed group, random error term used model uncertainty individual outcome. random error term individual outcome predicted, long run regular pattern can modeled distribution (normal distribution).key question study whether two types games different average completion times. two-sample t-test starts assumption two group means equal. often written null hypothesis \\(H_0 : \\mu_1 - \\mu_2 = 0\\) , equivalently, \\(H_0 : \\mu_1 = \\mu_2\\).underlying model used two-sample t-test designed account two group means (\\(\\mu_1\\) \\(\\mu_2\\)) random error. statistical model first population, color distracter group, :[[[Eq 2.1]]]j used represent observation sample first population. example, \\(y_{1, 9}\\) represents 9th observation first group (color distracter group). data set, 20 observations taken first population; thus, \\(n_1\\) = 20.model states color distracter game expected centered constant value \\(\\mu_1\\) . addition, observation expected variability (random error) typically modeled normal distribution mean equal zero fixed variance s2. Similarly, observation second group, standard game, can modeled sum \\(\\mu_2\\) plus random error term, \\(\\epsilon_{2, j}\\):\\(y_{2, j} = \\mu_2 + \\epsilon_{2, j}\\) \\(j = 1, 2, ... ,n_2\\)\\(n_2 = 20\\), \\(\\mu_2\\) mean standard group, \\(\\epsilon_{2, j}\\) random variables (typically \nnormal distribution) mean equal zero variance \\(\\sigma^2\\) . Often, statistical model succinctly written :\\(y_{, j} = \\mu_i + \\epsilon_{, j}\\) \\(j = 1, 2\\) \\(j = 1, 2, ... ,n_2\\) \\(\\epsilon_{, j} \\sim N(0,\\sigma^2)\\)MATHMATICAL NOTE\nmay recall introductory statistics course adding constant random variable population change shape spread population. Since mean response (\\(\\mu_i\\)) fixed (.e., constant value), Equation (2.1) can used show \\(y_{, j} \\sim N(\\mu_i,\\sigma^2)\\).model one assumption may made previously conducting two-sample t-test. Equation (2.1) states \\(\\epsilon_{, j}\\) come normally distributed population mean zero \nvariance s2 . called equal variance assumption. introductory statistics courses discuss two-sample t-test require equal variance assumption. equal variance assumption \nmade makes sense experiment, data support (\\(s_1\\) close \\(s_2\\)), allows direct comparison ANOVA regression models.Equation (2.1), mean response model population mean (\\(\\mu_1\\) \\(\\mu_2\\)). Just sample mean, \\(\\bar{y}_i\\), used estimate population means, \\(\\mu_i\\), residuals used estimate random error terms. Residuals difference observed response estimated mean response. example, random error term \\(\\epsilon_{1, 12} = + \\bar{y}_{1, 12} - \\mu_1\\) estimated \\(\\hat{\\epsilon}_{1, 12} = + \\bar{y}_{1, 12} - \\bar{y}_1\\).NOTE\nstatistic mathematical function sample data. Parameters actual population values known unless entire population sampled. mean response based population parameters. sample data set used, know population parameters. Sample statistics (sample mean, \\(\\bar{y}\\), sample standard deviation, \\(s\\)) used estimate population parameters (\\(\\mu\\) \\(\\sigma\\)). Statisticians often use hat top parameter represent estimate parameter. example, estimate population standard deviation written \\(s = \\hat{\\sigma}\\) , estimate mean written \\(\\bar{y}_1 = \\hat{\\mu}_1\\) \\(\\bar{y}_2 = \\hat{\\mu}_2\\).","code":""},{"path":"making-connections-the-two-sample-t-test-regression-and-anova.html","id":"statistical-models-for-the-two-sample-t-test","chapter":"2 Making Connections: The Two-Sample t-Test, Regression, and ANOVA","heading":"2.2.2 Statistical Models for the Two-Sample t-Test","text":"Assume two small populations can written \n\\(y_{1,1} = 15\\), \\(y_{1,2} = 17\\), \\(y_{1,3} = 16\\), \\(y_{2,1} = 11\\), \\(y_{2,2} = 9\\), \\(y_{2,3} = 10\\). Find \\(\\mu_1\\), \\(\\mu_2\\), \\(\\epsilon_{1, 1}\\), \\(\\epsilon_{1, 3}\\), \\(\\epsilon_{2, 1}\\).Notice double subscripts observed responses: \\(y_{1,1}\\) read “y one one.” first subscript tells us observation first group, second subscript tells us observation number. example, \\(y_{1,j}\\) jth observation first group.Use game study data file Games1 identify \\(n_1\\), \\(n_2\\), \\(y_{1,12}\\) , \\(y_{2,12}\\) , \\(\\epsilon_{1, 12}\\), \\(\\epsilon_{2, 12}\\),\n\\(y_{1,12}\\) represents 12th observation group 1 (color distracter group). Note since sample, population, know \\(\\mu_1\\) \\(\\mu_2\\) , can estimate \\(\\bar{y}_1 = \\hat{\\mu}_1\\) \\(\\bar{y}_2 = \\hat{\\mu}_2\\).","code":""},{"path":"making-connections-the-two-sample-t-test-regression-and-anova.html","id":"model-assumptions-for-the-two-sample-t-test","chapter":"2 Making Connections: The Two-Sample t-Test, Regression, and ANOVA","heading":"2.2.3 Model Assumptions for the Two-Sample t-Test","text":"Several implicit assumptions built model two-sample t-test shown Equation (2.1):Constant parameters: population values model (\\(\\mu_1\\) , \\(\\mu_2\\) , \\(\\sigma\\)) change throughout study.Additive terms: model described Equation (2.1) shows observed responses sum parameters error terms. example, considering models \n\\(y_{, j} = \\mu_i * \\epsilon_{, j}\\) .\\(\\epsilon_{, j} \\sim N(0,\\sigma^2)\\). assumption many key components:error terms independent identically distributed (iid).error terms follow normal probability distribution.error terms mean zero. implies average several observed values tend close true mean. essence, systematic bias error terms.population variance \\(\\sigma^2\\) groups (color distracter standard games) tested.first assumption tells us mean response. parameter estimate (\\(\\bar{y}_i\\)) meaningful true parameter value (\\(\\mu_i\\)) constant throughout study. second assumption simply states types models building. later chapters complex models, discuss use residual plots determine model appropriate. chapter, focus assumptions error termsMATHMATICAL NOTE\nlater chapters, show curved pattern residual versus fit plot suggests additive model may appropriate. example, two fitted values (.e., expected values), see curved patterns. additive assumption violated, residual plots may also indicate different standard deviations, nonnormal distribution, lack independence. Transforming data new scale can often make additivity assumption (several assumptions) appropriate.statistical model described Equation (2.1) assumes \\(\\epsilon_{, j}\\) modeled independent identically distributed (iid) random variables. independent error term assumption states relationship one observation next. example, knowing 8th subject group played game quickly average provide information whether 7th 9th person group average.identically distributed assumption states error assumed come population distribution. Thus, subject particular group population. error term\nbased particular observation comes different population, two-sample t-test valid. example, elementary school students may different expected completion times Shapesplosion game college students. inappropriate include younger students study population assumed college studentsModel assumptions residuals always checked plots data. extended activities describe normality tests detail, situations simple graph residuals \nsuffice. two sample t-test actually requires sample means (\\(\\bar{y}_{,j}\\)) normally distributed. central limit theorem allows us assume true group sample sizes similar large (\\(n_1 \\ge 15\\) \\(n_2 \\ge 15\\)) appear extreme skewness outliers residuals.Since residuals defined difference observed value corresponding group mean, always sum zero. Thus, check residuals determine whether error\nterms centered zero. assumption error terms centered zero really stating sources variability may biasing results. essence, difference two population means explained mean response.check assumption two populations variance, informal test can used. ratio sample standard deviations less 2, can proceed analysis.8Informal Test Equal Variances[[[ ]]]enough evidence conclude population variances different.Several key observations made individual value plot shown Figure 2.2:mean completion time higher color distracter group standard group.Neither group appears clear outliers, skewness, large gaps.spread (variance) two groups appears similar.Key Concept\nEvery statistical hypothesis test basic underlying conditions need checked valid conclusions can drawn.[[[]]]\nFigure 2.2 Individual value plot data color distracter standard games.","code":""},{"path":"making-connections-the-two-sample-t-test-regression-and-anova.html","id":"checking-assumptions-for-the-t-test","chapter":"2 Making Connections: The Two-Sample t-Test, Regression, and ANOVA","heading":"Checking Assumptions for the t-Test","text":"Calculate residuals Games1 data. Plot histogram residuals (create normal probability plot residuals). residuals appear somewhat normally distributed?Use informal test determine equal variance assumption appropriate study.variable StudentID represents order games played. Plot residuals versus order data determine patterns exist may indicate observations \nindependent.Use statistical software conduct two-sample t-test (assuming equal variances) find \\(p\\)-value corresponding statistic. addition, use software calculate 95% confidence interval \ndifference two means (\\(\\mu_1\\) - \\(\\mu_2\\) ). Equation (2.7) extended activities provide details conducting calculations hand. H0 : \\(\\mu_1\\) = \\(\\mu_2\\) true, \\(p\\)-value states likely random chance alone create difference two sample means (\\(\\bar{y}_1 - \\bar{y}_2\\)) least large one observed. Based \\(p\\), can conclude two types \ngames?","code":""},{"path":"making-connections-the-two-sample-t-test-regression-and-anova.html","id":"the-regression-model-to-compare-population-means","chapter":"2 Making Connections: The Two-Sample t-Test, Regression, and ANOVA","heading":"2.3 The Regression Model to Compare Population Means","text":"","code":""},{"path":"making-connections-the-two-sample-t-test-regression-and-anova.html","id":"the-linear-regression-model","chapter":"2 Making Connections: The Two-Sample t-Test, Regression, and ANOVA","heading":"2.3.1 The Linear Regression Model","text":"simple linear regression model discussed introductory statistics courses typically followingcform:\\[\\begin{equation}\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i ~\\text{ } = 1, 2, ... , n ~\\text{ } \\epsilon_i \\sim N(0,\\sigma^2)\n\\tag{2.2}\n\\end{equation}\\][[[add eq number (2.2)]]]simple linear regression model straight-line regression model single explanatory variable single response variable. linear regression model, mean response (\\(\\beta_0 + \\beta_1x_i\\)) function two parameters, \\(\\beta_0\\) \\(\\beta_1\\), explanatory variable, \\(x\\). random error terms, \\(\\epsilon_i\\), assumed independent follow normal distribution mean zero variance \\(\\sigma^2\\).Equation (2.1), used double subscripts: \\(= 1, 2\\) used show two distinct groups \\(j = 1, 2, ... , n_i\\) used identify \\(n_1 = n_2 = 20\\) items within two groups. regression model, one set subscripts: \\(= 1, 2, ..., n\\), \\(n = 40 = n_1 + n_2\\). Instead two distinct means model (\\(\\mu_1\\) \\(\\mu_2\\)), two-sample t-test, one regression model parameters, \\(\\beta_0\\) \\(\\beta_1\\), fixed. categorical explanatory variable, \\(x\\), indicates game type.procedure commonly used incorporate categorical explanatory variables, game type, regression model define indicator variables, also called dummy variables, take role x variable model. Creating dummy variables process mapping column categorical data 0 1 data. example, indicator variable value 1 every observation color distracter game 0 every observation standard game. statistical software packages command automatically creating dummy variables.NOTE\nTypically indicator variable created category. Thus, indicator variable called Color equal 1 color distracter game 0 otherwise another indicator variable called Standard equal 1 standard game 0 categories. Notice complete redundancy two indicator variables: Knowing value Color variable automatically tells us value Standard variable subject. Thus, one indicator variables needed model. Although study two categories games (color standard), common categorical explanatory variable two categories. Chapter 3 provides opportunity use indicator variables multiple categories.Key Concept\nIndicator variables can created incorporate categorical explanatory variables regression model.","code":""},{"path":"making-connections-the-two-sample-t-test-regression-and-anova.html","id":"calculating-a-regression-model-and-hypothesis-test-for-the-slope","chapter":"2 Making Connections: The Two-Sample t-Test, Regression, and ANOVA","heading":"Calculating a Regression Model and Hypothesis Test for the Slope","text":"Use software instructions Games1 data create indicator variables \\(x = 1\\) represents color distracter game \\(x = 0\\) represents standard game. Develop regression model using Time response indicator variable explanatory variable.Use software instructions Games1 data create indicator variables \\(x = 1\\) represents color distracter game \\(x = 0\\) represents standard game. Develop regression model using Time response indicator variable explanatory variable.Use statistical software calculate t-statistic \\(p\\)-value hypothesis tests \\(H_0 : \\beta_1 = 0\\) versus \\(H_1 : \\beta_1 \\ne 0\\). addition, construct 95% confidence interval \\(\\beta_1\\) . Based statistics, can conclude coefficient, \\(\\beta_1\\), significantly different zero? Details calculating statistics hand provided extended activities.Use statistical software calculate t-statistic \\(p\\)-value hypothesis tests \\(H_0 : \\beta_1 = 0\\) versus \\(H_1 : \\beta_1 \\ne 0\\). addition, construct 95% confidence interval \\(\\beta_1\\) . Based statistics, can conclude coefficient, \\(\\beta_1\\), significantly different zero? Details calculating statistics hand provided extended activities.Repeat two previous questions, use indicator variable \\(x = 1\\) represents standard game \\(x = 0\\) represents color distracter game. Compare regression line, hypothesis test, \\(p\\)-value previous questions. two categories (color distracter standard), choice indicator variable impact conclusions? ?Repeat two previous questions, use indicator variable \\(x = 1\\) represents standard game \\(x = 0\\) represents color distracter game. Compare regression line, hypothesis test, \\(p\\)-value previous questions. two categories (color distracter standard), choice indicator variable impact conclusions? ?previous questions, assigned \\(x\\) dummy variable indicates type game. Notice mean response still constant (nonrandom) value two game categories. \nwords, \\(x = 1\\) mean response fixed value, \\(x = 0\\) mean response fixed value. addition, “slope” coefficient (\\(\\beta_1\\)) can considered estimate average amount response variable change standard game (\\(x = 0\\)) color distracter game (\\(x = 1\\)).Although notation changed, regression model model used two-sample t-test mathematically equivalent. subject color distracter group, mean response \\(\\mu_1\\) t-test mean response sets \\(x = 1\\) regression model. Thus,\\(\\mu_1 = \\beta_0 + \\beta_1(1)  = \\beta_0 + \\beta_1\\)\n[[[add eq(2.3)]]]subject standard group, mean response \\(\\mu_2\\) t-test mean response sets \\(x = 0\\) regression. Thus,\\(\\mu_2 = \\beta_0 + \\beta_1(0)  = \\beta_0\\)\n[[[add eq(2.4)]]]Equations (2.3) (2.4) can combined show relationship two-sample t-test regression hypotheses.\\(\\mu_1 - \\mu_2 = (\\beta_0 + \\beta_1) -  \\beta_0 = \\beta_1\\)\n[[[eq (2.5)]]]Thus, stating \\(\\mu_1 - \\mu_2 = 0\\) equivalent stating \\(\\beta_1 = 0\\)Key Concept\ntesting difference two population means, testing null hypothesis \\(H_0 : \\beta_1 = 0\\) regression model equivalent testing two-sample t-test hypothesis \\(H_0 : \\mu_1 - \\mu_2 = 0\\) using equal variance assumption.","code":""},{"path":"making-connections-the-two-sample-t-test-regression-and-anova.html","id":"model-assumptions-for-regression","chapter":"2 Making Connections: The Two-Sample t-Test, Regression, and ANOVA","heading":"2.3.2 Model Assumptions for Regression","text":"distributional assumptions needed create estimates b0 b1 , necessary check model assumptions conducting hypothesis test b1. Just two-sample t-test, model assumes parameters b0 , b1 , \\(\\sigma^2\\) constant. addition, Equation (2.2) shows model consists mean response plus error term. regression model also assumes \\(\\epsilon_i \\sim N(0,\\sigma^2)\\).expression represents following four assumptions:error terms independent identically distributed (iid).error terms follow normal probability distribution.error terms mean zero .error terms regression model assumed come single population variance \\(\\sigma^2\\) (.e., variance depend \\(x\\)).regression, assumptions error terms also checked residual plots. , \\(y_i\\) represents observed response \\(\\hat{y}_i = b_0 + b_1x_i\\) represents estimated mean response. residuals simply observed value minus estimated value: \\(\\hat{\\epsilon}_i = y_i - \\hat{y}_i\\)Figure 2.3 shows histogram residuals plot residuals type game. histogram shows residuals approximately follow shape normal distribution. residual versus game type graph shows obvious outliers spread groups roughly equivalent. Since residuals just mean response subtracted observed value, center residual plots shifted zero. However, spread residual versus game plot identical spread individual value plot Figure 2.2.[[[Fig 2.3]]]\nFigure 2.3 Histogram residuals plot residuals versus color.Key Concept\nassumptions needed error terms calculate estimates (\\(b_1 = \\hat{\\beta}_1\\) \\(b_0 = \\hat{\\beta}_0\\)) slope intercept regression line. estimates simply well-known mathematical calculations. However, model assumptions satisfied order properly conduct hypothesis test create confidence interval \\(\\beta_1\\).","code":""},{"path":"making-connections-the-two-sample-t-test-regression-and-anova.html","id":"checking-model-assumptions","chapter":"2 Making Connections: The Two-Sample t-Test, Regression, and ANOVA","heading":"Checking Model Assumptions","text":"Calculate residuals regression line Question 11. Plot histogram residuals (create normal probability plot residuals). addition, create residual versus order plot use informal test determine equal variance assumption appropriate study. Compare plots residual plots created two-sample t-test. graphs similar?Calculate residuals regression line Question 11. Plot histogram residuals (create normal probability plot residuals). addition, create residual versus order plot use informal test determine equal variance assumption appropriate study. Compare plots residual plots created two-sample t-test. graphs similar?Create scatterplot regression line Question 11. Use graph give interpretation slope y-intercept, b1 b0, context game study.Create scatterplot regression line Question 11. Use graph give interpretation slope y-intercept, b1 b0, context game study.","code":""},{"path":"making-connections-the-two-sample-t-test-regression-and-anova.html","id":"anova-to-compare-population-means","chapter":"2 Making Connections: The Two-Sample t-Test, Regression, and ANOVA","heading":"2.4 ANOVA to Compare Population Means","text":"term ANOVA acronym ANalysis VAriance. ANOVA models often describe categorical explanatory variables terms factors levels. explanatory variable, also called factor, \nstudy type game; two conditions, two levels factor, color distracter standard.","code":""},{"path":"making-connections-the-two-sample-t-test-regression-and-anova.html","id":"the-anova-model","chapter":"2 Making Connections: The Two-Sample t-Test, Regression, and ANOVA","heading":"2.4.1 The ANOVA Model","text":"ANOVA model game study can written \\(y_{,j} = \\mu + \\alpha_i + \\epsilon_{,j}\\) \\(= 1, 2\\) \\(j = 1, 2, ... , n\\) \\(\\epsilon_{,j} \\sim N(0,\\sigma^2)\\)\n[[[add eq number 2.6]]]mean response ANOVA model \\(\\mu + \\alpha_1\\) color distracter group \\(\\mu + \\alpha_2\\) standard group, \\(\\mu\\) mean completion times study. overall mean often called grand mean benchmark value; \\(\\alpha_1\\) effect, main effect, color distracter group. Effects measure differences group means. effect \\(\\alpha_1\\) represents change response grand mean color distracter group mean.9To summarize, symbols model represent:\\(y_{,j}\\): observed completion time subject j group \\(\\mu\\): overall mean (benchmark value)\\(\\alpha_i\\): effect group (= 1, 2)\\(\\epsilon_{,j}\\): error jth subject ( \\(j = 1, 2, ... , 20\\)) ith group (\\(= 1, 2\\))Although notation varies, mean response ANOVA model mathematically equivalent mean response t-test.\\(\\mu_1 = \\mu + \\alpha_1\\): population mean color distracter games\\(\\mu_2 = \\mu + \\alpha_2\\): population mean standard games","code":""},{"path":"making-connections-the-two-sample-t-test-regression-and-anova.html","id":"the-anova-model-1","chapter":"2 Making Connections: The Two-Sample t-Test, Regression, and ANOVA","heading":"The ANOVA Model","text":"Explain (use equations show) ANOVA hypothesis H0 : \\(\\alpha_1\\) = \\(\\alpha_2\\) equivalent two- sample t-test hypothesis H0 : \\(\\mu_1\\) = \\(\\mu_2\\) .\n*text m always considered overall mean data. Also throughout chapter, always assuming balanced data.Key Concept\nANOVA model, appearance describing two means (\\(\\mu_1\\) \\(\\mu_2\\)) using three parameters (\\(\\mu\\), \\(\\alpha_1\\) , \\(\\alpha_2\\)). Since can shown \\(\\alpha_2 = -\\alpha_1\\), actually just two parameters (\\(\\mu\\) \\(alpha_1\\)) estimated. Thus, null hypothesis stating effect size can also written \\(H_0: \\alpha_1 = \\alpha_2 = 0\\) \\(H_0: \\mu_1 = \\mu_2 = \\mu\\).Write proper ANOVA model [provide appropriate ij subscripts Equation (2.6)] observation representing 3rd subject color distracter group. Also give notation observation representing 20th subject standard group.Write proper ANOVA model [provide appropriate ij subscripts Equation (2.6)] observation representing 3rd subject color distracter group. Also give notation observation representing 20th subject standard group.doesn’t \\(\\mu\\) subscript ANOVA model?doesn’t \\(\\mu\\) subscript ANOVA model?data collected, averages three meaningful groupings data can calculated. following mathematical notation often used represent calculated sample averages:\\(\\bar{y}_{..}\\): grand mean (overall average combined results)\\(\\bar{y}_{1.}\\): average color distracter game sample results\\(\\bar{y}_{2.}\\): average standard game sample resultsNote\nThroughout chapter, \\(\\bar{y}_{1.} = \\bar{y}_{1}\\) \\(\\bar{y}_{2.} = \\bar{y}_{2}\\). dot notation often used complex models\nindicate average taken values subscript. example, \\(bar{y}_{2.}\\) averages \n\\(j = 1, 2, 3, ... , n_2\\), observations standard game sample results.effect color distracter game, \\(\\alpha_1\\) , can estimated \\(\\hat{\\alpha}_1 = \\bar{y}_{1.} - \\bar{y}_{..}\\). Similarly, \\(\\hat{\\alpha}_2 = \\bar{y}_{2.} - \\bar{y}_{..}\\)\nestimates standard game effect, \\(\\alpha_2\\). regression two-sample t-test, residual \\(\\hat{\\epsilon}_{ij}\\) \ndifference observed value corresponding mean response.\\[\n\\begin{aligned}\n\\hat{\\epsilon}_{ij}  \n  &= \\text{observed} - (\\text{grand mean} + \\text{effect group}_i)\\\\\n  &= y_{,j} - [\\bar{y}_{..} + \\hat{\\alpha}_i]\\\\\n  &= y_{,j} - [\\bar{y}_{..} + (\\bar{y}_{.} - \\bar{y}_{..})]\\\\\n  &= y_{,j} - \\bar{y}_{.}\\\\\n\\end{aligned}  \n\\]","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"multiple-regression-how-much-is-your-car-worth","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3 Multiple Regression: How Much Is Your Car Worth?","text":"Essentially, models wrong; useful.\n-George E. P. Box10Multiple regression arguably single important method statistics.\nRegression models widely used many disciplines. addition, good understanding\nregression essential understanding many , sophisticated\nstatistical methods.chapter consists set activities enable build multivariate\nregression model. model used describe relationship retail price 2005 used GM cars various car characteristics, mileage, make, model, presence absence cruise control, engine size. set activities chapter allows work entire process model building assessment, includingApplying variable selection techniquesUsing residual plots check violations model assumptions, heteroskedasticity, outliers, autocorrelation, nonnormality distributed errorsTransforming data better fit model assumptionsUnderstanding impact correlated explanatory variablesIncorporating categorical explanatory variables regression modelApplying F-tests multiple regression","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"investigation-investigation-how-can-we-build-a-model-to-estimate-used-car-prices","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.1 Investigation: Investigation: How Can We Build a Model to Estimate Used Car Prices?","text":"ever browsed car dealership observed sticker prices vehicles? ever seriously considered purchasing vehicle, can probably relate difficulty determining whether vehicle good deal . dealerships willing negotiate sale price, can know much negotiate? novices (like author), helpful refer outside\npricing source, Kelley Blue Book, agreeing purchase price.80 years, Kelley Blue Book resource accurate vehicle pricing. company’s Website, http://www.kbb.com, provides free online resource anyone can input several car characteristics (age, mileage, make, model, condition) quickly receive good estimate retail price.chapter, use relatively small subset Kelley Blue Book database describe association several explanatory variables (car characteristics) retail value car. developing complex multiple regression model several variables, let’s start quick review simple linear regression model asking question: cars lower mileage worth ? seems reasonable\nexpect see relationship mileage (number miles car driven) retail value. data set Cars contains make, model, equipment, mileage, Kelley Blue Book suggested retail price several used 2005 GM cars.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"a-simple-linear-regression-model","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.1.1 A Simple Linear Regression Model","text":"Data set: Cars\n1. Produce scatterplot Cars data set display relationship mileage (Mileage) suggested retail price (Price). scatterplot show strong relationship Mileage Price?Calculate least squares regression line, \\(Price = b_0 + b_1(Mileage)\\). Report regression model, \\(R^2\\) value, correlation coefficient, t-statistics, \\(p\\)-values estimated model coefficients (intercept slope). Based statistics, can conclude Mileage strong indicator Price? Explain reasoning sentences.Calculate least squares regression line, \\(Price = b_0 + b_1(Mileage)\\). Report regression model, \\(R^2\\) value, correlation coefficient, t-statistics, \\(p\\)-values estimated model coefficients (intercept slope). Based statistics, can conclude Mileage strong indicator Price? Explain reasoning sentences.first car data set Buick Century 8221 miles. Calculate residual value car (observed retail price minus expected price calculated regression line).first car data set Buick Century 8221 miles. Calculate residual value car (observed retail price minus expected price calculated regression line).MATHMATICAL NOTE regression equation form \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\) hypothesis test slope regression equation (b1) similar t-tests discussed introductory textbooks. (Mathematical details hypothesis test described Chapter 2.) test null hypothesis slope coefficient zero (\\(H_0 : \\beta_1 = 0\\) versus \\(H_1 : \\beta_1 \\ne 0\\)), calculate following test statistic:b1 estimated slope calculated data \\(\\hat{\\sigma}_{b_1}\\) estimate standard deviation b1. Probability theory can used prove regression model assumptions true, t-statistic Equation (3.1) follows t-distribution n - 2 degrees freedom. sample statistic, \\(b_1\\), far away \\(b_1\\) = 0 relative estimated standard deviation, t-statistic large corresponding \\(p\\)-value small.[[[end note ]]]t-statistic slope coefficient indicates Mileage important variable. However, \\(R^2\\) value (percentage variation explained regression line) indicates regression line useful predicting retail price. (review \\(R^2\\) value given extended activities.) always case statistics, need visualize data rather focus solely \\(p\\)-value. Figure 3.1 shows expected price decreases mileage increases, observed points appear close regression line. Thus, seems reasonable including additional explanatory variables regression model might help better explain variation retail price.[[[Fig3.1]]]\nFigure 3.1 Scatterplot least squares regression model: \\(Price = 24,765 - 0.1725(Mileage)\\). regression line shows additional mile car driven, expected price car decreases 17 cents. However, many points close regression line, indicating expected price accurate estimate actual observed price.chapter, build linear combination explanatory variables explains response variable, retail price. work chapter, find one technique, “recipe,” give best model. fact, come see isn’t just one “best” model data.Unlike mathematics classes, every student expected submit one right answer assignment, expected final regression models submitted various students least slightly different. single “best” model may exist data, certainly many bad models avoided. chapter focuses understanding process developing statistical\nmodel. doesn’t matter developing regression model economics, psychology, sociology, engineering—common key questions processes evaluated final model submitted.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"goals-of-multiple-regression","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.2 Goals of Multiple Regression","text":"important note multiple regression analysis can used serve different goals. goals influence type analysis conducted. common goals multiple regression describe, predict, confirm.Describe: model may developed describe relationship multiple explanatory variables response variable.Predict: regression model may used generalize observations outside sample. Just simple linear regression, explanatory variables within range sample data predict future responses.Confirm: Theories often developed variables combination variables included model. example, mileage useful predicting retail price? Inferential techniques can used test association explanatory variables response just due chance. Theory may also predict type relationship exists, “cars lower mileage worth .” specific theories can also tested, “retail price decreases linearly mileage.”goal developing multiple regression model description prediction, primary issue often determining variables include model (leave ). potential explanatory variables can included regression model, often results cumbersome model difficult understand. hand, model includes one two explanatory variables, model Figure 3.1, may much less accurate complex model. tension finding simple model finding model best explains response makes difficult find “best” model. process finding reasonable mix, provides relatively simple linear combination explanatory variables, often resembles exploratory artistic process much formulaic recipe.Including redundant unnecessary variables creates unwieldy model also can lead test statistics (conclusions corresponding hypothesis tests) less reliable. explanatory variables highly correlated, effects model estimated imprecision. imprecision leads larger standard errors can lead insignificant test results individual variables can important model. Failing include relevant variable can result biased estimates regression coefficients invalid t-statistics, especially excluded variable highly significant excluded variable correlated variables also model.11","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"variable-selection-techniques-to-describe-or-predict-a-response","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.3 Variable Selection Techniques to Describe or Predict a Response","text":"objective describe relationship predict new response variables, variable selection techniques useful determining explanatory variables model. investigation, consider response suggested retail price Kelley Blue Book (Price variable data). may initially believe following relevant potential explanatory variables:Make (Buick, Cadillac, Chevrolet, Pontiac, SAAB, Saturn)Model (specific car previously listed Make)Trim (specific type Model)Type (Sedan, Coupe, Hatchback, Convertible, Wagon)Cyl (number cylinders: 4, 6, 8)Liter (measure engine size)Doors (number doors: 2, 4)Cruise (1 = cruise control, 0 = cruise control)Sound (1 = upgraded speakers, 0 = standard speakers)Leather (1 = leather seats, 0 = leather seats)Mileage (number miles car driven)","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"stepwise-regression","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.3.1 Stepwise Regression","text":"large number variables available, stepwise regression iterative technique historically used identify key variables include regression model. example, forward stepwise regression begins fitting several single-predictor regression models response; one regression model developed individual explanatory variable. single explanatory variable (call \\(X_1\\)) best explains response (highest \\(R^2\\) value) selected model.12In next step, possible regression models using \\(X_1\\) exactly one explanatory variable calculated. among two-variable models, regression model best explains response used identify \\(X_2\\). first second explanatory variables, \\(X_1\\) \\(X_2\\), selected, process repeated find X3. continues including additional variables model longer\ngreatly improves model’s ability describe response.13Backward stepwise regression similar forward stepwise regression except starts potential explanatory variables model. One one, technique removes variables make smallest contribution model fit “best” model found.sequential techniques easy implement, usually better approaches finding regression model. Sequential techniques tendency include many variables timeso metimes eliminate important variables.3 improvements technology, statisticians prefer use “global” techniques (best subset methods), compare possible subsets explanatory variables.Note Later sections show explanatory variables highly correlated, sequential procedures often leave variables explain (highly correlated ) response. addition, sequential procedures involve numerous iterations iteration involves hypothesis tests significanceof coefficients. Remember multiple comparison problem, \\(\\alpha\\)-level 0.05 means 5% chance irrelevant variable found significant may inappropriately determined important model.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"selecting-the-best-subset-of-predictors","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.3.2 Selecting the “Best Subset” of Predictors","text":"researcher must balance increasing \\(R^2\\) keeping model simple. models number parameters compared, model highest \\(R^2\\) value typically selected. larger \\(R^2\\) value indicates variation response variable explained model. However, \\(R^2\\) never decreases another explanatory variable added. Thus, techniques suggested comparing models different numbers explanatory variables.Statistics adjusted \\(R^2\\), Mallows’ \\(C_p\\), Akaike’s Bayes’ information criteria used determine “best” model. statistics includes penalty including many terms. words, two models equal \\(R^2\\) values, statistics select model fewer terms. Best subsets techniques use several statistics simultaneously compare several regression models number predictors.coefficient determination, \\(R^2\\) percentage variation response variable explained regression line.sum squared residuals \\({\\sum_{=1}^{n}({y}_i - \\hat{y})^2}\\) small compared total spread responses \\({\\sum_{=1}^{n}({y}_i - \\bar{y})^2}\\), \\(R^2\\) close one. \\(R^2\\) = 1 indicates regression model perfectly fits data.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"comparing-variable-selection-techniques","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"Comparing Variable Selection Techniques","text":"Dataset: Cars\n4. Use Cars data conduct stepwise regression analysis.Calculate seven regression models, one following explanatory variables: Cy1, Liter, Doors, Cruise, Sound, Leather, Mileage. Identify explanatory variable corresponds model largest \\(R^2\\) value. Call variable \\(X_1\\).Calculate seven regression models, one following explanatory variables: Cy1, Liter, Doors, Cruise, Sound, Leather, Mileage. Identify explanatory variable corresponds model largest \\(R^2\\) value. Call variable \\(X_1\\).Calculate six regression models. model two explanatory variables, \\(X_1\\) one six explanatory variables. Find two-variable model highest \\(R^2\\) value. much \\(R^2\\) improve second variable included?Calculate six regression models. model two explanatory variables, \\(X_1\\) one six explanatory variables. Find two-variable model highest \\(R^2\\) value. much \\(R^2\\) improve second variable included?Instead continuing process identify variables, use software instructions provided conduct stepwise regression analysis. List explanatory variables model suggested stepwise regression procedure.Instead continuing process identify variables, use software instructions provided conduct stepwise regression analysis. List explanatory variables model suggested stepwise regression procedure.Use software instructions provided develop model using best subsets techniques. Notice stepwise regression simply states model use, best subsets provides much information requires user choose many variables include model. general, statisticians select models relatively low Cp, large \\(R^2\\), relatively small number explanatory variables. (rare statistics suggest model. Thus, researcher much choose model based goals. extended activities provide additional details\nstatistics.) Based output best subsets, explanatory variables included regression model?Use software instructions provided develop model using best subsets techniques. Notice stepwise regression simply states model use, best subsets provides much information requires user choose many variables include model. general, statisticians select models relatively low Cp, large \\(R^2\\), relatively small number explanatory variables. (rare statistics suggest model. Thus, researcher much choose model based goals. extended activities provide additional details\nstatistics.) Based output best subsets, explanatory variables included regression model?Compare regression models Questions 4 5.Compare regression models Questions 4 5.different explanatory variables considered important?different explanatory variables considered important?stepwise regression Question 4 provide indication Liter useful predicting Price? best subsets output Question 5 provide indication Liter might useful predicting Price? Explain best subsets techniques can informative sequential techniques.stepwise regression Question 4 provide indication Liter useful predicting Price? best subsets output Question 5 provide indication Liter might useful predicting Price? Explain best subsets techniques can informative sequential techniques.Neither sequential best subsets techniques guarantee best model. Arbitrarily using slightly different criteria produce different models. Best subset methods allow us compare models specific number predictors, models predictors always include terms smaller models. Thus, often difficult interpret importance coefficients model.Variable selection techniques useful providing high \\(R^2\\) value limiting number variables. goal develop model describe predict response, concerned significance explanatory variable, well overall model fits.goal involves confirming theory, iterative techniques recommended. Confirming theory similar hypothesis testing. Iterative variable selection techniques test variable combination variables several times, thus \\(p\\)-values reliable. stated significance level t-statistic valid data used single test. multiple tests conducted find best equation, actual significance level test individual component invalid.Key Concept\nvariables selected iterative techniques, hypothesis tests used determine significance terms.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"checking-model-assumptions-1","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.4 Checking Model Assumptions","text":"simple linear regression model discussed introductory statistics courses typically following form:linear regression model, mean response\\(\\beta_0 + \\beta_1x_i\\) linear function explanatory variable, \\(x\\). multiple linear regression model similar form. key difference now terms included model.chapter, \\(p\\) represents number parameters regression model \\(\\beta_0 + \\beta_1x_{1,} + {...} + \\beta_{p-1}x_{p-1,}\\) \\(n\\) total number observations data. chapter, make following assumptions\nregression model:\n- model parameters 0 + 1x{1,} + {…} + {p-1}{x_{p-1,} \\(\\sigma^2\\) constant.\n- term model additive.\n- error terms regression model independent sampled single population (identically distributed). often abbreviated iid.\n- error terms follow normal probability distribution centered zero fixed variance, \\(\\sigma^2\\). assumption denoted \\(\\epsilon_i \\sim N(0,\\sigma^2)\\) $ = 1, {…}, ~ n$.Regression assumptions error terms generally checked looking residuals data: \\(({y}_i - \\hat{y}_i)\\). , \\(y_i\\) observed responses \\(\\hat{y}_i\\) estimated responses calculated regression model. Instead formal hypothesis tests, plots used visually assess whether assumptions hold. theory methods simplest scatterplot residuals resembles \nsingle, horizontal, oval balloon, real data may cooperate conforming ideal pattern. ornery plot may show wedge, curve, multiple clusters. Figure 3.2 shows examples types residual plots.Suppose held cup full coins dropped . hope find residual plots resemble random pattern likely result dropped coins (like oval-shaped plot). three plots show patterns unlikely occur random chance. plot patterns nice oval shapes suggest error terms violating least one model assumption, thus \nlikely unreliable estimates model coefficients. following section illustrates strategies dealing one unwanted shapes: wedge-shaped pattern.Note single-variable regression models, residual plots show information initial fitted line plot. However, residual plots often emphasize violations model assumptions better fitted line plot. addition, multivariate regression lines difficult visualize. Thus, residual plots essential\nmultiple explanatory variables used.[[[Figure 3.2]]]\nFigure 3.2 Common shapes residual plots. Ideally, residual plots look like randomly scattered set dropped\ncoins, seen oval-shaped plot. pattern exists, usually best try regression models.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"heteroskedasticity","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"Heteroskedasticity","text":"Heteroskedasticity term used describe situation variance error term constant levels explanatory variables. example, regression equation \\(Price = 24,765 - 0.173 (Mileage)\\), spread suggested retail price values around regression line whether mileage 0 mileage 50,000. heteroskedasticity exists model, common remedy transform either explanatory variable, response variable, hope transformed relationship exhibit homoskedasticity (equal variances around regression line) error terms.Using regression equation calculated Question 5, create plots residuals versus explanatory variable model. Also create plot residuals versus predicted retail price (often called residual versus fit plot).size residuals tend change mileage changesDoes size residuals tend change mileage changesDoes size residuals tend change predicted retail price changes? see patterns indicating heteroskedasticity (nonconstant variance).size residuals tend change predicted retail price changes? see patterns indicating heteroskedasticity (nonconstant variance).Another pattern may immediately obvious residual plots right skewness seen residual versus mileage plot. Often economic data, price, right skewed. see pattern, look just one vertical slice plot. pencil, draw vertical line corresponding mileage equal 8000. points residual plots balanced around line \\(Y = 0\\)?Another pattern may immediately obvious residual plots right skewness seen residual versus mileage plot. Often economic data, price, right skewed. see pattern, look just one vertical slice plot. pencil, draw vertical line corresponding mileage equal 8000. points residual plots balanced around line \\(Y = 0\\)?Describe patterns seen residual plots.Describe patterns seen residual plots.Transform suggested retail price log (Price) 2Price. Transforming data using roots, logarithms, reciprocals can often reduce heteroskedasticity right skewness. (Transformations discussed Chapter 2.[[[Add link]]]) Create regression models residual plots transformed response variables using \nexplanatory variables selected Question 5.transformation best job reducing heteroskedasticity skewness residual plots? Give \\(R^2\\) values new models.transformation best job reducing heteroskedasticity skewness residual plots? Give \\(R^2\\) values new models.best residual plots correspond best \\(R^2\\) values? Explain.\ntransformations tried, throughout investigation refer log-transformed response variable TPrice.best residual plots correspond best \\(R^2\\) values? Explain.\ntransformations tried, throughout investigation refer log-transformed response variable TPrice.Figure 3.3 shows residual plots created answer Questions 7 8. Notice response variable Price, residual versus fit plot clear wedge-shaped pattern. residuals much spread fitted value large (.e., expected retail price close $40,000) fitted value near $10,000. Using TPrice response improve residual versus fit plot. Although\nstill faint wedge shape, variability residuals much consistent fitted value changes. Figure 3.3 reveals another pattern residuals. following section address points plots appear clusters.[[[Fig 3.3 ]]]\nFigure 3.3 Residual versus fit plots using Price TPrice (log10 transformation), responses. residual plot Price response much stronger wedge-shaped pattern one TPrice.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"examining-residual-plots-across-timeorder","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"Examining Residual Plots Across Time/Order","text":"Autocorrelation exists consecutive error terms related. autocorrelation exists, assumption independence error terms violated. identify autocorrelation, plot residuals versus order data entries. ordered plot shows pattern, conclude autocorrelation exists. autocorrelation exists, variable responsible creating pattern ordered residual plot included model.Create residual versus order plot TPrice versus Mileage regression line. Describe pattern see ordered residual plot. Apparently something data affecting residuals based order data. Clearly, time influential factor data set (data 2005). Can suggest variable Cars data set may causing pattern?Create residual versus order plot TPrice versus Mileage regression line. Describe pattern see ordered residual plot. Apparently something data affecting residuals based order data. Clearly, time influential factor data set (data 2005). Can suggest variable Cars data set may causing pattern?Create second residual versus order plot using TPrice response using explanatory variables selected Question 5. Describe patterns see plots.Create second residual versus order plot using TPrice response using explanatory variables selected Question 5. Describe patterns see plots.Note (order data collected) perhaps common source autocorrelation, forms, spatial autocorrelations, can also present. time indeed variable included model, specific type regression model, called time series model, used.ordered plots make sense model checking meaningful order data, residual versus order plots demonstrate need include additional explanatory variables regression model. Figure 3.4 shows two residual plots created Questions 9 10. plots show data points clearly clustered order. However, less clustering six explanatory variables (Mileage, Cyl, Doors, Cruise, Sound, Leather) model. Also notice residuals tend closer zero second graph. Thus, second graph (six explanatory variables) tends estimates closer actual observed values.time variable data set, reordering data change meaning data. Reordering data eliminate pattern; however, clear pattern seen residual versus order plots ignored indicates create model much higher \\(R^2\\) value account pattern model. type autocorrelation called taxonomic autocorrelation, meaning relationship seen residual plot due \nitems data set classified. Suggestions address issue given later sections.[[[Fig 3.4]]]\nFigure 3.4 Residual versus order plots using TPrice response. first graph uses Mileage explanatory variable, second graph uses Mileage, Cyl, Doors, Cruise, Sound, Leather explanatory variables.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"outliers-and-influential-observations","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"Outliers and Influential Observations","text":"Calculate regression equation using explanatory variables suggested Question 5 Price response. Identify residuals (cluster residuals) don’t seem fit overall pattern residual versus fit residual versus mileage plots. data values don’t seem fit general pattern data set called outliers.Identify specific rows data represent points. consistencies can find?Identify specific rows data represent points. consistencies can find?cluster outliers helpful identifying patterns found ordered residual plots? ?cluster outliers helpful identifying patterns found ordered residual plots? ?Run analysis without largest cluster potential outliers (cluster outliers corresponds Cadillac convertibles). Use Price response. cluster outliers influence coefficients regression line?coefficients change dramatically regression models, points considered influential. observations influential, great care taken verify accuracy. addition reducing heterskedasticity, transformations can often reduce effect outliers. Figure 3.5 shows residual versus fit plots using Price TPrice, respectively. cluster outliers corresponding Cadillac convertibles much visible plot untransformed (Price) response\nvariable. Even though still clustering transformed data, residuals corresponding Cadillac convertibles longer unusually large.[[[Fig 3.5]]]\nFigure 3.5 Residual versus fit plots using Price TPrice response. circled observations plot using Price longer clear outliers plot using TPrice.situations, clearly understanding outliers can time consuming (possibly interesting) working rest data. can quite difficult determine outlier accurately recorded whether outliers included analysis.outliers accurately recorded transformations useful eliminating , can difficult know . simplest approach run analysis twice: outliers included without. results similar, doesn’t matter outliers included . results change, much difficult know . Outliers never automatically removed don’t fit overall pattern data. statisticians tend err side keeping outliers sample data set unless clear evidence mistakenly recorded. Whatever final model selected, important clearly state aware results sensitive outliers.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"normally-distributed-residuals","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"Normally Distributed Residuals","text":"Even though calculations regression model \\(R^2\\) depend normality assumption, identifying patterns residual plots can often lead another model better explains response variable.determine residuals normally distributed, two graphs often created: histogram residuals normal probability plot. Normal probability plots created sorting data (residuals case) smallest largest. sorted residuals plotted theoretical normal distribution. plot forms straight line, actual data theoretical data shape (.e., distribution). (Normal probability plots discussed detail \nChapter 2.)Create regression line predict TPrice Mileage. Create histogram normal probability plot residuals.residuals appear follow normal distribution?residuals appear follow normal distribution?ten outliers visible normal probability plot histogram?ten outliers visible normal probability plot histogram?Figure 3.6 shows normal probability plot using six explanatory variables estimate TPrice. outliers visible, plots still show evidence lack normality. Simply plugging data software package using iterative variable selection technique reliably create “best” model.[[[Fig3.6]]]\nFigure 3.6 Normal probability plot histogram residuals model using TPrice response Mileage, Cyl, Doors, Cruise, Sound, Leather explanatory variables.Key Concept\nfinal model selected, residuals plotted fitted (estimated) values, observation order, theoretical normal distribution, explanatory variable model. Table 3.1 shows residual plot used check model assumptions. pattern exists residual plots, \\(R^2\\) value likely improve different explanatory variables transformations included model.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"correlation-between-explanatory-variables","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"Correlation Between Explanatory Variables","text":"Multicollinearity exists two explanatory variables multiple regression model highly correlated . two explanatory variables \\(X_1\\) \\(X_2\\) highly correlated, can difficult identify whether \\(X_1\\), \\(X_2\\), variables actually responsible influencing response variable, \\(Y\\).Create three regression models using Price response variable. three cases, provide regression model, \\(R^2\\) value, t-statistic slope coefficients, corresponding \\(p\\)-values.first model, use Mileage Liter explanatory variables. Liter important explanatory variable model?second model, use Mileage number cylinders (Cyl) explanatory variables. Cyl important explanatory variable model?third model, use Mileage, Liter, number cylinders (Cyl) explanatory variables. test statistics \\(p\\)-values change three explanatory variables included model?Note \\(R^2\\) values essentially three models Question 14. coefficients Mileage also stay roughly three models—inclusion Liter Cyl model appear influence Mileage coefficient. Depending model used, state additional mile car, Price reduced somewhere $0.152 $0.16. Describe coefficient Liter depends whether Cyl model.Note \\(R^2\\) values essentially three models Question 14. coefficients Mileage also stay roughly three models—inclusion Liter Cyl model appear influence Mileage coefficient. Depending model used, state additional mile car, Price reduced somewhere $0.152 $0.16. Describe coefficient Liter depends whether Cyl model.Plot Cyl versus Liter calculate correlation two variables. strong correlation two variables? Explain.Plot Cyl versus Liter calculate correlation two variables. strong correlation two variables? Explain.Recall Question 4 suggested deleting Liter model. goal stepwise regression find “best” model based \\(R^2\\) value. two explanatory variables impact response way, stepwise regression rather arbitrarily pick one variable ignore .Key Concept\nStepwise regression can often completely miss important explanatory variables multicollinearity.software packages can create matrix plot correlations corresponding scatterplots explanatory variables. matrix plot helpful identifying patterns interdependence among explanatory variables. easy--apply guideline determining multicollinearity needs dealt use variance inflation factor (VIF).VIF conducts regression explanatory variable (\\(X_i\\)) remaining explanatory variables, calculates corresponding \\(R^2\\) value (\\({R_i}^2\\)), calculates following function variable Xi : \\(1/(1 - {R_i}^2)\\). \\({R_i}^2\\) value zero, VIF one, Xi uncorrelated explanatory variables. Montgomery, Peck, Vining state, “Practical experience indicates VIFs exceed 5 10, indication associated regression coefficients poorly estimated multicollinearity.”14Figure 3.7 shows Liter Cyl highly correlated. Within context problem, doesn’t make physical sense consider holding one variable constant variable increases. general, may possible “fix” multicollinearity problem. goal simply describe predict retail prices, multicollinearity critical issue. Redundant variables eliminated model, highly correlated variables contribute model acceptable interpreting coefficients. However, goal confirm whether explanatory variable associated response (test theory), essential identify presence multicollinearity recognize coefficients unreliable exists.Key Concept\ngoal create model describe predict, multicollinearity really problem. Note multicollinearity little impact \\(R^2\\) value. However, goal understand specific explanatory variable influences response, often done confirming theory, multicollinearity can cause coefficients (corresponding \\(p\\)-values testing significance) unreliable.[[[Fig 3.7]]]\nFigure 3.7 Scatterplot showing clear association Liter Cyl.following approaches commonly used address multicollinearity:\n- Get information: possible, expanding data collection may lead samples variables correlated. Consider whether greater range data collected whether data measured differently variables correlated. example, data GM cars. Perhaps relationship engine size liters number cylinders strong data wider variety manufacturers.\n- Re-evaluate model: two explanatory variables highly correlated, deleting one variable significantly impact \\(R^2\\) value. However, theoretical reasons include variables model, keep terms. example, Liter number cylinders (Cyl) measuring essentially quantity. Liter represents volume displaced one complete engine cycle; number cylinders (Cyl) also measure volume can displaced.\n- Combine variables: Using statistical techniques principal components, possible combine correlated variables “optimally” single variable can used model. may theoretical reasons combine variables certain way. example, volume (size) weight car likely highly positively correlated. Perhaps new variable defined density = weight/volume used model predicting price rather either individual variables.investigation, simply attempting develop model can used estimate price, multicollinearity much impact results. re-evaluate model light fact Liter number cylinders (Cyl) measure displacement (engine size), might note Liter specific variable, taking several values, Cyl three values data set. Thus, might choose keep Liter remove Cyl model.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"interpreting-model-coefficients","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.5 Interpreting Model Coefficients","text":"multiple regression useful understanding impacts various explanatory variables response, important limitations. predictors model highly correlated, size meaning coefficients can difficult interpret. Question 14, following three models developed:interpretation model coefficients complex multiple linear regression simple linear regression. can misleading try interpret coefficient without considering terms model. example, Mileage Liter two predictors regression model, Liter coefficient might seem indicate increase one Liter increase expected price $4968. However, Cyl also included model, Liter coefficient seems indicate increase one Liter increase expected price $1545. size regression coefficient even direction can change depending terms model.investigation, shown Liter Cyl highly correlated. Thus, unreasonable believe Liter change one unit Cyl stay constant. multiple linear regression coefficients considered isolation. Instead, Liter coefficient shows expected price change Liter increases one unit, accounting corresponding changes \nexplanatory variables model.Key Concept\nmultiple linear regression, coefficients tell us much expected response change explanatory variable increases one unit, accounting corresponding changes terms model.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"categorical-explanatory-variables","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.6 Categorical Explanatory Variables","text":"saw Question 9, clear pattern residual versus order plot Kelley Blue Book car pricing data. likely one categorical variables (Make, Model, Trim, Type) explain pattern.categorical variables related response variable, want add variables regression model. common procedure used incorporate categorical explanatory variables regression model define indicator variables, also called dummy variables. Creating indicator variables process mapping one column (variable) categorical data several columns (indicator variables) 0 1 data.Let’s take variable Make example. six possible values (Buick, Cadillac, Chevrolet, Pontiac, SAAB, Saturn) can recoded using six indicator variables, one six makes car. example, indicator variable Buick value 1 every car Buick 0 car Buick. statistical software packages command creating indicator variables automatically.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"creating-indicator-variables","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"Creating Indicator Variables","text":"Create boxplots individual value plots response variable TPrice versus categorical variables Make, Model, Trim, Type. Describe patterns see.Create boxplots individual value plots response variable TPrice versus categorical variables Make, Model, Trim, Type. Describe patterns see.Create indicator variables Make. Name columns, order, Buick, Cadillac, Chevrolet, Pontiac, SAAB, Saturn. Look new data columns describe indicator variables defined. example, list possible outcomes Cadillac indicator variable explain outcome\nrepresents.Create indicator variables Make. Name columns, order, Buick, Cadillac, Chevrolet, Pontiac, SAAB, Saturn. Look new data columns describe indicator variables defined. example, list possible outcomes Cadillac indicator variable explain outcome\nrepresents.indicator variables Question 18 can incorporated regression model. However, want include Make entirety model, include six indicator variables. Five suffice complete redundancy sixth indicator variable. first five indicator variables 0 particular car, automatically know car belongs \nsixth category. , leave Buick indicator variable model. coefficient indicator variable estimate average amount response variable change.example, estimated coefficient Saturn variable estimate average difference TPrice car Saturn rather Buick (adjusting corresponding changes terms model).Mathematical Note\ncategorical explanatory variable \\(g\\) groups, \\(g - 1\\) terms included regression model. software packages use matrix algebra develop multiple regression models. \\(g\\) terms model, explanatory variables 100% correlated (can exactly predict value one variable know variables) needed matrix inversion done. \nresearcher chooses leave \\(g\\) terms model, software packages arbitrarily remove one term needed matrix calculations can completed.may tempting simplify model including significant indicator variables. example, instead including five indicator variables Make, might consider using Cadillac SAAB. statisticians recommend . limiting model indicator\nvariables significant sample data set, can overfit model.Models overfit researchers overwork data set order increase \\(R^2\\) value. example, researcher spend significant amount time picking indicator variables Make, Model, Trim, Type order find best \\(R^2\\) value. model likely estimate mean response well, unlikely accurately predict new values response variable.fairly nuanced point. purpose variable selection techniques select variables best explain response variable. However, overfitting may occur break categorical variables smaller units pick choose among best parts variables. (Model Validation section\nextended activities discusses topic detail.[[[add link]]])","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"building-regression-models-with-indicator-variables","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"Building Regression Models with Indicator Variables","text":"19 Build new regression model using TPrice response Mileage, Liter, Saturn, Cadillac, Chevrolet, Pontiac, SAAB explanatory variables. Explain expect \\(R^2\\) value increase add terms Make.Create indicator variables Type. Include Make Type indicator variables, plus variables Liter, Doors, Cruise, Sound, Leather, Mileage, model predict TPrice. Remember leave least one category model Make Type indicator variables (e.g., leave Buick Hatchback model). Compare regression model models fit\ndata investigation. normal probability plot suggest residuals follow normal distribution? Describe whether residual versus fit, residual versus order, residual versus explanatory variable plots look random earlier problems.additional categorical variables important improving regression model. Figure 3.8 shows Make Type model, residuals clustered. Make Type included model, residuals appear randomly distributed. incorporating Make Type, able explain variability causing clustering. addition, sizes residuals much smaller. Smaller residuals indicate better fitting model higher \\(R^2\\) value.Even though Make Type improved residual plots, still clustering might improved incorporating Model regression equation. However, goal simply develop model accurately estimates retail prices, \\(R^2\\) value Question 20 already fairly high. terms can added model dramatically improve \\(R^2\\) value?determine final model, attempt maximize \\(R^2\\) value simultaneously keeping model relatively simple. final model, comment residual versus fit, residual versus order, residual plots previously showed patterns. pattern exists residual plots, may worth attempting new regression model account patterns. regression model can modified address patterns residuals, \\(R^2\\) value improve. However, note \\(R^2\\) value already fairly high. may worth making model complex slight increase \\(R^2\\) value.Create regression model simple (.e., many terms) still accurately predicts retail price. Validate model assumptions. Look residual plots check heteroskedasticity, multicollinearity, autocorrelation, outliers. final model significant clusters, skewness, outliers, heteroskedasticity appearing residual plots. Submit suggested least squares regression formula along limited number appropriate graphs provide justification model. Describe believe model “best.”[[[Fig 3.8]]]\nFigure 3.8 Residual versus order plots show incorporating indicator variables regression model improves random behavior reduces sizes residuals.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"what-can-we-conclude-from-the-2005-gm-car-study","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.7 What Can We Conclude from the 2005 GM Car Study?","text":"data observational study, experiment. Therefore, even though \\(R^2\\) value reveals strong relationship explanatory variables response, significant correlation (thus significant coefficient) imply causal link explanatory variable response. may theoretical practical reasons believe mileage (\nexplanatory variables) causes lower prices, final model can used show association.Best subsets residual graphs used develop model useful describing predicting retail price based function explanatory variables. However, since iterative techniques used, \\(p\\)-values corresponding significance individual coefficient reliable.data set, cars randomly selected within make, model, type 2005 GM car produced, suggested retail prices determined Kelley Blue Book. simple random sample 2005 GM cars actually road, still reason believe final model provide accurate description prediction retail price used 2005 GM cars. course, time goes , value cars reduced updated models need developed.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"f-tests-for-multiple-regression","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.8 3.8 \\(F\\)-Tests for Multiple Regression","text":"","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"decomposition-of-sum-of-squares","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.8.1 Decomposition of Sum of Squares","text":"Many calculations involved multiple regression closely related simple linear regression model. Simple linear regression models advantage easily visualized scatterplots. Thus, start simple linear regression model derive several key equations used multiple regression.Figure 1 shows scatterplot regression line subset used 2005 Kelly Blue Book Cars data. data set restricted just Chevrolet Cavalier Sedans. scatterplot, one specific observation highlighted: point \\(Mileage\\) 11,488 observed \\(Price\\) \\(y_i = 14{,}678.1\\).Figure 1, see individual observation total deviation \\((y_i - \\bar{y})\\) decomposed two parts:\\[\\begin{equation}\ny_i - \\bar{y} = (y_i - \\hat{y}_i) + (\\hat{y}_i - \\bar{y})\n\\tag{3.7}\n\\end{equation}\\]\n(#fig:df_1-plot)Scatterplot regression line Chevrolet Cavalier Sedans: Price = 15,244 - 0.111(Mileage).\nUsing highlighted observation (11,488, 14,678.1), see \n\\[\\begin{equation}\ny_i - \\bar{y} = (y_i - \\hat{y}_i) + (\\hat{y}_i - \\bar{y}) \\notag\n\\end{equation}\\]\n\\[\\begin{equation}\n14{,}678.1 - 12{,}962 = (14{,}678.1 - 13{,}967.68) + (13{,}967.68 - 12{,}962) \\notag\n\\end{equation}\\]\n\\[\\begin{equation}\n1716.1 = 710.42 + 1005.68 \\notag\n\\end{equation}\\]Squaring sides Equation (3.7) summing observations results \\[\\begin{align}\n\\sum_{=1}^n (y_i - \\bar{y})^2 \\notag\n  &= \\sum_{=1}^n (y_i - \\hat{y}_i)^2\n  + \\sum_{=1}^n (\\hat{y}_i - \\bar{y})^2\n  + 2 \\sum_{=1}^n (\\hat{y}_i - \\bar{y})(y_i - \\hat{y}_i) \\\\[6pt] \\tag{3.8}\n  &= \\sum_{=1}^n (y_i - \\hat{y}_i)^2 + \\sum_{=1}^n (\\hat{y}_i - \\bar{y})^2 \\\\[6pt] \\notag\n\\end{align}\\]key point previous calculations show total variability response, \\(\\sum_{=1}^n (y_i - \\bar{y})^2\\), can decomposed following:\\[\\begin{equation}\n\\text{Total sum squares (SST) = Residual sum squares (SSE) + Regression sum squares (SSR)} \\notag\n\\end{equation}\\]MATHEMATICAL NOTE\nshow Equation (3.8) true, can write\n\\[\\begin{equation}\n2 \\sum_{=1}^n (y_i - \\hat{y}_i)(\\hat{y}_i - \\bar{y})\n  = 2 \\sum_{=1}^n \\hat{y}_i (y_i - \\hat{y}_i)\n  - 2 \\bar{y} \\sum_{=1}^n (y_i - \\hat{y}_i) \\notag\n\\end{equation}\\]MATHEMATICAL NOTE\nRecall sum residuals, \\(\\sum_{=1}^n (y_i - \\hat{y}_i)\\), equals zero. addition, can shown sum residuals, weighted corresponding predicted value, always sums zero: \\(\\sum_{=1}^n \\hat{y}_i (y_i - \\hat{y}_i) = 0.\\) (See Questions 25 29.)","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"extended-activity-a-closer-look-at-least-squares-regression-equations","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"Extended Activity: A Closer Look at Least Squares Regression Equations","text":"Data set: \\(Cavalier\\)\nNote calculus required Activity Questions 25 29.Create regression model predict Price Mileage Cavalier data. Calculate total sum squares (SST), residual sum squares (SSE), regression sum squares (SSR). Verify SST = SSE + SSR.Show \\(\\sum_{=1}^n (y_i - \\hat{y}_i)(\\hat{y}_i - \\bar{y}) = 0\\) model given previous question.Using final model Question 21, calculate total sum squares (SST), residual sum squares (SSE), regression sum squares (SSR). Verify SST = SSE + SSR.Set partial derivative residual sum squares respect \\(b_0\\) zero, show \\(b_0 n + b_1 \\sum_{=1}^n x_i = \\sum_{=1}^n y_i.\\)Set partial derivative residual sum squares respect \\(b_1\\) zero, show \\(b_0 \\sum_{=1}^n x_i + b_1 \\sum_{=1}^n x_i^2 = \\sum_{=1}^n x_i y_i.\\)equations Questions 25 26 called normal equations simple linear regression. Use normal equations derive least squares regression coefficients, \\(b_0\\) \\(b_1\\).Use fact \\(\\sum_{=1}^n (y_i - \\hat{y}_i) = 0\\) \\(\\hat{y}_i = b_0 + b_1 x_i\\) show \n\\[\\begin{equation}\n\\sum_{=1}^n \\hat{y}_i (y_i - \\hat{y}_i)\n  = b_1 \\Bigl(\\sum_{=1}^n x_i y_i - b_0 \\sum_{=1}^n x_i - b_1 \\sum_{=1}^n x_i^2\\Bigr). \\notag\n\\end{equation}\\]Use Questions 26 28 show \\(\\sum_{=1}^n \\hat{y}_i (y_i - \\hat{y}_i) = 0.\\)","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"the-analysis-of-variance-table","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.8.2 The Analysis of Variance Table","text":"objective regression create model best fits observed points. Least squares regression models define “best fit” model minimizes sum squared residual values, \\(\\sum_{=1}^n (y_i - \\hat{y}_i)^2\\).coefficient determination, \\(R^2\\), percentage variation response variable explained regression line:KEY CONCEPT\ncoefficient determination, \\(R^2\\), measure usefulness explanatory variables model. explanatory variables useful predicting response, residual sum squares, \\(\\sum_{=1}^n (y_i - \\hat{y}_i)^2\\), small compared total spread responses, \\(\\sum_{=1}^n (y_i - \\bar{y})^2\\). words, amount variability explained regression model, \\(\\sum_{=1}^n (\\hat{y}_i - \\bar{y})^2\\), large proportion total variability responses.sum squares calculations often summarized analysis variance (ANOVA) table, shown Table 1.Table 3.1: ANOVA table least squares regression model, n number observations p number terms model (including constant term).","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"testing-the-significance-of-a-regression-model","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.8.3 Testing the Significance of a Regression Model","text":"model developed, often interested testing relationship response set explanatory terms model. conduct overall test model adequacy, test following null alternative hypotheses:\n\n\n\n\n\nNotice \\(\\beta_0\\) term regression model included null alternative hypothesis. Table 1 provides details calculation F-statistic:\n\\[\\begin{equation}\nF = \\frac{MS_{Regr}}{MSE}\n  = \\frac{SSR/(p - 1)}{SSE/(n - p)}\n\\tag{3.11}\n\\end{equation}\\]statistic follows \\(F_{p-1,\\,n-p}\\) distribution, \\(n\\) number observations \\(p\\) number terms model (including \\(\\beta_0\\)). assumptions error terms, \\(\\epsilon_i \\overset{\\mathrm{iid}}{\\sim}N(0,\\sigma^2)\\), need checked conducting hypothesis test.NOTE\nmodel assumptions needed error terms calculate estimates coefficients. However, model assumptions checked conducting hypothesis test.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"the-extra-sum-of-squares-f-test","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.8.4 The Extra Sum of Squares \\(F\\)-Test","text":"often interested testing contribution particular variable (subset variables) regression sum squares. extra sum squares \\(F\\)-test can test contribution specific set variables comparing residuals full reduced model.Suppose model fit \\(k\\) terms—call full model. may hypothesize \\(p < k\\) terms really contribute regression model—call smaller model reduced model. situation, want test whether\n\n\n\n\n\nprevious ANOVA \\(F\\)-test can modified provide \\(F\\)-test hypothesis. Notice hypothesis test makes assumptions terms, \\(\\beta_0, \\beta_1, \\dots, \\beta_{p-1}\\), model. addition, every term reduced model must also full model.\n\n\n\nstatistic follows F-distribution \\(k-p\\) \\(n-k\\) degrees freedom. extra sum squares \\(F\\)-test determines whether difference sum squared residuals full reduced\nmodels large unlikely occur chance.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"extended-activity-testing-multiple-coefficients","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"Extended Activity: Testing Multiple Coefficients","text":"Data set: \\(Cavalier\\)\nConsider Cavalier data set regression model \\(y = \\beta_0 + \\beta_1(Mileage) + \\beta_2(Cruise) + \\epsilon\\)\n30. Submit ANOVA table, \\(F\\)-statistic, \\(p\\)-value test hypothesis \\(H_0: \\beta_1 = \\beta_2 = 0\\) versus \\(H_a\\): least one coefficients 0.\n31. Conduct extra sum squares test determine \\(Trim\\) useful. specifically, use reduced model previous question full model\n\\[\\begin{equation}\ny = \\beta_0 + \\beta_1(\\text{Mileage}) + \\beta_2(\\text{Cruise}) + \\beta_3(\\text{LS Sport Sedan 4D}) + \\beta_4(\\text{Sedan 4D}) \\notag\n\\end{equation}\\]\ntest hypothesis \\(H_0: \\beta_3 = \\beta_4 = 0\\) versus \\(H_a\\): least one coefficients 0.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"developing-a-model-to-confirm-a-theory","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.9 3.9 Developing a Model to Confirm a Theory","text":"goal confirm theoretical relationship, statisticians tend go following steps identify appropriate theoretical model.\\(\\bullet\\) Verify response variable provides information needed address question interest. range variability responses expect observe? response measurement precise enough address question interest?\n\\(\\bullet\\) Investigate explanatory variables may importance potentially influence results. Note terms model included even though coefficients may significant. studies, often prior information theoretical explanation relationship explanatory response variables. Nonstatistical information often essential developing good statistical models.\\(\\bullet\\) explanatory variables plan include model, describe whether expect positive negative correlation variable response variable.\\(\\bullet\\) Use background information available identify factors assumed controlled within model. measurements, materials, process data collection create unwanted variability? Identify explanatory variables may influence response; determine information variables can collected variables can controlled throughout study. example, Kelley Blue Book data set, condition car assumed cars. data collected GM cars model year 2005. Since cars relatively new cars considered excellent condition, model create data relevant cars type accident.\\(\\bullet\\) conditions considered normal type study? conditions controllable? condition changed study, might impact results?theoretical model developed, regression analysis conducted one time determine data support theories.KEY CONCEPT\ndata looked develop model test .","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"extended-activity-testing-a-theory-on-cars","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"Extended Activity: Testing a Theory on Cars","text":"Data set: \\(Cars\\)\nAssume asked determine association explanatory\nvariables response \\(Cars\\) data set.Use background information may (Cars data set) predict explana-\ntory variable (except \\(Model\\) \\(Trim\\)) influence \\(TPrice\\). example, \\(Liter\\) \\(Mileage\\)\npositive negative association \\(TPrice\\)? List \\(Make\\) identify impact\n\\(TPrice\\) direction.Use background information may (Cars data set) predict explana-\ntory variable (except \\(Model\\) \\(Trim\\)) influence \\(TPrice\\). example, \\(Liter\\) \\(Mileage\\)\npositive negative association \\(TPrice\\)? List \\(Make\\) identify impact\n\\(TPrice\\) direction.Identify factors controlled data set. Can suggest factors outside pro-\nvided data set included? coefficients found significant (small\n\\(p\\)-values), relationships hold areas United States? relationships hold 2004 2001 cars?Identify factors controlled data set. Can suggest factors outside pro-\nvided data set included? coefficients found significant (small\n\\(p\\)-values), relationships hold areas United States? relationships hold 2004 2001 cars?Run regression analysis test hypothesized model. variables important model? accurately estimate direction relationship? Note even variable significant, typically kept model theoretical justification\n.Run regression analysis test hypothesized model. variables important model? accurately estimate direction relationship? Note even variable significant, typically kept model theoretical justification\n.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"interaction-and-terms-for-curvature","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.9.1 Interaction and Terms for Curvature","text":"addition using variables provided data set, often beneficial create new variables functions existing explanatory variables. new explanatory variables often quadratic (\\(X^2\\)), cubic (\\(X^3\\)), product two explanatory variables (\\(X_1*X_2\\)), called interaction terms.interaction present effect one variable, \\(Mileage\\), depends second variable, \\(Cyl\\). interaction exists, influence \\(Cyl\\) changes different \\(Mileage\\) values, also influence \\(Mileage\\) depend \\(Cyl\\).data set \\(4-8Cyl\\) includes several four- eight-cylinder cars original Cars data. Figure 2 shows scatterplot regression line predict \\(Price\\) using \\(Mileage\\) \\(Cyl\\). regression model Figure 2 interaction term. parallel lines show estimated impact changing cylinder size depend mileage. Thus, given number miles, number cylinders changes four eight, expect increase Price \\(4 \\times 3443 = 13{,}772\\).way, \\(Mileage\\) coefficient states holding \\(Cyl\\) constant, expect \\(Price\\) decrease \\(0.20\\) additional mile car.\n(#fig:df_2-plot)Scatterplot least squares regression line: Price = 15,349 - 0.20(Mileage) + 3443(Cyl). cylinder size, increase one mile expected reduce price $0.20.\nFigure 3 shows scatterplot regression line predict \\(Price\\) using \\(Mileage\\), \\(Cyl\\), \\(Mileage*Cyl\\) interaction term (called \\(MileCyl\\)). lack parallel lines regression model \\(\\text{Price} = 4533 + 0.340(\\text{Mileage}) + 5431(\\text{Cyl}) - 0.0995(\\text{MileCyl})\\) indicates interaction effect.Caution used interpreting coefficients interaction terms present. coefficient \\(Mileage\\) can longer globally interpreted reducing \\(Price\\) \\(0.20\\) additional mile. Now, four cylinders, \\(Price\\) reduced \\(0.058\\ [0.340(1) - 0.0995(1 \\times 4) = -0.058]\\) additional mile. eight cylinders, \\(Price\\) reduced \\(0.456\\ [0.340(1) - 0.0995(1 \\times 8) = -0.456]\\) additional mile. Thus, additional mile impacts \\(Price\\) differently depending second variable, \\(Cyl\\).\n(#fig:df_3-plot)Scatterplot least squares regression line: Price = 4533 + 0.340(Mileage) + 5431(Cyl) + 0.0995(MileCyl). interaction term (MileCyl) important, expect regression lines parallel.\n","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"extended-activity-understanding-interaction-terms","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"Extended Activity: Understanding Interaction Terms","text":"Data set: \\(4-8Cyl\\)Use \\(4-8Cyl\\) data set calculate two regression equations shown Figures 2 3.\\(R^2_{\\text{adj}}\\) value increase interaction term added? Based change \\(R^2_{\\text{adj}}\\), interaction term included model?models, calculate estimated price four-cylinder car \\(Mileage\\) = 10,000.Assuming \\(Mileage\\) = 10,000, models explain increasing four eight cylinders impact estimated price.Conduct extra sum squares test determine \\(MileCyl\\) interaction term important model.Use \\(4-8Cyl\\) data set calculate regression line \\(Price = \\beta_0 + \\beta_1(Mileage) + \\beta_3(Cadillac) + \\beta_4(SAAB)\\). need create indicator variables \\(Make\\) calculating regression line.Create scatterplot \\(Mileage\\) explanatory variable \\(Price\\) response. Overlay second graph \\(Mileage\\) explanatory variable \\(\\hat y\\) response. Notice predicted values (\\(\\hat y\\) values) form two separate lines. parallel lines (interaction model) look appropriate?Conduct one extra sum squares test determine interaction terms (\\(MileCadillac\\) \\(MileSAAB\\)) important model (.e., test hypothesis \\(H_0: \\beta_5 = \\beta_6 = 0\\) versus \\(H_a\\): least one coefficients 0, \\(\\beta_5\\) \\(\\beta_6\\) coefficients two interaction terms). Create scatterplot full regression model explain results hypothesis test.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"quadratic-and-cubic-terms","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.9.2 Quadratic and Cubic Terms","text":"plot residuals versus explanatory variable shows curvature, model may improved including quadratic term. relationship mileage retail price linear quadratic Kelley Blue Book data? test , quadratic term \\(Mileage*Mileage\\) can created included regression model.MATHEMATICAL NOTE\nEven though models quadratic (\\(x^2\\)) cubic (\\(x^3\\)) terms linear functions original explanatory variables, mean response linear regression coefficients (\\(\\beta_0, \\beta_1, \\beta_2, \\dots\\)). example \\(y = \\beta_0 + z_1 \\beta_1 + z_2 \\beta_2 + \\epsilon\\) considered linear regression model \\(z_1 = x\\) \\(z_2 = x^2\\).","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"extended-activity-understanding-quadratic-terms","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"Extended Activity: Understanding Quadratic Terms","text":"Data set: \\(MPG\\)\nMPG data compare miles per gallon several cars speed car going well displacement. Displacement measure volume cylinders within engine. larger displacement, quickly fuel can move engine, giving vehicle power.Use \\(MPG\\) data create regression model predict \\(MPG\\) \\(Speed\\) \\(Displacement\\): \\(\\mathrm{MPG} = \\beta_0 + \\beta_1(\\mathrm{Speed}) + \\beta_2(\\mathrm{Displacement}).\\)regression equation \\(R^2\\) value?Look residual versus \\(Speed\\) residual versus \\(Displacement\\) plots. Describe patterns see.residual normal probability plot show?Create regression model predict \\(MPG\\) Speed: \\(\\mathrm{MPG} = \\beta_0 + \\beta_1(\\mathrm{Speed}).\\)regression equation \\(R^2\\) value?Look residual versus \\(Speed\\) residual versus \\(Displacement\\) plots. Describe patterns residual plots.Describe patterns residual normal probability plot.\\(Displacement\\) important explanatory variable? Use residual plots \\(R^2\\) give intuitive explanation.Create model using displacement predict \\(MPG\\): \\(\\mathrm{MPG} = \\beta_0 + \\beta_1(\\mathrm{Displacement}).\\)regression equation \\(R^2\\) value?Look residual versus \\(Speed\\) residual versus \\(Displacement\\) plots. Describe patterns residual plots.Create (\\(\\mathrm{Speed}\\))^2 term (called \\(Speed_Sq\\)) incorporate term regression model predict \\(MPG\\): \\(\\mathrm{MPG} = \\beta_0 + \\beta_1(\\mathrm{Speed}) + \\beta_2(\\mathrm{Displacement}) + \\beta_3(\\mathrm{Speed\\_Sq}).\\)regression equation \\(R^2\\) value?Look residual versus \\(Speed\\) residual versus \\(Displacement\\) plots. Describe changes (\\(\\mathrm{Speed}\\))^2 added model.residual normal probability plot show?","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"extended-activity-creating-new-terms-to-predict-the-retail-price-of-cars","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.9.3 Extended Activity: Creating New Terms to Predict the Retail Price of Cars","text":"Data set: \\(Cars\\)\npotential outliers identified Question 11 can provide interesting demonstration interaction. Figure 3.12 shows slope predict \\(Price\\) \\(Mileage\\) ten Cadillac XLR-V8s much steeper slope found using cars. shows depreciation high-end cars almost 50 cents mile, opposed 15 cents mile average car types combined.\n(#fig:df_4-plot)Scatterplot regression lines: Cadillac XLR-V8, regression line Price = 71,997 - 0.4827(Mileage). much steeper line regression line cars: Price = 23,894 - 0.1549(Mileage).\nCreate quadratic mileage term. Create two models predict \\(TPrice\\), one \\(Mileage\\) another \\(Mileage\\) \\((Mileage)^2\\) (called \\(MileSq\\)).much \\(R^2\\) value increase quadratic term added model \\(TPrice = \\beta_0 + \\beta_1(Mileage)\\)?Look plots residuals versus \\(Mileage\\) models. addition \\(MileSq\\) term improve residual plots?Create interaction term \\(Mileage*Cyl\\) (called \\(MileCyl\\)). Use \\(Mileage\\), \\(Cyl\\), \\(MileCyl\\) predict \\(Price\\). interaction term appear improve model? Use residual plots \\(R^2\\) justify answer.\n“best” model, many final models developed students Question 21 tend include terms \\(Cadillac\\), \\(Convertible\\), \\(Liter\\). Since terms related Cadillac XLR-V8, may helpful include interaction term \\(Mileage*Cadillac\\), \\(Mileage*Convertible\\), \\(Mileage*Liter\\). \\(Mileage\\), \\(Make\\), \\(Type\\) interactions may also helpful additions model.Create interaction term \\(Mileage*Cyl\\) (called \\(MileCyl\\)). Use \\(Mileage\\), \\(Cyl\\), \\(MileCyl\\) predict \\(Price\\). interaction term appear improve model? Use residual plots \\(R^2\\) justify answer.\n“best” model, many final models developed students Question 21 tend include terms \\(Cadillac\\), \\(Convertible\\), \\(Liter\\). Since terms related Cadillac XLR-V8, may helpful include interaction term \\(Mileage*Cadillac\\), \\(Mileage*Convertible\\), \\(Mileage*Liter\\). \\(Mileage\\), \\(Make\\), \\(Type\\) interactions may also helpful additions model.Develop additional quadratic interaction terms. Determine improve regression model Question 42.Develop additional quadratic interaction terms. Determine improve regression model Question 42.Submit new regression model best predicts \\(TPrice\\). including quadratic interaction terms improve model developed Question 21?Submit new regression model best predicts \\(TPrice\\). including quadratic interaction terms improve model developed Question 21?Unless clear reason include , researchers typically create interaction terms test whether included model. researcher’s effort spent determining whether original explanatory variables provided data set related response. interaction term (\\(X_i * X_j\\)) included final model, common practice include original terms (\\(X_i\\) \\(X_j\\)) model well (even coefficients original terms close zero).","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"a-closer-look-at-variable-selection-criteria","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.10 3.11 A Closer Look at Variable Selection Criteria","text":"growing number large data sets well increasing computer power dramatically improved ability researchers find parsimonious model (model carefully selects relatively small number useful explanatory variables). However, even intensive computing power, process finding “best” model often art science.shown earlier, stepwise procedures use prespecified conditions automatically add delete variables can limitations:\\(\\bullet\\) explanatory variables correlated, stepwise procedures often fail include variables useful describing results.\\(\\bullet\\) Stepwise procedures tend overfit data (fit unhelpful variables) searching terms explain variability sample results. chance variability sample results may reflected entire population sample collected.\\(\\bullet\\) automated stepwise process provides “best” model can easily misinterpreted, since doesn’t require researcher explore data get intuitive feel data. example, stepwise procedures don’t encourage researchers look residual plots may reveal interesting patterns within data.##$ Adjusted \\(R^2\\)\\(R^2\\) useful determining well particular model fits data, useful variable selection. Adding new explanatory variables regression model never increase residual sum squares; thus, \\(R^2\\) increase (stay ) even explanatory variable contribute fit.adjusted \\(R^2\\) (\\(R^2_{adj}\\)) increases improvement model fit outweighs cost adding additional term model:\n\\[\\begin{equation}\nR^2_{adj} = 1 - \\left(\\frac{n - 1}{n - p}\\right)\\frac{\\sum_{=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{=1}^n (y_i - \\bar{y})^2}\n= 1 - \\left(\\frac{n - 1}{n - p}\\right)(1 - R^2)\n\\tag{3.13}\n\\end{equation}\\]\n\\(n\\) sample size \\(p\\) number coefficients model (including constant term).##MATHEMATICAL NOTE##\nIntuitively, additional term regression model means one additional parameter value must estimated. parameter estimate costs additional degree freedom. Thus, \\(R^2_{adj}\\) \\(R^2\\) value adjusted degrees freedom can written \n\\[\\begin{equation}\nR^2_{adj} = 1 - \\frac{\\text{MSE}}{\\text{SST}/(n - 1)} \\notag\n\\end{equation}\\]\n\\(R^2_{adj}\\) measures spread residuals using MSE, \\(R^2\\) measures spread residuals using SSE.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"mallows-c_p","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.10.1 Mallows’ \\(C_p\\)","text":"Another approach variable selection use Mallows’ \\(C_p\\) statistic:\n\\[\\begin{equation}\nC_p = (n - p)\\left(\\frac{\\hat{\\sigma}^2}{\\hat{\\sigma}^2_{Full}}\\right) + (2p - n)\n= (n - p)\\left(\\frac{MSE}{MSE_{Full}}\\right) + (2p - n)\n\\tag{3.14}\n\\end{equation}\\]\\(n\\) sample size, \\(p\\) number coefficients model (including constant term), \\(\\hat{\\sigma}^2\\) estimates variance residuals model, \\(\\hat{\\sigma}^2_{Full}\\) estimates variance residuals full model (.e., model potential explanatory variables data set).current model lacks important explanatory variable, \\(\\hat{\\sigma}^2\\) much larger \\(\\hat{\\sigma}^2_{Full}\\) \\(C_p\\) tends large. models \\(\\hat{\\sigma}^2\\) similar \\(\\hat{\\sigma}^2_{Full}\\), \\(C_p\\) provides penalty, \\(2p - n\\), favor models smaller number terms. fixed number terms, minimizing \\(C_p\\) equivalent minimizing SSE, also equivalent maximizing \\(R^2\\).\\(C_p\\) statistic assumes \\(\\hat{\\sigma}^2_{Full}\\) unbiased estimate overall residual variability, \\(\\sigma^2\\). full model several terms useful predicting response (.e., several coefficients essentially zero), \\(\\hat{\\sigma}^2_{Full}\\) overestimate \\(\\sigma^2\\) \\(C_p\\) small.*current model explains data well full model, \\(\\hat{\\sigma}^2 / \\hat{\\sigma}^2_{Full} = 1\\). \\(C_p = (n - p)(1) + 2p - n = p\\) Thus, objective often find smallest \\(C_p\\) value close p.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"akaikes-information-criterion-aic-and-bayes-information-criterion-bic","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.10.2 Akaike’s Information Criterion (AIC) and Bayes’ Information Criterion (BIC)","text":"Two additional model selection criteria Akaike information criterion (AIC) Bayesian information criterion (BIC).† criteria popular also applicable regression models fit maximum likelihood techniques (logistic regression), whereas \\(R^2\\) \\(C_p\\) appropriate least squares regression models.Calculations two criteria provided . statistics also include measure variability residuals plus penalty term:\n\\[\\begin{equation}\nAIC = n[\\log(\\hat{\\sigma}^2)] + 2p \\notag\n\\end{equation}\\]\n\\[\\begin{equation}\nBIC = n[\\log(\\hat{\\sigma}^2)] + p[\\log(n)] \\notag\n\\end{equation}\\]\\(n\\) sample size, \\(p\\) number coefficients model, \\(\\hat{\\sigma}^2\\) estimates variance residuals model.AIC BIC identical except penalties, \\(2p\\) \\(p[\\log(n)]\\), respectively. Thus, AIC BIC tend select slightly different models based \\(n\\). AIC BIC select models correspond smallest value.KEY CONCEPT\nindividual criterion (\\(R^2_{adj}\\), \\(C_p\\), AIC, BIC) universally better selection criteria. tools helpful selecting models, produce model necessarily “best.”","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"model-validation","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.10.3 Model Validation","text":"Often goal just describe sample data, generalize entire population sample drawn. Even regression model developed fits existing sample data well satisfies model assumptions, guarantee model accurately predict new observations.Variable selection techniques choose variables account variation response. many explanatory variables, likely least terms selected don’t explain patterns seen entire population; included simply chance variability seen sample.validate regression model useful predicting observations used develop\nmodel, following:\\(\\bullet\\) Collect new data population original data. Use new data determine \npredictive ability original regression model.\\(\\bullet\\) Split original data. example, randomly select 75% observations original data\nset, develop model, check appropriate model assumptions. Test predictive ability \nmodel remaining 25% data set. often called cross-validation.\\(\\bullet\\) data set large enough split, try jackknife validation. Hold one observation \ntime, develop model using \\(n-1\\) remaining observations, estimate value observation held . Repeat process n observations calculate predictive \\(R^2\\) value evaluate predictive ability model.\\(\\bullet\\) Use theory prior experience compare regression model models developed \nsimilar data.","code":""},{"path":"multiple-regression-how-much-is-your-car-worth.html","id":"chapter-summary__needs-several-tables","chapter":"3 Multiple Regression: How Much Is Your Car Worth?","heading":"3.11 Chapter Summary__[[[NEEDS SEVERAL TABLES]]]","text":"chapter, discussed techniques describe, predict, test hypotheses relationship\nquantitative response variable multiple explanatory variables. goals regression model\ninfluence techniques used conclusions can drawn. Cars activities\nchapter focused developing model describe relationship explanatory\nvariables response variable well predict value response based function explanatory variables.Iterative techniques best subsets regression often useful identifying terms included model. process selecting explanatory variables include model often involves iterative techniques, numerous models created compared step process. Iterative techniques used goal multiple regression test hypotheses. Stepwise regression used find model highest R2 value; however, provide much useful information model. example, important variables (Liter investigation) often included stepwise regression models. Best subsets regression useful iterative technique\nallows researcher better identify important explanatory variables, even multicollinearity (highly correlated explanatory variables) exists. Table 3.3 lists key measures used variable selection.individual criterion (${R^2}{adj}, \\(C_p\\), \\(AIC\\), \\(BIC\\)) universally better selection criteria. tools helpful selecting models, produce model necessarily “best.”iterative techniques useful reducing large number explanatory variables manageable set, researcher ask following questions evaluate resulting model:techniques used create model appropriate based goals regression model?coefficients make sense? magnitudes coefficients reasonable? coefficients opposite sign expected, multicollinearity may present, range explanatory variables may small, important explanatory variables may missing model.residual plots identify outliers patterns indicate unexplained structure included model?goal use hypothesis testing determine explanatory variables impacts response, iterative techniques appropriate. addition, hypothesis tests specific explanatory variables reliable multicollinearity lack normality exists.Model assumptions need met goal test hypotheses. least squares regression models can calculated without checking model assumptions, identifying patterns residual plots may indicate heteroskedasticity, autocorrelation, outliers, lack normality important creating good model.pattern exists residual plots, likely another model exists better explains response variable. Researchers need somewhat creative deciding graphs create adapt model based see.[[[Table 3.3]]]#####CHAPTER 2#################Note\nThroughout chapter, \\(\\bar{y}_{1.} = \\bar{y}_{1}\\) \\(\\bar{y}_{2.} = \\bar{y}_{2}\\). dot notation often used complex models\nindicate average taken values subscript. example, \\(bar{y}_{2.}\\) averages \n\\(j = 1, 2, 3, ... , n_2\\), observations standard game sample results.effect color distracter game, \\(\\alpha_1\\) , can estimated \\(\\hat{\\alpha}_1 = \\bar{y}_{1.} - \\bar{y}_{..}\\). Similarly, \\(\\hat{\\alpha}_2 = \\bar{y}_{2.} - \\bar{y}_{..}\\)\nestimates standard game effect, \\(\\alpha_2\\). regression two-sample t-test, residual \\(\\hat{\\epsilon}_{ij}\\) \ndifference observed value corresponding mean response.R2 \\(R^2\\)\nX1 \\(X_1\\)\nX2 \\(X_2\\)\np-values \\(p\\)-values\nm1 \\(\\mu_1\\)\nm2 \\(\\mu_2\\)\na1 \\(\\alpha_1\\)\ns2 \\(\\sigma^2\\)","code":""},{"path":"the-design-and-analysis-of-factorial-experiments-microwave-popcorn.html","id":"the-design-and-analysis-of-factorial-experiments-microwave-popcorn","chapter":"4 The Design and Analysis of Factorial Experiments: Microwave Popcorn","heading":"4 The Design and Analysis of Factorial Experiments: Microwave Popcorn","text":"However beautiful strategy, occasionally\nlook results.\n— Winston ChurchillStatistics viewed whole: understanding process formulating questions, properly designing study, actively collecting meaningful data, deciding\nproperly organize draw conclusions data. Advancements technology made data collection computationally intensive statistical techniques much \nfeasible. one time, many statisticians narrowly defined roles considered \nprimarily “number crunchers.” Today, statisticians characteristically work interdisciplinary\nteams emphasize scientific inference understanding data context.\nInstead emphasizing formulas, computation, mathematical theory, chapter\nuses simple popcorn experiment demonstrate numerous challenges can occur \ndesigning experiments collecting data.activities chapter discuss following:\\(\\bullet\\) Key features well-designed experiment proper data collection.\\(\\bullet\\) Proper determination response variables, experimental factors, levels.\\(\\bullet\\) Building one-way ANOVA discussed Chapter 2 describe multivariate factorial designs.\\(\\bullet\\) Evaluating multiple hypotheses based main effects interaction terms.\\(\\bullet\\) Calculating -group within-group variances needed ANOVA tables balanced factorial designs.\\(\\bullet\\) Calculating effects developing mathematical models.\\(\\bullet\\) Using multiple comparison tests ANOVA tables.","code":""},{"path":"investigation-which-microwave-popcorn-is-the-best.html","id":"investigation-which-microwave-popcorn-is-the-best","chapter":"5 4.1 Investigation: Which Microwave Popcorn Is the Best?","heading":"5 4.1 Investigation: Which Microwave Popcorn Is the Best?","text":"Popcorn staple many college students. many students like popcorn inexpensive\neasy prepare, also whole grain food ’s low fat calories. According Popcorn\nInstitute, Americans consume average 54 quarts popcorn year per person.Two popcorn lovers, also happened taking statistics course, decided test whether \ndifference quality microwave popcorn brands. Yvonne Tue wanted know cheaper brand\npopcorn just good expensive popcorn. students chosen conduct study\nanalyzed two-sample t-test simply compared two brands popcorn. However,\ntwo-sample t-test, need hold many factors constant, type microwave,\ncooking time, storage procedures. Since Yvonne Tue believed factors also\nimpact quality popcorn, decided include additional factors study.Modeling real-world phenomena often requires just one factor explain changes \nresponse. Factorial designs statistical designs structured use factors (.e., explanatory\nvariables) organize meaningful groups treatment conditions. two-sample t-test can considered \nspecial case factorial design just one factor (popcorn brand case) two levels (Brand \nBrand B). Factorial designs powerful statistical tools allow researcher simul-\ntaneously test effects multiple factor-level combinations response interest.KEY CONCEPT\nfactorial designs, explanatory variable called factor specific conditions within \nfactor called levels. study, factor-level combinations called conditions; experi-\nments, often called treatments. Factorial designs often used test effects multiple\nfactors simultaneously, factor two levels.","code":""},{"path":"elements-of-a-well-designed-experiment.html","id":"elements-of-a-well-designed-experiment","chapter":"6 4.2 Elements of a Well-Designed Experiment","heading":"6 4.2 Elements of a Well-Designed Experiment","text":"Unfortunately, many people mistakenly believe statistics process performing mathematical\ncalculations data order examine validity hypothesis. Proper experimental design just \nimportant , important , choice statistical calculations. fact, designing experi-\nments collecting appropriate data often difficult time-consuming aspects conducting\nexperiments.KEY CONCEPT\ngood design attempts answer question(s) interest clearly efficiently possible. \nstatistical analysis good quality data.experiment defined study purposeful changes made controlled conditions \norder identify changes response. experiment imposes treatment subjects experimental units,\nobservational study simply collects data varying conditions without imposing changes.Well-designed experiments conducted controlled conditions make easier isolate impact\ntreatment combination. observational studies, conditions study rarely char-\nacteristic makes two () populations different. Thus, unknown factors may bias results\nunfortunately built observational study.KEY CONCEPT\nexperiments observational studies use sample data draw conclusions larger\npopulation, process, system. often much easier show cause effect relationships \nwell-designed experiment conditions controlled.NOTE\ntexts state experiments can used show cause effect relationships. However, poorly\ndesigned experiments used show causation. addition, observational studies (\ntesting relationship smoking lung cancer) can used show causation (1) \nstrong association explanatory response variables, (2) higher doses associated \nstronger responses (e.g., cigarettes increase likelihood getting cancer), (3) consistent\nresults across many studies, (4) credible explanations cause effect relationship.Experimental design process planning experiment collects right amount type\ndata answer question interest. Several decisions need made experiment \nconstructed executed ensure experimental results valid. Taking time properly design\nexperiment improve precision answers research questions. addition, well-designed experi-\nments often much efficient obtain stronger conclusions studies.first step designing experiment clearly define problem state objectives \nexperiment. often much difficult first appears. data collected, essential\neveryone involved understand objectives experiment, measurements taken, \nmaterial needed, procedures used. Good experimental design typically involves gaining\nbackground knowledge outside field statistics.many possible ways conduct experiment determine effect brand quality\npopcorn. microwave popcorn something students quite familiar , needed \ndetermine brands compare, appropriate cooking time (vary microwave), \ndefine measure “good” popcorn.","code":""},{"path":"elements-of-a-well-designed-experiment.html","id":"identifying-a-response-variable","chapter":"6 4.2 Elements of a Well-Designed Experiment","heading":"6.1 Identifying a Response Variable","text":"Many possible measurements taken microwave popcorn. Yvonne Tue created \ntaste rating texture rating, measured volume kernels, counted number popped kernels, \ncalculated percentage “old maids,” kernels pop bag cooked.Identifying response variable corresponds determining measurements taken. \nexperiment ensure response variable provides information needed response\nmeasurement precise enough address question interest. Yvonne Tue determined \ndefinition “quality” popcorn popcorn highest percentage popped kernels per bag.\nNotice Yvonne Tue counted popped kernels, unpopped kernels, might\ngotten distorted response, since brands may tend kernels per bag.Yvonne Tue initially discussed randomly sampling 20 kernels popped bag calculating percentage popped kernels. However, size shape differences popped \nun-popped kernels made rather difficult simply pull random sample. Thus, order\nensure counts accurate possible, decided count every kernel every bag \nexperiment.also useful discuss range variability responses expected observed. example,\nconducted study conditions typically gave two outcomes, either 0% 100%, \nresponse categorical (yes/popped/popped), analysis based cat-\negorical response variables used. Studies categorical response variables can analyzed\ntechniques chi-square test logistic regression, discussed Chapters 6 7,\nrespectively.percentage popped kernels considered quantitative response variable experiment.\nBackground research showed popcorn companies expected 94% 97% popped kernels,\nbased prior popctorn eating experience, Yvonne Tue expected percentage little\nlower. Yvonne Tue’s study, roughly estimated responses 60% 99%\npopped kernels, average close 90%.KEY CONCEPT\nCare needs taken study conducted ensure response measurement accu-\nrate applicable research question.","code":""},{"path":"elements-of-a-well-designed-experiment.html","id":"identifying-the-factors-and-levels","chapter":"6 4.2 Elements of a Well-Designed Experiment","heading":"6.2 Identifying the Factors and Levels","text":"next step designing experiment investigate factors may importance may\npotentially bias results. Yvonne Tue two microwaves typically used make popcorn,\none dorm lounge one room. lounge microwave “popcorn” setting, cooked\n1 minute 45 seconds, though package instructions brand suggested varying cooking times.\nmicrowaves also power settings. popcorn always popped highest power setting?little research, students found quality popcorn can also affected \nstored. Popcorn stored moisture-rich environment, refrigerator, tends higher percentage\npopped kernels. However, much moisture may cause popcorn gummy texture. Finally,\nYvonne Tue wanted compare relatively expensive brand popcorn (Pop Secret) relatively\ninexpensive brand (Fastco). brands also variety flavors, butter, kettle corn, \ncaramel.Notice discussion factors potential levels based statistical calculations, \nnonstatistical knowledge. Nonstatistical knowledge often essential choosing factors, determining factor\nlevels, interpreting results study.Yvonne Tue decided three factors interest, factors included study \ndetermine different levels impact results:\nFactor 1: popcorn Brand two levels, Fastco Pop Secret\nFactor 2: Microwave two levels, Lounge Room\nFactor 3: cooking Time two levels, 105 seconds 135 secondsIt can sometimes difficult identify reasonable range factor. Yvonne Tue noticed\nbrands popcorn tended burn around 150 seconds (2.5 minutes). Even though cooking pop-\ncorn longer 135 seconds might increase percentage popped kernels, Yvonne Tue decided \navoid cooking times likely cause burning.Yvonne Tue listed suspected extraneous variables, factors need controlled\nexperiment eliminate potential biases. Yvonne Tue decided hold extraneous vari-\nables constant. particular, used highest power setting microwave, stored \npopcorn shelf room, used butter flavor brand. \nvariables control, age popcorn, manufacturing plant prepared bag\npopcorn, different retail stores stored popcorn. account extraneous variables\ncontrol (even thought ), best randomly select bags popcorn \nentire population. Instead, Yvonne Tue best randomly select several bags Fastco \nPop Secret butter popcorn variety stores town. true random sample, Yvonne\nTue careful making statements results study extended larger\npopulation. addition, possible, bag popcorn study randomly allocated factor-\nlevel combination. bags popcorn can randomly assigned cooking time microwave, \nrandomly assigned popcorn brand.KEY CONCEPT\ngood design controls known extraneous variables (often holding constant throughout\nstudy) uses random sampling random allocation control unknown \nuncontrollable extraneous variables.","code":""},{"path":"elements-of-a-well-designed-experiment.html","id":"choosing-a-design","chapter":"6 4.2 Elements of a Well-Designed Experiment","heading":"6.3 Choosing a Design","text":"addition determining conditions test measurements take, order create good\nexperimental design, researcher must properly define units determine units structured. \nexperimental unit smallest part experimental material assigned (randomly, possible) \nfactor-level combination within study. Since Yvonne Tue counted every kernel popcorn, may\nincorrectly assume kernel unit. experiment, units randomly assigned treatments.\nstudy, kernel randomly assigned condition, bag popcorn randomly\nassigned popped particular microwave particular length time. Thus, bags popcorn \nconsidered units Yvonne Tue’s study.KEY CONCEPT\n(1) units similar possible, (2) units randomly assigned treatment combinations, \n(3) large enough sample sizes used, can conclude statistically significant differences\nresponse can explained different treatment combinations.chapter focus completely randomized factorial designs. completely randomized designs,\nunit assigned exactly one factor-level combination. one measurement collected \nunit. following section, use Yvonne Tue’s data simultaneously test effects two\nbrands, two cooking times, two microwaves percentage popped kernels.NOTE\nchapter focuses completely randomized factorial designs, important recognize\ndifference completely randomized, block split-plot (repeated measures) designs.\nunit assigned one factor-level combination, appropriate use completely rand-\nomized design. unit assigned several conditions multiple measurements taken \nunit, complex design structure, block split-plot (repeated measures) design, may \nneeded.\nBlock split-plot (repeated measures) designs also work different types factors. factors\ninterest chapter fixed effects. Fixed effects correspond factors level \nselected specific interest researcher. Random effects correspond factors \nlevels randomly selected larger population. factors completely randomized design\nalso crossed; means every level factor can tested combination every\nlevel every factor. Alternatively, nested factors factors levels occur\nsimultaneously factor levels. Random effects nested factors can analyzed block\nsplit-plot (repeated measures) designs; described Chapter 5.KEY CONCEPT\npossible, straightforward design analysis usually better complex design analy-\nsis. design complicated data collected properly, even advanced\nstatistical techniques may able draw appropriate conclusions experiment.","code":""},{"path":"elements-of-a-well-designed-experiment.html","id":"determining-sample-sizes-for-completely-randomized-designs","chapter":"6 4.2 Elements of a Well-Designed Experiment","heading":"6.4 Determining Sample Sizes for Completely Randomized Designs","text":"units tested, likely statistical analysis identify true differences \nconditions. However, every unit tested cost, important carefully determine many units \npractical test. worth testing additional units gain better understanding unit--unit variability?Yvonne Tue estimated count kernels one bag (popped unpopped) \n10 15 minutes. also thought get close friends count bags exchange\nfree popcorn. Care always taken measuring results. result subjective measure-\nment (taste popcorn quality artwork), clear procedures written , \npossible, people record measurement. popcorn study, counting percentage \npopped kernels per bag objective measurement. long Yvonne Tue trustworthy friends,\nproblem several people help count popcorn kernels.* estimated \nconduct 32 tests four hours.choice 32 tests, instead round number like 30, also related well-designed experiment.\nYvonne Tue found , based choice factors levels, total eight treatment\ncombinations. Table 4.1 lists eight possible treatment conditions can “assigned” bag \npopcorn. Yvonne Tue wanted balanced design, design number units \nassigned (randomly selected ) condition. Balanced designs often easier analyze \nlikely identify true differences effects different conditions.3 Yvonne Tue wanted conduct\nbalanced design, needed conduct tests multiple 8 (16, 24, etc.).","code":""},{"path":"elements-of-a-well-designed-experiment.html","id":"table-4.1-treatment-combinations","chapter":"6 4.2 Elements of a Well-Designed Experiment","heading":"6.4.1 Table 4.1: Treatment Combinations","text":"(#tab:table4.1)Table 4.1 possible treatment conditions three‐factor popcorn study.MATHEMATICAL NOTE\nTable 4.1 lists variables standard order. Listing conditions standard order simple technique\nensures factor combination listed exactly . first variable alternates levels every\nrow. second variable, levels alternate every row. third variable alternates every fourth\nrow. process can work multiple factors multiple levels. four factors, \ntwo levels, fourth column alternate every eight rows. studies two levels,\nsimply ensure current variable lists level exactly prior combinations","code":""},{"path":"elements-of-a-well-designed-experiment.html","id":"activity-determining-the-number-of-treatment-combinations","chapter":"6 4.2 Elements of a Well-Designed Experiment","heading":"6.4.2 Activity: Determining the Number of Treatment Combinations","text":"Assume want use three cooking times popping popcorn instead two. List possible\ntreatment combinations can assigned. many ?Without listing possibilities, calculate many treatment combinations exist design\ntested five brands three microwaves four cooking times.","code":""},{"path":"elements-of-a-well-designed-experiment.html","id":"analyzing-a-two-way-factorial-design","chapter":"6 4.2 Elements of a Well-Designed Experiment","heading":"6.5 4.3 Analyzing a Two-Way Factorial Design","text":"Since several factors included experiment, also several hypotheses tested. Yvonne Tue actually six research questions, discussed Section 4.4. keep calculations simple, section assume two factors tested (Brand Time). leads three hypotheses corresponding Brand Time factors:\\(H_{01}: \\mu_{\\text{Fastco}} = \\mu_{\\text{PopSecret}}\\), difference mean response (PopRate) two Brands.\n\\(\\quad H_{a1}: \\mu_{\\text{Fastco}} \\neq \\mu_{\\text{PopSecret}}\\), two Brand means different.\\(H_{01}: \\mu_{\\text{Fastco}} = \\mu_{\\text{PopSecret}}\\), difference mean response (PopRate) two Brands.\n\\(\\quad H_{a1}: \\mu_{\\text{Fastco}} \\neq \\mu_{\\text{PopSecret}}\\), two Brand means different.\\(H_{02}: \\mu_{105} = \\mu_{135}\\), difference mean PopRate two Times.\n\\(\\quad H_{a2}: \\mu_{105} \\neq \\mu_{135}\\), two Time means different.\\(H_{02}: \\mu_{105} = \\mu_{135}\\), difference mean PopRate two Times.\n\\(\\quad H_{a2}: \\mu_{105} \\neq \\mu_{135}\\), two Time means different.\\(H_{03}:\\) Brand influence Time affects PopRate. equivalent stating \\(H_{03}:\\) Time influence Brand affects PopRate \\(H_{03}:\\) effect Time Brands \\(H_{03}:\\) interaction Time Brand.\\(H_{a3}:\\) Brand influences Time affects PopRate \\(H_{a3}:\\) interaction Brand Time).\\(H_{03}:\\) Brand influence Time affects PopRate. equivalent stating \\(H_{03}:\\) Time influence Brand affects PopRate \\(H_{03}:\\) effect Time Brands \\(H_{03}:\\) interaction Time Brand.\\(H_{a3}:\\) Brand influences Time affects PopRate \\(H_{a3}:\\) interaction Brand Time).often written \\(H_{0,1}:\\) Time × Brand interaction (\\(\\alpha_{105} - \\alpha_{135} = 0\\)), \\(\\alpha\\) called effect size. example, \\(\\alpha_{105} = \\mu_{105\\,.} - \\mu_{\\,..}\\), \\(\\mu\\) overall grand mean responses.Factorial designs efficient much information can calculated, p-values \nmultiple hypothesis tests, without requiring experimental units typical two-sample t-test.\nFactorial designs beneficial situations experimental units expensive difficult obtain.\nnext sections discuss organize draw conclusions hypotheses \nfactorial design two factors, also called two-way factorial design.","code":""},{"path":"elements-of-a-well-designed-experiment.html","id":"visualizing-the-data","chapter":"6 4.2 Elements of a Well-Designed Experiment","heading":"6.6 Visualizing the Data","text":"formal analysis done, carry informal analysis looking graph. individual value plot data shown Figure 4.1.","code":""},{"path":"elements-of-a-well-designed-experiment.html","id":"activity-visualizing-and-summarizing-data","chapter":"6 4.2 Elements of a Well-Designed Experiment","heading":"6.6.1 Activity: Visualizing and Summarizing Data","text":"Use Figure 4.1 compare PopRate four factor-level combinations. four groups appear similar means similar standard deviations? outliers (extreme observations don’t seem fit rest data)? Describe patterns see data.Calculate average PopRate Brand Time. Calculate overall average PopRate.Use data set labeled Popcorn calculate appropriate summary statistics (median, mean, standard deviation, range, etc.) four groups. Fastco brand, calculate difference average PopRate two cooking times. Pop Secret brand.","code":""},{"path":"elements-of-a-well-designed-experiment.html","id":"notation-for-multiple-explanatory-variables","chapter":"6 4.2 Elements of a Well-Designed Experiment","heading":"6.7 Notation for Multiple Explanatory Variables","text":"Table 4.2 shows Popcorn data set organized Brand Time factors. four treatment combinations eight observations. data also provided file Popcorn.[[[Figure 4.1 goes ]]]\n(#tab:table4.2)(#tab:table4.2)Table 4.2 Popcorn study data: PopRate bag popcorn sample mean corresponding factor‐level combination.\nTable 4.3 useful visualizing differences similarities among meaningful groups within \ndata set: overall average, two Brand groups, two Time groups, four factor-level combinations. Table 4.3 also includes mathematical notation representing mean. example, y11. represents \nmean 8 responses first Brand, Fastco, first Time group, 105 seconds y2.. represents\nmean 16 responses second Brand group, Pop Secret. y… represents overall mean \n32 responses\n(#tab:table4.3)(#tab:table4.3)Table 4.3 Popcorn study sample means: mean PopRate (per 100 kernels) nine meaningful groups sample data.\nMATHEMATICAL NOTE\ndot subscript indicates average taken values subscript. key recognize groups identified subscripts. Brand first subscript, Time second. individual observation Brand Time factor-level combination represented third subscript. example, 4th observation Table 4.2 Fastco brand (brand 1) 135-second time (time 2) group \\(y_{1,2,4} = 71.50\\). average 8 observations Fastco brand (brand 1) 135-second time group represented \\(\\bar y_{1,2,.} = 82.38\\). addition, \\(\\bar y_{1..}\\) average response 16 Fastco brand (brand 1 observations), \\(\\bar y_{.1.}\\) average response 16 105-second times (time 1 observations). \\(\\bar y_{...}\\) overall average PopRate, averaged observations Brand cooking time. , \\(\\bar y_{...}\\) overall average PopRate.","code":""},{"path":"elements-of-a-well-designed-experiment.html","id":"activity-understanding-notation","chapter":"6 4.2 Elements of a Well-Designed Experiment","heading":"6.7.1 Activity: Understanding Notation","text":"notation used describe sample average 135-second group?Explain difference \\(\\bar y_{21.}\\) \\(\\bar y_{12.}\\).","code":""},{"path":"elements-of-a-well-designed-experiment.html","id":"comparing-variances","chapter":"6 4.2 Elements of a Well-Designed Experiment","heading":"6.8 Comparing Variances*","text":"Figure 4.1 Table 4.3 indicate difference Time means much larger difference Brand means. addition, difference Time means much larger Pop Secret brand Fastco brand. section, conduct analysis, called analysis variance, find p-values testing three hypotheses stated earlier underlying mean responses.Analysis variance (ANOVA) conducted comparing variability groups variability within groups. example, variability Brand means (groups) appear large compared variation responses within two Brand levels (within groups)? ANOVA, measures variability called mean squares. example, variability brands called mean square brand denoted \\(MS_{Brand}\\). actual practice, following ANOVA calculations done\ncomputer software instead hand. reason working equations detail \nbetter illustrate logic behind using ANOVA determine group variability significant (.e.,\ndetermine whether can reject null hypotheses).-Group Variability create measure variability Brand means (\\(MS_{Brand}\\)), calculate weighted variance Brand group means, using size group weight. weighted variance Brand group means calculated following equation:\\[\\begin{equation}\n\\mathrm{MS}_{\\mathrm{Brand}}\n= \\frac{\\sum_{=1}^2 n_{.} \\times (\\bar y_{..} - \\bar y_{...})^2}{2 - 1}\n= \\frac{16\\times(81.8 - 81.3)^2 + 16\\times(80.9 - 81.3)^2}{2 - 1}\n\\tag{4.1}\n\\end{equation}\\]\\(n_{.}\\) number observations brand \\(\\). study, \\(n_{.} = 16\\). Notice Equation (4.1) looks similar typical variance calculation:\\(\\bullet\\) two observed group means: 81.8 80.9.\\(\\bullet\\) spread measured summing squared distance observed group mean overall mean dividing number group means minus one.variance calculation, finding average squared distance (mean squared distance,\ndenoted \\(MS_{Brand}\\)). difference calculation typical variance calculation use weights:\\(\\bullet\\) observed mean based group size 16; group sample size (\\(n_{.} = 16\\)) multiplied\nsquared distance.MATHEMATICAL NOTE\nstudy, balanced design (equal sample sizes). However, formulas allow unbalanced designs.calculation variability Time means (\\(MS_{Time}\\)) similar Equation (4.1):\\[\\begin{equation}\n\\mathrm{MS}_{\\mathrm{Time}}\n= \\frac{\\sum_{j=1}^2 n_{.j} \\times (\\bar y_{.j.} - \\bar y_{...})^2}{2 - 1}\n\\tag{4.2}\n\\end{equation}\\]first two hypotheses beginning section correspond questions main factors\nincorporated experiment, Brand Time. third hypothesis focuses whether impact \none variable (Time) depends second variable (Brand). called interaction effect.Table 4.3 provides evidence interaction Brand Time. Fastco brand popcorn,\nlonger cooking time increases percentage popped kernels \\(\\bar y_{12.}\\) - \\(\\bar y_{11.}\\) = 82.38 - 81.13 = 1.25,\nincrease Pop Secret brand many times larger: \\(\\bar y_{22.}\\) - \\(\\bar y_{21.}\\) = 86.42 - 75.44 = 10.98.test interaction effect (third hypothesis), first measure variability four\ngroups (Brand Time combination) subtract squared values main factors.\\[\\begin{equation}\n\\mathrm{MS}_{\\mathrm{Brand}\\times\\mathrm{Time}}\n= \\frac{\\sum_{=1}^2 \\sum_{j=1}^2 n_{ij}(\\bar y_{ij\\,.} - \\bar y_{..})^2\n    - \\sum_{=1}^2 n_{.}(\\bar y_{..} - \\bar y_{...})^2\n    - \\sum_{j=1}^2 n_{.j}(\\bar y_{.j.} - \\bar y_{...})^2}\n{4 - 1 - 1 - 1}\n\\tag{4.3}\n\\end{equation}\\]key aspect Equation (4.3) calculates squared distance four factor-level\ngroup means overall mean accounting main factor group means. Thus, calculation \nestimate spread four group means accounting influence main factor means.denominator mean square interaction based denominators \\(\\mathrm{MS}_{\\mathrm{Brand}}\\) Equation\n(4.1) \\(\\mathrm{MS}_{\\mathrm{Time}}\\) Equation (4.2). example, four factor-level group means. Thus, denomina-\ntor calculated 4 - 1- (denominator \\(\\mathrm{MS}_{\\mathrm{Brand}}\\)) - (denominator \\(\\mathrm{MS}_{\\mathrm{Time}}\\)) = 4 - 1 - 1 - 1.\nDetails deriving mean squares provided extended activities.KEY CONCEPT\ninteraction term simply measure spread four factor-level group means.\nmeasures remaining spread means adjusting differences main factor\nmeans.Within-Group Variability best estimate variability within group (MSE) simply \nweighted average sample variances within four factor-level groups:\\[\\begin{equation}\n\\mathrm{MSE}\n= \\frac{\\sum_{=1}^2 \\sum_{j=1}^2 (n_{ij} - 1)s_{ij}^2}\n{(n_{11}-1) + (n_{12}-1) + (n_{21}-1) + (n_{22}-1)}\n= \\frac{(8-1)s_{11}^2 + (8-1)s_{12}^2 + (8-1)s_{21}^2 + (8-1)s_{22}^2}\n{4\\times(8-1)}\n\\tag{4.4}\n\\end{equation}\\]\\(s_{ij}^2\\) sample variance group representing brand time j. implicit assumption \nvariances possible responses four group populations , makes\nsense “pool” sample variances single estimate overall response variability. equal variance assumption\nkey validity ANOVA statistical method. variability within group quite\ndifferent, MSE may appropriate estimate. often useful create individual value plots side--side\nboxplots groups check spreads sample groups roughly similar.MATHEMATICAL NOTE\ngroups data factor-level combination different sample sizes least one\ngroup small sample size (e.g., less 5 units per group), ANOVA may appropriate. \ngroup(s) smallest sample size (s) unusually high variance, MSE likely underes-\ntimate true variance ANOVA likely incorrectly reject null hypothesis (conclude \ndifferences really differences group means). group(s) smallest\nsample size(s) unusually small variance, MSE likely overestimate true variance. \nlarger MSE may cause us incorrectly fail reject null hypothesis (fail detect true differences).Equation (4.4) often called mean square error (MSE) responses, “error” represents \nunit--unit variability response can’t explained main factors interactions. \nnow ready calculate test statistic corresponding three hypotheses beginning section.F-Statistic F-statistic ratio -group variability (variation factor-level\naverages) within-group variability (pooled estimate variability within factor-level combina-\ntion): (MS factor)/MSE. Mathematical theory proves assumptions ANOVA model hold,\nF-statistic follows F-distribution degrees freedom corresponding denominators \nMS factor tested MSE. p-value gives likelihood observing F-statistic \nleast large, assuming true population factor equal level means. Thus, p-value \nsmall, conclude difference level means. Additional details provided \nextended activities end chapter.KEY CONCEPT\nF-statistic simply ratio -group variability within-group variability.","code":""},{"path":"elements-of-a-well-designed-experiment.html","id":"activity-calculating-f-statistics","chapter":"6 4.2 Elements of a Well-Designed Experiment","heading":"6.8.1 Activity: Calculating F-Statistics","text":"Use Equation (4.1) estimate \\(\\mathrm{MS}_{\\mathrm{Brand}}\\), variability Brand means.Use Equation (4.1) estimate \\(\\mathrm{MS}_{\\mathrm{Brand}}\\), variability Brand means.Calculate variability Time means, \\(\\mathrm{MS}_{\\mathrm{Time}}\\). Explain key differences Equation (4.1) Equation (4.2).Calculate variability Time means, \\(\\mathrm{MS}_{\\mathrm{Time}}\\). Explain key differences Equation (4.1) Equation (4.2).Use Equation (4.3) estimate \\(\\mathrm{MS}_{\\mathrm{Brand}\\times\\mathrm{Time}}\\).Use Equation (4.3) estimate \\(\\mathrm{MS}_{\\mathrm{Brand}\\times\\mathrm{Time}}\\).Calculate \\(F\\)-statistics corresponding three hypothesis tests:\n\\(F_1 = \\frac{\\mathrm{MS}_{\\mathrm{Brand}}}{\\mathrm{MSE}},\\quad F_2 = \\frac{\\mathrm{MS}_{\\mathrm{Time}}}{\\mathrm{MSE}},\\quad F_3=\\frac{\\mathrm{MS}_{\\mathrm{BrandTime}}}{\\mathrm{MSE}}\\)Calculate \\(F\\)-statistics corresponding three hypothesis tests:\n\\(F_1 = \\frac{\\mathrm{MS}_{\\mathrm{Brand}}}{\\mathrm{MSE}},\\quad F_2 = \\frac{\\mathrm{MS}_{\\mathrm{Time}}}{\\mathrm{MSE}},\\quad F_3=\\frac{\\mathrm{MS}_{\\mathrm{BrandTime}}}{\\mathrm{MSE}}\\)think largest smallest possible values \\(F\\)-statistic?think largest smallest possible values \\(F\\)-statistic?Use technology instructions provided CD check answers. Submit software output. Note \\(p\\)-value \\(F\\)-statistic provided. State conclusions three hypotheses based \\(p\\)-values.Use technology instructions provided CD check answers. Submit software output. Note \\(p\\)-value \\(F\\)-statistic provided. State conclusions three hypotheses based \\(p\\)-values.Don’t surprised hand calculations Question 11 differ somewhat software\noutput . data Table 4.3 rounded one decimal place, calculations Questions 8\n11 accurate statistical softwareExplain large \\(F\\)-statistic corresponds small \\(p\\)-value referring definition \\(F\\)-statistic: ratio -group variability within-group variability.Checking Assumptions described Chapter 2, assumptions need checked ensure \\(p\\)-value ANOVA \\(F\\)-test reliable:\\(\\bullet\\) observations within group (factor-level combination) independent identically distributed.\\(\\bullet\\) group equal variances.\\(\\bullet\\) residual values follow normal distribution mean zero.Examine individual value plot Figure 4.1 comment assumptions hypothesis tests. evidence skewness outliers may cause us doubt normal\nassumption PopRate within factor-level combination Brand Time?Examine individual value plot Figure 4.1 comment assumptions hypothesis tests. evidence skewness outliers may cause us doubt normal\nassumption PopRate within factor-level combination Brand Time?Figure 4.1 indicate spread group appears roughly similar, equal variance\nassumption seems reasonable? Another informal check equal variance assumption can\ndone calculating ratio maximum sample standard deviation minimum sample\nstandard deviation. ratio less two, can generally assume strong\nevidence equal variance assumption. Compare standard deviations four treat-\nment level combinations determine \\[\n\\frac{\\max(s_{ij})}{\\min(s_{ij})} < 2.\n\\]Figure 4.1 indicate spread group appears roughly similar, equal variance\nassumption seems reasonable? Another informal check equal variance assumption can\ndone calculating ratio maximum sample standard deviation minimum sample\nstandard deviation. ratio less two, can generally assume strong\nevidence equal variance assumption. Compare standard deviations four treat-\nment level combinations determine \\[\n\\frac{\\max(s_{ij})}{\\min(s_{ij})} < 2.\n\\]Create normal probability plot histogram residuals Question 13. appear \nresiduals follow normal distribution?Create normal probability plot histogram residuals Question 13. appear \nresiduals follow normal distribution?CAUTION\nstatisticians reject equal variance assumption ratio standard deviations \ngreater 3 instead 2. Others recommend formal tests used test equal variances.\nHowever, tests, Bartlett’s test, sensitive nonnormality. Box criticized using\nBartlett’s test preliminary test equal variances, saying “make preliminary test variances\nrather like putting sea rowing boat find whether conditions sufficiently calm\nocean liner leave port.” Levene’s test homogeneity variance less sensitive departures\nnormality.","code":""},{"path":"elements-of-a-well-designed-experiment.html","id":"interpreting-interaction-terms","chapter":"6 4.2 Elements of a Well-Designed Experiment","heading":"6.9 Interpreting Interaction Terms","text":"Question 13, p-value corresponding third hypothesis test listed beginning section\n0.04. demonstrates interaction: effect one variable (Time) response depends \nsecond variable (Brand). Figure 4.2 provides side--side boxplot interaction plot Popcorn\ndata. interaction plot simply plot four factor-level group means shown Table 4.3. plots\nshow brands, average PopRate increases cooking time changes 105 135\nseconds. However, change means Fastco brand small compared change observed\nPop Secret brand.[[[Figure 4.2 goes ]]]interaction plot helpful visualizing effect one factor can depend another factor,\nespecially multiple factors study. lines interaction plot essentially\nparallel, effect first variable influenced second variable. Nonparallel lines indicate \ninteraction main factors (e.g., effect Time depends Brand). However, interaction plot\nshow within group variability, p-value ANOVA can used determine\ninteraction significant. p-value 0.04 shows observed interaction effect large \nunlikely occurred just chance. conclude \\(H_{a3}\\) true: Brand influences effect \nTime PopRate.","code":""},{"path":"elements-of-a-well-designed-experiment.html","id":"analyzing-a-three-way-factorial-design","chapter":"6 4.2 Elements of a Well-Designed Experiment","heading":"6.10 4.4 Analyzing a Three-Way Factorial Design","text":"One advantage ANOVA analysis can easily extended multiple factors many levels. section, three factors popcorn study (Brand, Time, Microwave) simultaneously examined influence PopRate three-way ANOVA (also called three-\nfactor ANOVA).Using 32 observations Popcorn data, three-way ANOVA allow us simultaneously test following six hypotheses:\\(H_{01}: \\mu_{\\text{Fastco}} = \\mu_{\\text{PopSecret}}\\)\\(H_{a1}: \\mu_{\\text{Fastco}} \\neq \\mu_{\\text{PopSecret}}\\)\\(H_{02}: \\mu_{105} = \\mu_{135}\\)\\(H_{a2}: \\mu_{105} \\neq \\mu_{135}\\)\\(H_{03}:\\) Brand influence Time affects PopRate\\(H_{a3}:\\) interaction Brand Time\\(H_{04}: \\mu_{\\text{Room}} = \\mu_{\\text{Lounge}}\\)\\(H_{a4}: \\mu_{\\text{Room}} \\neq \\mu_{\\text{Lounge}}\\)\\(H_{05}:\\) Microwave influence Brand affects PopRate\\(H_{a5}:\\) interaction Microwave Brand\\(H_{06}:\\) Microwave influence Time affects PopRate\\(H_{a6}:\\) interaction Microwave TimeNOTE\nalso reasonable test three-way interaction, \\(H_{a3}\\); size effect Time level Brand also depends Microwave. practice, three-way interaction effect may difficult interpret, researchers choose include analysis. impacts including additional tests described Chapter 5.","code":""},{"path":"elements-of-a-well-designed-experiment.html","id":"activity-conducting-a-three-way-anova","chapter":"6 4.2 Elements of a Well-Designed Experiment","heading":"6.10.1 Activity: Conducting a Three-Way ANOVA","text":"Create individual value plots eight possible factor-level groups listed Table 4.1.\nsee patterns PopRate among groups?\nspread responses within group look roughly similar?\noutliers unusual observations group(s)?\nsee patterns PopRate among groups?spread responses within group look roughly similar?outliers unusual observations group(s)?Calculate eight group standard deviations. largest standard deviation two\ntimes smallest standard deviation, typically appropriate assume equal variances \npopulation responses within group. appropriate assume equal variances Popcorn\nstudy?Use statistical software package simultaneously test six hypotheses given beginning \nsection.\nSubmit appropriate software output.\nCreate normal probability plot (described Chapter 2) histogram residuals. \nresiduals consistent assumption normal distribution?\nUse p-value corresponding hypothesis state conclusions.\nAddress random sampling random allocation influence conclusions.\nSubmit appropriate software output.Create normal probability plot (described Chapter 2) histogram residuals. \nresiduals consistent assumption normal distribution?Use p-value corresponding hypothesis state conclusions.Address random sampling random allocation influence conclusions.Determine whether \\(\\mathrm{MS}_{\\mathrm{Brand}}\\), \\(\\mathrm{MS}_{\\mathrm{Time}}\\), \\(\\mathrm{MS}_{\\mathrm{BrandTime}}\\) Question 13. Explain \\(F\\)-statistics changed.completely randomized designs, F-statistics corresponding tests main factor \ninteraction use denominator. mean square main factor (\\(\\mathrm{MS}_{\\mathrm{Brand}}\\), \\(\\mathrm{MS}_{\\mathrm{Time}}\\) measurement variability level means. Since balanced design, level means\nfactor farther apart, corresponding mean square F-statistics larger. Thus, main effects\nplot shown Figure 4.3 allows us quickly see Time factor significant (smallest\np-value) Brand factor least significant[[[Figure 4.3 goes ]]]Figure 4.3 also provides interaction plots corresponding three hypotheses tests interactions.\nUsing logic, Figure 4.3 shows hypothesis test corresponding Brand Time inter-\naction smallest p-value. effect Time effect Brand somewhat\ninfluenced Microwave, effect Time influenced changing Brand.","code":""},{"path":"what-can-we-conclude-from-the-popcorn-study.html","id":"what-can-we-conclude-from-the-popcorn-study","chapter":"7 4.5 What Can We Conclude from the Popcorn Study?","heading":"7 4.5 What Can We Conclude from the Popcorn Study?","text":"Yvonne Tue’s study illustrated essential carefully plan study data col-\nlected. data collected properly, typically statistical analysis can draw accurate\nconclusions. study well designed data reliable, analysis often straight forward \nstatistical software.units study (Bags) randomly assigned Time Microwave factor-level combina-\ntions. ANOVA results allowed us conclude Time causes difference PopRate. addition, \nfound evidence Brand Time interaction.bags popcorn study true random sample popcorn produced two\nbrands. Thus, need careful making conclusions extend larger population. \nefforts students made properly collect random samples various stores around college\ntown, author chapter feel fairly comfortable stating conclusions hold two\nbrands butter-flavored microwave popcorn town time study.","code":""},{"path":"paper-towels-developing-a-statistical-model-for-a-two-way.html","id":"paper-towels-developing-a-statistical-model-for-a-two-way","chapter":"8 4.6 Paper Towels: Developing a Statistical Model for a Two-Way","heading":"8 4.6 Paper Towels: Developing a Statistical Model for a Two-Way","text":"final project introductory statistics class, several students decided conduct study test \nstrength paper towels. Several television advertisements claimed certain brand paper towel \nstrongest, students wanted determine really difference. students sampled\n26 towels two brands paper towels, Comfort Decorator.NOTE\nRecall random sampling needed extend results larger population. students ran-\ndomly sampled 26 towels just one roll, conclusions hold roll. Ideally \nrandomly purchased 26 rolls brand multiple locations randomly\nselected one towel per rollBefore data collected, students determined following conditions held \nconstant possible throughout study:\\(\\bullet\\) Paper towels selected size.\\(\\bullet\\) towels held four corners two people.\\(\\bullet\\) Weights (10, 25, 50, 100, 250 grams) slowly added center towel third person broke.study, two factors. One two levels, Comfort (Brand C) Decorator (Brand D), \nthree levels (0, 5, 15 drops water applied center paper towel). leads \n\\(2 \\times 3 = 6\\) conditions, called factor-level combinations factorial combinations:\nBrand C & 0 drops water\nBrand C & 5 drops water\nBrand C & 15 drops water\nBrand D & 0 drops water\nBrand D & 5 drops water\nBrand D & 15 drops waterTwenty-six sheets tested six factor-level combinations. Thus, 156 experimental\nunits used study. response variable breaking strength paper towel grams. Breaking\nstrength defined total weight towel successfully held. next additional weight caused\ntowel break.three null hypotheses corresponding two-factor design follows:\\(H_{01}:\\) difference mean strength two brands paper towel\n\\(H_{a1}:\\) two brand means different\\(H_{01}:\\) difference mean strength two brands paper towel\n\\(H_{a1}:\\) two brand means different\\(H_{02}:\\) difference mean strength 0, 5, 15 drops water used\\(H_{a2}:\\) mean strength least one water amount group different others\\(H_{02}:\\) difference mean strength 0, 5, 15 drops water used\\(H_{a2}:\\) mean strength least one water amount group different others\\(H_{03}:\\) amount water influence brand affects strength\n\\(H_{03}:\\) effect amount water strength brands\n\\(H_{03}:\\) interaction brand water\\(H_{a3}:\\) interaction brand water\\(H_{03}:\\) amount water influence brand affects strength\n\\(H_{03}:\\) effect amount water strength brands\n\\(H_{03}:\\) interaction brand water\\(H_{a3}:\\) interaction brand waterTable 4.4 represents data paper towel study. six cells 26 observations.\ncomplete data set file PaperTowels. observations shown, Table 4.4 helps us\nunderstand data structure. data collected, averages meaningful groups \ndata can calculated shown Table 4.5.","code":""},{"path":"paper-towels-developing-a-statistical-model-for-a-two-way.html","id":"extended-activity-algebraic-notation","chapter":"8 4.6 Paper Towels: Developing a Statistical Model for a Two-Way","heading":"8.0.1 Extended Activity: Algebraic Notation","text":"Data set: PaperTowels\n20. values Table 4.4 represented \\(\\bar y_{213}\\) \\(\\bar y_{122}\\)?\n21. Give proper algebraic notation observation representing 3rd paper towel Brand D 15 drops water.\n22. values \\(\\bar y_{.3.}\\) \\(\\bar y_{21.}\\)?\n23. Complete Table 4.5 calculating three missing averages.","code":""},{"path":"paper-towels-developing-a-statistical-model-for-a-two-way.html","id":"calculating-effects","chapter":"8 4.6 Paper Towels: Developing a Statistical Model for a Two-Way","heading":"8.1 Calculating Effects","text":"data structure becomes complex, statistical model becomes useful describing population(s) data may come. Generally, statistical models consist mean response random error term (details provided Chapter 2). mean response describes expected (mean) breaking strength. Figure 4.4 useful visualizing meaningful groups within model contribute mean response model: grand mean, two brand groups, three water amount groups, six factor-level combination groups.random error term follows overall pattern can modeled probability distribution (e.g., normal distribution). error term incorporates reality observations vary within factor-level combination. Even weights applied brand paper towel, using water amount, observed breaking strength may .labels (e.g., grand mean, group effects, random errors) symbols Figure 4.4 described :\\(y_{ijk}\\): \\(k\\)th observed breaking strength (\\(k = 1, 2, \\dots, 26\\)) brand \\(\\) water amount \\(j\\)\\(\\mu\\): overall mean breaking strength entire population paper towels across brands water amounts (also called grand mean)\\(\\alpha_i\\): brand effect (\\(= 1, 2\\)), \\(\\alpha_1\\) effect Brand C \\(\\alpha_2\\) effect Brand D\\(\\beta_j\\): amount water effect (\\(j = 1, 2, 3\\)), \\(\\beta_1\\) represents effect 0 drops water\\((\\alpha\\beta)_{ij}\\): interaction effect, \\((\\alpha\\beta)_{23}\\) represents Brand D/15 drops water interaction effect\\(\\varepsilon_{ijk}\\): random error—difference \\(k\\)th observed value (\\(k = 1, 2, \\dots, 26\\)) population mean breaking strength brand \\(\\) water amount \\(j\\)[[[Figure 4.4 goes ]]]Notice \\(2 \\times 3 \\times 26 = 156\\) observed strength measurements represents one 156 equations Figure 4.4. structure data now used write statistical model data:\n\\[\ny_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}\n\\quad\\text{} = 1,2;\\; j = 1,2,3;\\; k = 1,2,\\dots,26\n\\tag{4.5}\n\\]KEY CONCEPT\nIdentifying meaningful groups within data set (data structure) first step developing statistical model population(s) data come.Table 4.5 shows Strength average changes 1772.8 1123.4 change Brand C Brand D paper towels. difference smaller differences due changes water amount. Main effects calculated measure impact changing levels factor model. main effect difference factor-level average grand mean. example:\\[\n\\hat\\alpha_1\n= \\text{effect Brand C}\n= \\text{Brand C mean} - \\text{grand mean}\n= \\bar y_{1\\cdot\\cdot} - \\bar y_{\\cdot\\cdot\\cdot}\n= 1772.8 - 1448.1\n= 324.7,\n\\]\n\\[\n\\hat\\beta_3\n= \\text{effect 15 drops water}\n= \\text{15 drops mean} - \\text{grand mean}\n= \\bar y_{\\cdot\\cdot3} - \\bar y_{\\cdot\\cdot\\cdot}\n= 423.6 - 1448.1\n= -1024.5\n\\tag{4.6}\n\\]NOTE\\(\\mu,\\alpha_i,\\beta_j,(\\alpha\\beta)_{ij}\\) Equation (4.5) population parameters. Statistics \\(\\hat\\alpha_1\\) \\(\\hat\\beta_3\\) Equation (4.6) used estimate population effect sizes.","code":""},{"path":"paper-towels-developing-a-statistical-model-for-a-two-way.html","id":"extended-activity-estimating-main-effects","chapter":"8 4.6 Paper Towels: Developing a Statistical Model for a Two-Way","heading":"8.1.1 Extended Activity: Estimating Main Effects","text":"Data set: PaperTowelsUse PaperTowels data estimate effect Brand D explain symmetry find effect Brand C calculated .Use PaperTowels data estimate effect Brand D explain symmetry find effect Brand C calculated .main effects plot graph plots average response level factor. properly compare effect sizes, vertical axis factor. Use statistical software create main effects plot. Identify label following values plot:\\(\\bar y_{1\\cdot\\cdot},\\bar y_{\\cdot\\cdot3},\\bar y_{\\cdot\\cdot\\cdot}, \\hat\\alpha_1, \\hat\\beta_3.\\)main effects plot graph plots average response level factor. properly compare effect sizes, vertical axis factor. Use statistical software create main effects plot. Identify label following values plot:\\(\\bar y_{1\\cdot\\cdot},\\bar y_{\\cdot\\cdot3},\\bar y_{\\cdot\\cdot\\cdot}, \\hat\\alpha_1, \\hat\\beta_3.\\)NOTE\nstill balanced design, since \\(n_{ij}\\) factor-level combination. However, since sample sizes every group level (78 towels Brand mean 52 towels Water level), factor largest difference means necessarily correspond smallest p-value.addition determining main effects factor, often critical identify multiple factors interact affecting results. interaction occurs one factor affects response variable differently depending second factor. calculate effect brand water interaction, take average particular factor combination minus grand mean corresponding main effects:\\[\\begin{align}\n\\text{Interaction effect Brand C 15 drops water}\n  &=\n    \\begin{aligned}[]\n      \\text{average Brand C, 15 drops group}\\\\\n      \\quad\n      -\\bigl(\\text{effect Brand C}\n             +\\;\\text{effect 15 drops}\n             +\\;\\text{grand mean}\\bigr)\n    \\end{aligned}\n    \\nonumber\n\\\\[6pt]\n&= \\bar y_{13\\cdot} - \\bigl[\\hat\\alpha_1 + \\hat\\beta_3 + \\bar y_{\\cdot\\cdot\\cdot}\\bigr]\n    \\nonumber\n\\\\[6pt]\n&= \\bar y_{13\\cdot}\n  - \\bigl[(\\bar y_{1\\cdot\\cdot}-\\bar y_{\\cdot\\cdot\\cdot})\n         +(\\bar y_{\\cdot3\\cdot}-\\bar y_{\\cdot\\cdot\\cdot})\n         +\\bar y_{\\cdot\\cdot\\cdot}\\bigr]\n    \\nonumber\n\\\\[6pt]\n&= 401.0 - \\bigl[324.7 + (-1024.5) + 1448.1\\bigr]\n    \\nonumber\n\\\\[6pt]\n&= -347.3\n\\tag{4.7}\n\\end{align}\\]estimate Brand C 15 drops water interaction effect Equation (4.7) tells us best\nestimate paper towel strength group reduced additional 347.3 take\naccount influencing factors (grand mean main effects).","code":""},{"path":"paper-towels-developing-a-statistical-model-for-a-two-way.html","id":"extended-activity-calculating-interaction-effects","chapter":"8 4.6 Paper Towels: Developing a Statistical Model for a Two-Way","heading":"8.1.2 Extended Activity: Calculating Interaction Effects","text":"Data set: PaperTowelsShow \\(\\bar y_{ij\\cdot} - \\bar y_{\\cdot\\cdot} - \\bar y_{\\cdot j\\cdot} + \\bar y_{\\cdot\\cdot\\cdot}\\) equivalent \\(ij\\)th interaction effect.Show \\(\\bar y_{ij\\cdot} - \\bar y_{\\cdot\\cdot} - \\bar y_{\\cdot j\\cdot} + \\bar y_{\\cdot\\cdot\\cdot}\\) equivalent \\(ij\\)th interaction effect.Calculate five interaction effects. Hand draw Figure 4.4 fill effect sizes observed values (.e., replace \\(\\mu, \\alpha_i, \\beta_j,\\) \\((\\alpha\\beta)_{ij}\\) estimates data). fill observations error terms (\\(y_{ijk}\\) \\(\\varepsilon_{ijk}\\)).Calculate five interaction effects. Hand draw Figure 4.4 fill effect sizes observed values (.e., replace \\(\\mu, \\alpha_i, \\beta_j,\\) \\((\\alpha\\beta)_{ij}\\) estimates data). fill observations error terms (\\(y_{ijk}\\) \\(\\varepsilon_{ijk}\\)).Create interaction plot. appear evidence interaction effect?Create interaction plot. appear evidence interaction effect?Draw diagram similar Figure 4.4 two-way factorial design four levels first factor, three levels second factor, two observations per factor-level combination Equation (4.5).Draw diagram similar Figure 4.4 two-way factorial design four levels first factor, three levels second factor, two observations per factor-level combination Equation (4.5).Residuals, observed random error terms, defined observed responses, (y_{ijResiduals, observed random error terms, defined observed responses, (y_{ijThe effect Brand C might positive (Brand C higher average breaking strength Brand D), effect Brand D must negative exactly size effect Brand C. two effects sum zero. called restriction model terms. entire set restrictions model Equation (4.5) provided .\\[\n\\sum_{=1}^2 \\alpha_i = \\alpha_1 + \\alpha_2 = 0,\n\\quad\n\\sum_{j=1}^3 \\beta_j = \\beta_1 + \\beta_2 + \\beta_3 = 0,\n\\]\n\\[\n\\sum_{=1}^2 (\\alpha\\beta)_{ij} = (\\alpha\\beta)_{1j} + (\\alpha\\beta)_{2j} = 0\n\\quad\\text{} j,\n\\]\n\\[\n\\sum_{j=1}^3 (\\alpha\\beta)_{ij} = (\\alpha\\beta)_{i1} + (\\alpha\\beta)_{i2} + (\\alpha\\beta)_{i3} = 0\n\\quad\\text{} \n\\tag{4.8}\n\\]specifically, restrictions state interaction effects involving Brand C must sum zero\n(.e., \\((\\alpha\\beta)_{11} + (\\alpha\\beta)_{12} + (\\alpha\\beta)_{13} = 0\\)).\nway, interactions corresponding 15 drops \nwater groups must also sum zero. six restrictions corresponding interactions can checked \nQuestion 27. residual values sum zero within group interest. always true whenever\ncalculating effects. surprising, since effects measure deviation particular group mean \noverall mean.","code":""},{"path":"paper-towels-the-relationship-between-effects-and-anova.html","id":"paper-towels-the-relationship-between-effects-and-anova","chapter":"9 4.7 Paper Towels: The Relationship Between Effects and ANOVA","heading":"9 4.7 Paper Towels: The Relationship Between Effects and ANOVA","text":"three hypothesis tests paper towel study tested separate line ANOVA table. F-statistics ANOVA already calculated earlier chapter comparing -group within-group variability. section show relationship calculating effects ANOVA table. null hypothesis, statement “means equal levels factor” equivalent statement “factor effects zero.”sum squares (SS) main factor multi-factor ANOVA identical one-factor SS described Chapter 2. sum squares (numerator mean square calculation) sum squared effects corresponding factor. Brand effect, written mathematically \\[\nSS_{\\text{Brand}}\n= \\sum (\\text{Brand effect 156 observations})^2\n= \\sum_{=1}^{2} 78\\,(\\bar y_{\\cdot\\cdot} - \\bar y_{\\cdot\\cdot\\cdot})^2.\n\\tag{4.9}\n\\]equation can generalized. Instead 78 elements Brand group, can indicate \\(n_i\\) elements. Instead 2 levels Brand, can indicate \\(\\) levels. first factor, Brand, can labeled factor \\(\\); second factor, Water, can labeled factor \\(B\\); etc. \\[\nSS_{\\text{Brand}} = SS_A = \\sum_{=1}^{} n_i \\,(\\bar y_{\\cdot\\cdot} - \\bar y_{\\cdot\\cdot\\cdot})^2\n\\quad\\text{} = 2 \\text{ (number Brand levels)}.\n\\]Similarly, \\(SS_{\\text{Water}} = SS_B\\) sum squares Water effect observation:\\[\nSS_{\\text{Water}}\n= \\sum (\\text{Water effect 156 observations})^2\n= \\sum_{j=1}^{3} 52\\,(\\bar y_{\\cdot j \\cdot} - \\bar y_{\\cdot\\cdot\\cdot})^2\n= \\sum_{j=1}^{J} n_j \\,(\\bar y_{\\cdot j \\cdot} - \\bar y_{\\cdot\\cdot\\cdot})^2\n\\]\n\\[\n\\quad\\text{} J = 3 \\text{ (number Water levels)}.\n\\tag{4.10}\n\\]sum squares interaction term, \\(SS_{AB}\\), \\[\\begin{align}\nSS_{AB} \\notag\n  &= \\sum (\\text{interaction effect 156 observations})^2 \\\\[6pt] \\notag\n  &= \\sum_{=1}^{}\\sum_{j=1}^{J} n_{ij}\\,(\\text{ijth level effect})^2 \\\\[6pt] \\notag\n  &= \\sum_{=1}^{2}\\sum_{j=1}^{3} 26\\,(\\bar y_{ij\\cdot} - \\bar y_{\\cdot\\cdot} - \\bar y_{\\cdot j \\cdot} + \\bar y_{\\cdot\\cdot\\cdot})^2. \\\\[6pt] \\tag{4.11}\n\\end{align}\\]error sum squares (\\(SS_{\\text{Error}}\\)) measures spread observed residuals. residual defined observed value minus estimated value: \\(\\hat\\varepsilon_{ijk} = y_{ijk} - \\hat y_{ij\\cdot}\\). \\[\\begin{align}\nSS_{\\text{Error}} \\notag\n  &= \\sum (\\text{residual effect})^2 \\\\[6pt] \\notag\n  &= \\sum_{=1}^{}\\sum_{j=1}^{J}\\sum_{k=1}^{n_{ij}} (y_{ijk} - \\bar y_{ij\\cdot})^2 \\\\[6pt] \\notag\n  &= \\sum_{=1}^{}\\sum_{j=1}^{J} [(n_{ij}-1)\\,s_{ij}^2] \\\\[6pt] \\notag\n  &= 25\\,s_{11}^2 + 25\\,s_{12}^2 + 25\\,s_{13}^2 + 25\\,s_{21}^2 + 25\\,s_{22}^2 + 25\\,s_{23}^2. \\\\[6pt] \\tag{4.12}\n\\end{align}\\]total sum squares (\\(SS_{\\text{Total}}\\)) measures overall spread responses full data set:\\[\\begin{align}\nSS_{\\text{Total}} \\notag\n  &= \\sum (\\text{distance observation grand mean})^2 \\\\[6pt] \\notag\n  &= \\sum_{=1}^{}\\sum_{j=1}^{J}\\sum_{k=1}^{n_{ij}} (y_{ijk} - \\bar y_{\\cdot\\cdot\\cdot})^2 \\\\[6pt] \\notag\n  &= (N-1)\\,s^2. \\\\[6pt] \\tag{4.13}\n\\end{align}\\]MATHEMATICAL NOTE\nvariance within factor-level group calculated \\[s_{ij}^2 = \\frac{\\sum_{k=1}^{n_{ij}} (y_{ijk} - \\bar y_{ij\\cdot})^2}{n_{ij}-1},\\]\noverall sample variance response variable \\[s^2 = \\frac{\\sum_{=1}^\\sum_{j=1}^J\\sum_{k=1}^{n_{ij}} (y_{ijk} - \\bar y_{\\cdot\\cdot\\cdot})^2}{N-1},\\]\n\\(N = 156\\) total sample size.","code":""},{"path":"paper-towels-the-relationship-between-effects-and-anova.html","id":"degrees-of-freedom","chapter":"9 4.7 Paper Towels: The Relationship Between Effects and ANOVA","heading":"9.1 Degrees of Freedom","text":"Degrees freedom (df) determined many “free” pieces information available calculating effects. example, Equation (4.8) shows main effects must sum zero. Thus, knowing effects two levels Water forces known effect last level. example, effect 0 drops water increases expected mean strength 1264.4. Similarly, effect using 5 drops water –239.9. effects must sum zero: \\(\\hat\\beta_1 + \\hat\\beta_2 + \\hat\\beta_3 = 1264.4 - 239.9 - 1024.5 = 0.\\)KEY CONCEPT\nmain factor \\(J\\) levels, one effect fixed know \\(J-1\\) effects. Thus, \\(J-1\\) degrees freedom (free pieces information).","code":""},{"path":"paper-towels-the-relationship-between-effects-and-anova.html","id":"extended-activity-calculating-degrees-of-freedom-for-interaction-terms","chapter":"9 4.7 Paper Towels: The Relationship Between Effects and ANOVA","heading":"9.1.1 Extended Activity: Calculating Degrees of Freedom for Interaction Terms","text":"Data set: PaperTowelsTable 4.6 table two rows three columns, similar interaction effect term two-way factorial diagram Figure 4.4. However, question assume two effects known: \\((\\alpha\\beta)_{11}=2\\) \\((\\alpha\\beta)_{12}=-5\\).Equation (4.8) states AB effects within Brand C add zero \\([\\;(\\alpha\\beta)_{11} + (\\alpha\\beta)_{12} + (\\alpha\\beta)_{13} = 0\\;]\\). Use rule calculate \\((\\alpha\\beta)_{13}\\).Equation (4.8) states AB effects within Brand C add zero \\([\\;(\\alpha\\beta)_{11} + (\\alpha\\beta)_{12} + (\\alpha\\beta)_{13} = 0\\;]\\). Use rule calculate \\((\\alpha\\beta)_{13}\\).Equation (4.8) also states AB effects within 0 water amount add zero (5 15 drops water). Use rule calculate \\((\\alpha\\beta)_{21}\\), \\((\\alpha\\beta)_{22}\\), \\((\\alpha\\beta)_{23}\\).Equation (4.8) also states AB effects within 0 water amount add zero (5 15 drops water). Use rule calculate \\((\\alpha\\beta)_{21}\\), \\((\\alpha\\beta)_{22}\\), \\((\\alpha\\beta)_{23}\\).Consider different interaction table two rows three columns. Explain possible effects \\((\\alpha\\beta)_{11}=4\\), \\((\\alpha\\beta)_{13}=-4\\), \\((\\alpha\\beta)_{22}=6\\) still follow restrictions Equation (4.8).Consider different interaction table two rows three columns. Explain possible effects \\((\\alpha\\beta)_{11}=4\\), \\((\\alpha\\beta)_{13}=-4\\), \\((\\alpha\\beta)_{22}=6\\) still follow restrictions Equation (4.8).degrees freedom corresponding interaction term (balanced completely randomized design) two levels factor three levels factor B? words, restrictions Equation (4.8), number free pieces information (number cells Table 4.6 fixed)?degrees freedom corresponding interaction term (balanced completely randomized design) two levels factor three levels factor B? words, restrictions Equation (4.8), number free pieces information (number cells Table 4.6 fixed)?Table 4.7 another table interaction effects, five rows three columns (five levels factor three levels factor B). , assume effects known.Use Equation (4.8) calculate effect corresponding remaining cells.Table 4.7, eight cells filled. seven cells filled, possible calculate\neffects corresponding remaining cells?degrees freedom corresponding interaction term (balanced completely\nrandomized design) five levels factor three levels factor B? words, \nminimum number cells must filled order allow us use Equation (4.8) esti-\nmate effects?Use previous two questions determine degrees freedom interaction term balanced completely randomized design three levels factor four levels factor B.AB interaction term, * J effects calculated. popcorn study, \\(\\times J = 2 \\times 3 = 6\\). effect represents one piece information. addition:\n\\(\\bullet\\) AB effects within Brand C add zero [\\(\\hat{(\\alpha\\beta)}_{11} + \\hat{(\\alpha\\beta)}_{12} + \\hat{(\\alpha\\beta)}_{13}= 0\\)]. Within Brand C,\ntwo effects known, third fixed (holds Brand D). Thus, restrictions\neliminate \\(= 2\\) free pieces information (free cells interaction effects table).\\(\\bullet\\) Similarly, \\(AB\\) effects within 0 water amount add zero [\\(\\hat{(\\alpha\\beta)}_{11} + \\hat{(\\alpha\\beta)}_{21} = 0\\)]. restriction holds levels factor \\(B\\) (5 15 drops water). Thus, additional \\(J = 3\\) pieces information longer free. However, one piece information already fixed requirement sum brand effects zero. Thus, \\(J - 1 = 3 - 1 = 2\\) free pieces information taken water amounts.degrees freedom interaction effect \\[\\begin{align}\ndf_{AB} \\notag\n  &= \\text{number interaction effects} - [df_A + df_B + 1] \\\\[6pt] \\notag\n  &= IJ - [(- 1) + (J - 1) + 1] \\\\[6pt] \\notag\n  &= IJ - - J + 1 \\\\[6pt] \\notag\n  &= (- 1)(J - 1) \\\\[6pt] \\tag{4.14}\n\\end{align}\\]MATHEMATICAL NOTE\nCalculating interaction degrees freedom \\((- 1)(J - 1)\\) Equation (4.14) quite easy. However, reason Equation (4.14) also shows interaction \\(df = IJ - [(- 1) + (J - 1) + 1]\\) follows calculation interaction effect shown Equation (4.7): \\(\\bar y_{ij\\cdot} - \\bigl[(\\bar y_{\\cdot\\cdot} - \\bar y_{\\cdot\\cdot\\cdot}) + (\\bar y_{\\cdot j \\cdot} - \\bar y_{\\cdot\\cdot\\cdot})+ \\bar y_{\\cdot\\cdot\\cdot}\\bigr].\\)\nkey point recognize knowing effects calculated drives formulas sum squares degrees freedom. also true complex designs beyond scope chapter.KEY CONCEPT\nterm model, degrees freedom represent number cells factor diagr","code":""},{"path":"paper-towels-the-relationship-between-effects-and-anova.html","id":"extended-activity-analyzing-the-paper-towel-data","chapter":"9 4.7 Paper Towels: The Relationship Between Effects and ANOVA","heading":"9.1.2 Extended Activity: Analyzing the Paper Towel Data","text":"Data set: PaperTowelsChecking Assumptions statistical model Equation (4.5), following assumptions need validated random error terms, \\(\\varepsilon_{ijk}\\), formal hypothesis test can developed:\n\\(\\bullet\\) error terms independent identically distributed.\\(\\bullet\\) error terms follow normal probability distribution, denoted \\(\\varepsilon \\sim N(0,\\sigma^2)\\).Note second assumption includes equal variance assumption random errors different factor-level groups:\\(\\sigma^2_{11} = \\sigma^2_{12} = \\sigma^2_{13} = \\sigma^2_{21} = \\sigma^2_{22} = \\sigma^2_{23}.\\)independence assumption implies relationship one observation next. identically distributed assumption means observation sampled within brand/water combination population mean variance. 26 paper towels sampled one roll assess Brand C/5 drops factor combination, concerned violating independence /identically distributed assumption? ?independence assumption implies relationship one observation next. identically distributed assumption means observation sampled within brand/water combination population mean variance. 26 paper towels sampled one roll assess Brand C/5 drops factor combination, concerned violating independence /identically distributed assumption? ?Calculate sample means standard deviations six factor-level combinations. Clearly, groups much larger variation others. addition, variation within group increases average breaking strength increases. address issue, transformation data can often used “stabilize” variances equal variance assumption reasonable new scale. (Chapter 2 describes transformations detail.)Calculate sample means standard deviations six factor-level combinations. Clearly, groups much larger variation others. addition, variation within group increases average breaking strength increases. address issue, transformation data can often used “stabilize” variances equal variance assumption reasonable new scale. (Chapter 2 describes transformations detail.)Transform response variable using natural log (Strength) \\(\\sqrt{\\text{Strength}}\\). transformations improve equal variance assumption?Transform response variable using natural log (Strength) \\(\\sqrt{\\text{Strength}}\\). transformations improve equal variance assumption?Visualizing Data Draw individual value plots side--side boxplots square-root-transformed responses six factor-level groups.\n  . extreme skewness outliers cause us question normality assumption?\n  b. Without statistical calculations, expect reject three null hypotheses paper towel study? Justify answer visually comparing variation (.e., spread) strength groups variation within groups.Visualizing Data Draw individual value plots side--side boxplots square-root-transformed responses six factor-level groups.\n  . extreme skewness outliers cause us question normality assumption?\n  b. Without statistical calculations, expect reject three null hypotheses paper towel study? Justify answer visually comparing variation (.e., spread) strength groups variation within groups.Analyzing Data Use computer software conduct analysis square-root transformed PaperTowels data test differences brands, water levels, interactions. Use ANOVA well appropriate graphs state conclusions paper towel study.Analyzing Data Use computer software conduct analysis square-root transformed PaperTowels data test differences brands, water levels, interactions. Use ANOVA well appropriate graphs state conclusions paper towel study.","code":""},{"path":"contrasts-and-multiple-comparisons.html","id":"contrasts-and-multiple-comparisons","chapter":"10 4.8 Contrasts and Multiple Comparisons","heading":"10 4.8 Contrasts and Multiple Comparisons","text":"Chapter 2 describes study researchers tested whether color distracter influenced completion time online computer game. addition color distracter, also interested whether subjects play game quickly right left hand. Chapter 2 restricted one-factor ANOVAs. Thus, chapter data sorted four groups: StandardRight, ColorRight, StandardLeft, ColorLeft. Instead testing evidence general hypothesis test (\\(H_0\\): \\(\\mu_{SR} = \\mu_{CR} = \\mu_{SL} = \\mu_{CL}\\)), two-way ANOVA allows us test three specific hypotheses interest.","code":""},{"path":"contrasts-and-multiple-comparisons.html","id":"extended-activity-comparing-one-way-and-two-way-anova","chapter":"10 4.8 Contrasts and Multiple Comparisons","heading":"10.0.1 Extended Activity: Comparing One-Way and Two-Way ANOVA","text":"Data set: Games2The data set Games2 shows column Type2 four types games based distracter hand used. Conduct ANOVA using Type2 (just one explanatory variable four levels) test differences completion time. \\(p\\)-value corresponding null hypothesis \\(H_0\\): \\(\\mu_{SR} = \\mu_{CR} = \\mu_{SL} = \\mu_{CL}\\) versus alternative \\(H_a\\): least one mean different another?data set Games2 shows column Type2 four types games based distracter hand used. Conduct ANOVA using Type2 (just one explanatory variable four levels) test differences completion time. \\(p\\)-value corresponding null hypothesis \\(H_0\\): \\(\\mu_{SR} = \\mu_{CR} = \\mu_{SL} = \\mu_{CL}\\) versus alternative \\(H_a\\): least one mean different another?Conduct two-way ANOVA using Type, Hand, Type*Hand interaction test differences completion time. List three null alternative hypotheses provide \\(p\\)-value test.Conduct two-way ANOVA using Type, Hand, Type*Hand interaction test differences completion time. List three null alternative hypotheses provide \\(p\\)-value test.Since response variable used Question 37 Question 38, surprising total sum squares identical questions. Compare sums squares ANOVAs Questions 37 38. \\(SS_{\\text{Type2}}\\) related \\(SS_{\\text{Type}}\\), \\(SS_{\\text{Hand}}\\), \\(SS_{\\text{TypeHand}}\\)?Since response variable used Question 37 Question 38, surprising total sum squares identical questions. Compare sums squares ANOVAs Questions 37 38. \\(SS_{\\text{Type2}}\\) related \\(SS_{\\text{Type}}\\), \\(SS_{\\text{Hand}}\\), \\(SS_{\\text{TypeHand}}\\)?Since Question 37 leads us reject \\(H_0\\): \\(\\mu_{SR} = \\mu_{CR} = \\mu_{SL} = \\mu_{CL}\\), seems reasonable conduct multiple comparisons test (conduct multiple tests identify differences group mean every group mean).NOTE\nsix possible comparisons four group means. Chapter 1 discussed familywise type error comparisonwise type error. least-significant differences method technique using comparisonwise type error: \\(p\\)-value less \\(\\alpha\\), reject \\(H_0\\) favor \\(H_a\\). Assuming particular null hypothesis true, least-significant differences method \\(\\alpha\\%\\) chance (incorrectly) rejecting hypothesis. multiple tests conducted data set, least-significant differences method leads type errors: rejecting null hypotheses rejected.Bonferroni’s method example technique maintains familywise type error: \\(p\\)-value less \\(\\alpha/K\\) (\\(K\\) number pairs), reject \\(H_0\\) favor \\(H_a\\). familywise type error = \\(\\alpha\\), assuming really difference \\(K\\) pairs, \\(\\alpha\\%\\) chance test reject \\(H_0\\). leads type II errors: failing reject null hypotheses rejected. Chapter 1 describes need multiple comparison procedures describes least-significant difference Bonferroni method detail.Question 38 can thought testing orthogonal contrasts. orthogonal contrast linear combination treatment means coefficients add zero. example, collected data, researchers game study interested several specific comparisons:Comparing standard color games: \\(H_{01}:\\) \\((1)\\bar \\mu_{SR} + (-1)\\bar \\mu_{CR} + (1)\\bar \\mu_{SL} + (-1)\\bar \\mu_{CL} = 0\\). mathematically equivalent \\(H_{01}:\\) \\[(1/2)\\bar \\mu_{SR} + (-1)\\bar \\mu_{SL} = (1/2)\\bar \\mu_{CR} + (1/2)\\bar\\mu_{CL} \\] \\(H_{01}: \\mu_{SR} = \\mu_{CR}\\).Comparing right left hand: \\(H_{02}:\\) \\((1)\\bar y_{SR} + (1)\\bar y_{CR} + (-1)\\bar y_{SL} + (-1)\\bar y_{CL} = 0\\). mathematically equivalent \\(H_{02}: \\mu_{R} = \\mu_{L}\\).Testing interaction: \\(H_{03}:\\) \\((1)\\bar y_{SR} + (-1)\\bar y_{CR} + (-1)\\bar y_{SL} + (1)\\bar y_{CL} = 0\\).linear combinations population means can estimated contrast:Contrast 1 (C1) = \\((1)\\bar y_{SR} + (-1)\\bar y_{CR} + (1)\\bar y_{SL} + (-1)\\bar y_{CL}\\)\nContrast 2 (C2) = \\((1)\\bar y_{SR} + (1)\\bar y_{CR} + (-1)\\bar y_{SL} + (-1)\\bar y_{CL}\\)\nContrast 3 (C3) = \\((1)\\bar y_{SR} + (-1)\\bar y_{CR} + (-1)\\bar y_{SL} + (1)\\bar y_{CL}\\)coefficients linear combinations sum zero:\nCoefficients contrast 1 = \\(C_{11} + C_{21} + C_{31} + C_{41} = (1) + (-1) + (1) + (-1) = 0\\)\nCoefficients contrast 2 = \\(C_{12} + C_{22} + C_{32} + C_{42} = (1) + (1) + (-1) + (-1) = 0\\)\nCoefficients contrast 3 = \\(C_{13} + C_{23} + C_{33} + C_{43} = (1) + (-1) + (-1) + (1) = 0\\)KEY CONCEPT\nnull hypothesis test comparing \\(G\\) group means \\(H_0\\!: \\mu_1 = \\mu_2 = \\dots = \\mu_G\\) versus alternative $H_a!: $ least one group mean different another, contrast estimate linear combination group means: Contrast 1 (C1) = \\((C_{11})\\bar y_1 + (C_{21})\\bar y_2 + \\dots + (C_{G1})\\bar y_G\\), coefficients sum zero: \\(C_{11} + C_{21} + \\dots + C_{G1} = 0\\).MATHEMATICAL NOTE\nset \\(G\\) group means (four group means case; \\(H_0: \\mu_{SR} = \\mu_{CR} = \\mu_{SL} = \\mu_{CL}\\) compared -group sum squares) can used create \\(G - 1\\) mutually orthogonal contrasts (\\(H_{01}, H_{02}, H_{03}\\)). two contrasts said orthogonal dot product (sum cross products) coefficient vectors zero. Contrasts must orthogonal ensure independent. independence allows us partition variation ANOVA, sum squares corresponding \\(G - 1\\) contrasts sum -group sum squares.Often contrasts incorporated ANOVA analysis. F-statistic contrast simply mean square particular -groups measure (example, \\(MS_{C1}\\) mean square C1) divided pooled within-group variances (MSE). can write mean square contrast \\[\\begin{align}\nMS_{C1}\n  &= \\frac{(C1)^2}{\\sum_{g=1}^G \\frac{C_g^2}{n_g}}\n  \\tag{4.15}\n\\end{align}\\]\\(C_{g1}\\) contrast coefficient \\(n_g\\) sample size group. game study,\\[\nC1 = (1)\\bar y_{SR} + (-1)\\bar y_{CR} + (1)\\bar y_{SL} + (-1)\\bar y_{CL}\n   = (1)\\,34 + (-1)\\,36 + (1)\\,37.1 + (-1)\\,40.2\n   = -5.1\n\\]Thus, mean square contrast 1 (\\(MS_{C1}\\)) based Question 37 identical mean square game type (\\(MS_{\\text{Type}}\\)) Question 38:\\[\\begin{align}\nMS_{C1}\n  &= \\frac{(-5.1)^2}{\\frac{1^2}{10} + \\frac{(-1)^2}{10} + \\frac{1^2}{10} + \\frac{(-1)^2}{10}}\n  = \\frac{(-5.1)^2}{0.4}\n  = 65.025\n  = MS_{\\text{Type}} \\notag\n\\end{align}\\]","code":""},{"path":"contrasts-and-multiple-comparisons.html","id":"extended-activity-calculating-contrasts","chapter":"10 4.8 Contrasts and Multiple Comparisons","heading":"10.0.2 Extended Activity: Calculating Contrasts","text":"Data set: Games2Use Equation (4.15) calculate \\(MS_{C2}\\) \\(MS_{C3}\\). Show work.Compare \\(MS_{C2}\\) \\(MS_{C3}\\), mean squares found Question 38.KEY CONCEPT\nOrthogonal contrasts allow multiple comparisons linear combinations group means. key advantage contrasts inflated type type II errors. However, contrasts always determined data collected. Looking data order develop contrasts bias results.MATHEMATICAL NOTE\ncommon techniques multiple comparisons. Scheffé’s method produces simultaneous confidence intervals contrasts, including contrasts suggested data (often called post hoc data exploration). Instead traditional formula confidence intervals, Scheffé suggested using wider confidence interval (.e., one less likely reject null hypothesis) account multiple testing. method often fails reject null hypotheses even differences groups, can useful pairwise comparison tests appropriate. Remember introductory statistics course: zero confidence interval, fail reject corresponding hypothesis test; zero confidence interval, reject corresponding hypothesis test. Tukey’s honest significant difference (HSD) uses studentized range distribution instead \\(F\\)-distribution create confidence intervals differences meaningful pairs. large number pairwise comparisons, Tukey’s method typically preferred Bonferroni’s method.","code":""},{"path":"footnotes-and-citations.html","id":"footnotes-and-citations","chapter":"11 Footnotes and citations","heading":"11 Footnotes and citations","text":"","code":""},{"path":"footnotes-and-citations.html","id":"footnotes","chapter":"11 Footnotes and citations","heading":"11.1 Footnotes","text":"Footnotes put inside square brackets caret ^[]. Like one 15.","code":""},{"path":"footnotes-and-citations.html","id":"citations","chapter":"11 Footnotes and citations","heading":"11.2 Citations","text":"Reference items bibliography file(s) using @key.example, using bookdown package16 (check last code chunk index.Rmd see citation key added) sample book, built top R Markdown knitr17 (citation added manually external file book.bib).\nNote .bib files need listed index.Rmd YAML bibliography key.bs4_book theme makes footnotes appear inline click . example book, added csl: chicago-fullnote-bibliography.csl index.Rmd YAML, include .csl file. download new style, recommend: https://www.zotero.org/styles/RStudio Visual Markdown Editor can also make easier insert citations: https://rstudio.github.io/visual-markdown-editing/#/citations","code":""},{"path":"categorical-data-analysis-is-a-tumor-malignant-or-benign.html","id":"categorical-data-analysis-is-a-tumor-malignant-or-benign","chapter":"12 Categorical Data Analysis: Is a Tumor Malignant or Benign?","heading":"12 Categorical Data Analysis: Is a Tumor Malignant or Benign?","text":"commonly believed anyone tabulates numbers statistician. like believing anyone owns scalpel surgeon.\n—Robert Hooke1This chapter introduces inference techniques data explanatory \nresponse variables categorical. term categorical data analysis often refers \nanalysis data response variable categorical. However, chapter\nrestrict focus cases explanatory response variables \ncategorical natural ordering categories.hypothesis tests discussed previous chapters use normal distribution \nmodel mean response. chapter, proportions, odds ratios, relative risk \nused summarize categorical data. start looking cancer cell data determine\nrelationship shape cell nuclei proportion malignant\ncells. chapter, discuss following:Conduct simulation study, chi-square test, Fisher’s exact testCalculate properly interpret summary statistics relative risk odds ratioDetermine test use based various sampling schemes questions interest","code":""},{"path":"categorical-data-analysis-is-a-tumor-malignant-or-benign.html","id":"investigation-is-cell-shape-associated-with-malignancy","chapter":"12 Categorical Data Analysis: Is a Tumor Malignant or Benign?","heading":"12.1 Investigation: Is Cell Shape Associated with Malignancy?","text":"Cancer disease occurs abnormal cells grow body. DNA (substance every\ncell) damaged, normal cells often repair damaged DNA. Cancer cells cells \nDNA repaired. DNA can damaged many things, including viruses, tobacco smoke, alcohol,\nmuch sunlight. Cells damaged DNA can also inherited. Cancer cells can continue grow\ndivide usually form tumors (lump mass) somewhere body. Cancer cells can also outlive\nimportant note tumors cancerous. lump detected, part can removed\nsurgically biopsy conducted determine mass benign malignant. Benign tumors scar\ntissue abnormal growths spread typically harmless. Malignant (invasive) cancer\ncells cells can travel, typically bloodstream lymph nodes, begin replace normal\ncells parts body. tumor malignant, essential remove destroy cancerous cells\norder keep spreading. tumor benign, surgery typically needed harmless\ntumor can remain.biopsy requires surgery remove section () tumor leave scar. Fine needle\naspiration (FNA) technique small sample tumor taken using needle visually\ninspected microscope. Since many tumors benign, often preferable FNA, \nless invasive less traumatic biopsy.Breast cancer second leading cause cancer death among women United States. \nAmerican Cancer Society estimated , United States 2010, 39,840 women 390 men died \nbreast cancer 207,090 women diagnosed breast cancer.2 type cancer often detected\nfinding lump (mass) breast.Wolberg Mangasarian developed technique accurately diagnose breast masses using visual\ncharacteristics cells within tumor.3 system used University Wisconsin hospitals assist\ndoctors diagnosis breast cancer.4 FNA sample placed slide, characteristics cellular\nnuclei within tumor examined microscope. Several measurements, size, shape, \ntexture, collected nuclei visible slide, algorithm used determine \nlikelihood mass benign malignant.chapter, focus just characteristics relatively small data set col-\nlected University Wisconsin hospitals Madison. start determining shape cell\nnucleus can help us determine whether tumor malignant benign.\nTypically, healthy cell nuclei round ellipsoid shapes.Figure 6.1 shows sample malignant cells appear grown perimeters cell nuclei somewhat concave points.[[[Figure 6.1]]]\nFigure 6.1 image malignant cells nuclei outlined curve-fitting program. Reprinted permission. Mangasarian, Street & Wolberg, “Breast Cancer Diagnosis Prognosis via Linear Programming,” INFORMS Journal Operations Research, 43.4, 1995. © 1995, Institute Operations Research Management Sciences (INFORMS).","code":""},{"path":"categorical-data-analysis-is-a-tumor-malignant-or-benign.html","id":"summarizing-categorical-data","chapter":"12 Categorical Data Analysis: Is a Tumor Malignant or Benign?","heading":"12.2 Summarizing Categorical Data","text":"Table 6.1 shows data 37 FNA slide samples. Slides smooth ellipsoid-shaped nuclei classified\nround, slides poorly shaped cell nuclei classified concave. biopsy also conducted\nsamples determine malignant benign.Table 6.1, variables categorical. categorical variable one measurement\nconsists categories, college major, political affiliation, personality type, gender, pass/fail grade.\nvariable categorical, subject unit must fit one one category. binary variable\nspecial type categorical variable just two possible categories.Summarizing quantitative data age, weight, income often includes calculating mean value.\nHowever, Table 6.1 used calculate mean Shape mean Type value. Since categorical\nvariables ordinal interval meaning, appropriate focus mean response; rather, \nfocus proportion (percent) responses fall category.table counts number observations group, Table 6.1, called contingency table (cross-tabulation table). contingency table two variables called two-way contingency table. Table 6.1 also called 2 x 2 table, since two row groups two column groups. Table\n6.1, Shape cell called row variable, since horizontal row represents one shape group.\nSimilarly, Type cell called column variable.\nCategorical data natural ordering, level agreement (strongly disagree, disagree,\nindifferent, agree, strongly agree) evaluation product (poor, fair, good, excellent), called\nordinal data. Categorical data natural ordering, gender major, called\nnominal data. Techniques nominal data give identical results ordering categories,\nwhereas results based techniques ordinal data depend ordering data. chapter\nrestricted examples nominal data analysis techniques.\n\n","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"activity-descriptive-statistics-and-graphs","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"13 Activity: Descriptive Statistics and Graphs","text":"Identify observational units, explanatory variable, response variable cancer cell\ndata Table 6.1.Calculate proportion round cell samples malignant proportion concave cell\nsamples malignant.Create segmented bar graph using Table 6.1. Typically, explanatory variable along \nhorizontal axis. Assuming random sample larger population, graph show evi-\ndence nucleus shape related likelihood cell malignant? Explain.Bar graphs often useful comparing two categorical variables. Figure 6.1 shows segmented bar graph (also called stacked bar graph) cancer cell data. graph shows conditional percentages\nnucleus shape. 80% concave nuclei malignant, whereas 45% round\nnuclei malignant.\n(#fig:fig6.2)Figure 6.2 Segmented bar graph nucleus shape malignancy.\n","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"a-simulation-study-how-likely-is-it-that-the-observed-sample-would-occur-by-chance","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"13.1 A Simulation Study: How Likely Is It That the Observed Sample Would Occur by Chance?","text":"Figure 6.2 shows sample 37 slides indicates relationship shape cell nuclei malignancy. However, statistical inference needed draw conclusions entire population samples selected. section, conduct hypothesis test determine sample data provide evidence proportion malignant cells greater concave nuclei round nuclei.hypothesis test example one-sided medical reason suspect concave nuclei likely malignant. example, null alternative hypotheses can written \\[\\begin{align}\nH_0: p_C = p_R \\quad\\text{vs.}\\quad H_a: p_C > p_R\n\\tag{6.1}\n\\end{align}\\]\\(p_C\\) true proportion concave nuclei malignant \\(p_R\\) true proportion round nuclei malignant.null hypothesis, \\(H_0\\), true, two populations (concave round cells) proportion malignant cells observed difference round concave cells sample due simply random sampling process. words, samples just randomly happened malignant cells concave nucleus population round nucleus group.\\(p\\)-value test probability obtaining difference sample proportions (\\(\\hat p_C - \\hat p_R\\)) large larger one observed sample null hypothesis true. \\(p\\)-value small, unlikely null hypothesis true conclude alternative hypothesis (\\(p_C > p_R\\)) true.One way estimate \\(p\\)-value simulate taking samples many times following three conditions:• Assume malignancy unrelated cell nucleus shape (.e., assume cell nucleus shapes proportion malignant cells).\n• total 13 benign 24 malignant cells observed.\n• total 16 round cell nuclei 21 concave cell nuclei observed.","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"activity-conducting-a-simulation-study-with-cards","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"Activity: Conducting a Simulation Study with Cards","text":"Use 37 index cards represent sample 37 cancer cells. 24 cards write M “malig-\nnant,” 13 cards write B “benign.” Shuffle cards randomly select 21 cards. \n21 cards can represent concave nucleus group. many 21 concave cards also malignant?Repeat simulation process Question 4 nine times. seem likely 17 \nmalignant cells occur concave group chance alone?simulations can done hand cards, process time consuming, large number\nsimulations needed get true feel likelihood outcome (many statisticians suggest\n10,000 simulations). Instead repeating process 10,000 times hand, use computer\nprogram conduct simulation.","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"activity-computer-simulation","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"Activity: Computer Simulation","text":"Use technology instructions provided CD conduct one simulation. simulation, \none column listing 24 malignant 13 benign cells. Randomly select 21 rows represent\nconcave nucleus shapes. Count number observations fall concave malignant group.Repeat computer simulation process Question 6 nine times, time recording num-\nber malignant cells sampled concave nucleus group. simulation compare \nindex card simulation? expect get exactly number samples 17 \nmalignant cells? ?Use software instructions repeat computer-simulated randomization process total 10,000\ntimes. Create histogram 10,000 simulated counts concave malignant group. Estimate \np-value dividing number counts greater equal 17 10,000.Table 6.2 shows 10,000 simulated trials, based Question 8. simulation 220 observations\ngreater equal 17, providing p-value \\(P(X \\geq 17)= 0.022\\), X number concave\nmalignant cells. Thus, can conclude null hypothesis true (proportion malignant cells\ncells round concave nuclei), likelihood finding 17 malignant cells\n21 cells concave nuclei approximately 220 10,000. small p-value shows\ndifference sample proportions large unlikely occurred chance. Thus,\nreject null hypothesis conclude cells concave nuclei likely malignant \ncells round nuclei.Using simulations approximate p-values many advantages. Often simple programs mac-\nros can written quickly simulate thousands samples. Computer programs can modified \nfit variety situations, parametric tests theoretical assumptions can somewhat rigid\nrequire large sample sizes. Simulations provide approximate p-values. However, increasing\nnumber simulations improves precision p-value; 10,000 simulations usually provides\nprecise p-values.\n\n","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"fishers-exact-test","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"13.2 Fisher’s Exact Test","text":"use simulations determine \\(p\\)-values quickly gaining popularity, sometimes exact \\(p\\)-value can calculated based appropriate probability model. Fisher’s exact test uses hypergeometric distribution calculate exact probabilities.Just simulation study, interested testing \\(H_0: p_C = p_R\\) vs. \\(H_a: p_C > p_R\\). addition, use three assumptions find \\(p\\)-value, probability 17 malignant cells occur concave nucleus group \\(H_0\\) true. Table 6.3 provides data Table 6.1, notation included extend test 2 × 2 table.2 × 2 table, total \\(N\\) observations can classified either success failure. \\(M\\) represents number success, thus \\(N - M\\) number failures. interesting finding probability observing \\(x\\) successes random selection \\(n\\) observations. cancer cell data inTable 6.3, \\(x\\) represents event interest (observing 17 concave malignant cells), \\(N = 37\\) represents total number observations, \\(M = 24\\) total number malignant cells (number successes), \\(n\\) total number observations concave group.\nsection, simply discuss conduct Fisher’s exact test statistical software. extended activities shows Fisher’s exact test uses hypergeometric distribution calculate exact \\(p\\)-values. 2 × 2 contingency table \\(N\\) total observations \\(M\\) total successes, probability observing \\(x\\) successes, \\(P(X = x)\\), sample size \\(n\\) \n\\[\\begin{align}\n\\frac{\\text{number ways select $x$ successes $n - x$ failures}}{\\text{number ways select $n$ subjects}}\n&= \\frac{\\binom{M}{x}\\,\\binom{N - M}{n - x}}{\\binom{N}{n}} \\notag\n\\end{align}\\]\\(\\binom{M}{x} = M \\text{“choose”} x = \\frac{M!}{x!(M - x)!}\\). Similar calculations hold \\(\\binom{N - M}{n - x}\\) \\(\\binom{N}{n}\\).”","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"activity-calculating-fishers-exact-test","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"Activity: Calculating Fisher’s Exact Test","text":"cancer study, assume N= 37 observations M = 24 successes. n = 21 observations\nselected, use technology instructions provided CD calculate exact probabilities\nP(X = 17), P(X = 18), P(X = 19), P(X = 20), P(X= 21).Assuming N= 37, M= 24 successes, n = 21, create histogram probabilities x. Com-\npare probabilities Question 9 probabilities Question 8. Since Question 8 simula-\ntion hypergeometric distribution, histograms look similar.exact p-value P(X \\(\\geq\\) 17)? exact p-value compare simulated\np-value?nothing special success failure defined. Assume table \nN = 37 observations M = 13 successes (benign cell considered success). \nsample size 16 (round nuclei), find P(X \\(\\geq\\) 9). answer compare answer \nQuestion 11?Assume table N = 37 observations M = 13 successes (benign cell consid-\nered success). sample size 21 (concave nuclei), find P(X \\(\\geq\\) 4). answer com-\npare answer Question 11?*?positive integer, notation n! read “n factorial” defined \\(n!= n(n- 1)(n- 2) \\cdot\\cdot\\cdot (3)(2)(1)\\).\nexample, “3 factorial” \\(3 \\times 2 \\times 1= 6\\) “four factorial” \\(4!= 4 \\times 3 \\times 2 \\times 1= 24\\). addition, \\(0!= 1\\).simulation Section 6.3 Fisher’s exact test can considered permutation tests. simu-\nlation study provides approximation Fisher’s exact test. Fisher’s exact test provides \\(p\\)-value \nproblem \\(P(X \\geq 17)= 0.0225\\). truly difference likelihood malignancy \ntwo nucleus shapes (mean null hypothesis \\(H_0\\) true), random sampling produce\noutcome (17 malignant cells concave group) 2.25% time. small probability\nprovides evidence \\(H_0\\) rejected.\nFisher’s exact test corresponding simulation study derived using rather strong assump-\ntion null hypothesis. tests completed assumption observed 13\nbenign 24 malignant cells totals every randomization. Statisti-\ncians call conditional test independence. words, row column totals\nknown (fixed) study conducted. However, extended activities show Fisher’s\nexact test can used 2 \\(\\times\\) 2 table, even margin totals fixed.\\(^5\\)\n\nFisher’s exact test uses hypergeometric distribution provide exact p-values even small sample sizes succes/failure proportions near 0% 100%\n","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"two-sided-hypothesis-tests","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"13.3 Two-Sided Hypothesis Tests","text":"Fisher’s exact test simulation study, 17 concave malignant cells corresponded difference sample proportions \\(\\hat p_C - \\hat p_R = 0.8095 - 0.4375 = 0.372\\) . \\(p\\)-value probability calculating sample statistic greater equal \\(\\hat p_C - \\hat p_R = 0.372\\) given \\(p_C = p_R\\).sample collected, researchers medical reasons believe cells concave nuclei (.e., malformed nuclei) might likely malignant. specific reasoning justify one-sided hypothesis, two-sided hypothesis test appropriate:\\[\\begin{align}\nH_0: p_C = p_R \\quad\\text{vs.}\\quad H_a: p_C \\neq p_R\n\\tag{6.2}\n\\end{align}\\]\\(p\\)-value two-sided hypothesis probability calculating sample statistic least extreme \\(\\hat p_C - \\hat p_R = 0.372\\) given \\(p_C = p_R\\). , null hypothesis true, probability \\(\\hat p_C - \\hat p_R \\ge 0.372\\) \\(\\hat p_C - \\hat p_R \\le -0.372\\).Question 14 helps show difference proportions, \\(\\hat p_C - \\hat p_R \\le -0.372\\), corresponds 10 fewer concave malignant cells. Table 6.2 can used calculate approximate \\(p\\)-value two-sided hypothesis test:\\[\\begin{align}\n\\text{p-value}\n&= P(\\hat p_C - \\hat p_R \\le -0.372) + P(\\hat p_C - \\hat p_R \\ge 0.372) \\notag \\\\ \\notag\n&= P(X \\le 10) + P(X \\ge 17) \\\\ \\notag\n&= \\frac{1 + 8 + 118 + 197 + 21 + 2}{10{,}000} \\\\ \\notag\n&= 0.0347 \\notag\n\\end{align}\\]Based simulation, two-sided hypothesis test reject null hypothesis conclude \\(p_C \\neq p_R\\).","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"activity-calculating-a-two-sided-hypothesis-test","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"Activity: Calculating a Two-Sided Hypothesis Test","text":"three conditions Section 6.3 stay one-sided two-sided tests. total number malignant cells must 24, number concave cells 21, number round cells 16. Thus, \\(Y =\\) number concave malignant cells, \\(\\hat p_C - \\hat p_R = Y/21 - (24 - Y)/16\\).Find \\(\\hat p_C - \\hat p_R\\) \\(Y = 17\\).Find \\(\\hat p_C - \\hat p_R\\) \\(Y = 9, Y = 10,\\) \\(Y = 11\\).\nNotice two-sided test completely balanced; count, \\(Y\\), exactly corresponds \\(\\hat p_C - \\hat p_R = -0.372\\). However, \\(Y \\le 10\\) satisfy \\(\\hat p_C - \\hat p_R \\le -0.372\\).Repeat Question 8 estimate \\(p\\)-value two-sided hypothesis test.Use hypergeometric distribution find \\(p\\)-value two-sided hypothesis test.\nTwo-sided tests can somewhat cumbersome. Many statisticians suggest simply approximating two-sided \\(p\\)-value doubling one-sided \\(p\\)-value. example, \\(p\\)-value \\(H_0: p_C = p_R\\) vs. \\(H_a: p_C \\neq p_R\\) \\(2 \\times P(X \\ge 17) = 2(0.0225) = 0.045\\).","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"chi-square-test","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"13.4 Chi-Square Test","text":"Fisher’s exact test advantage providing exact p-values. technology available provide\nquick easy ways conduct Fisher’s exact test simulation studies, chi-square test typically\nused.may recall previous statistics courses chi-square test requires certain assumptions\nmet analysis done. example, large sample sizes needed, especially proportion\nsuccesses either group close 0% 100%. sample size large enough, distribution \nchi-square test statistic resemble chi-square distribution.\n\nStep 1: State null alternative hypotheses:null alternative hypotheses can written exactly terms two-sided per-\nmutation test:\\[\\begin{align}\nH_0: p_C &= p_R \\quad\\text{vs.}\\quad H_a: p_C \\neq p_R\n\\notag\n\\end{align}\\]*? 2 × 2 tables, chi-square test statistic identical square Z-statistic testing equal population proportions. addition, \\(p\\)-values two tests identical.null hypothesis equivalent stating distributions (malignant cells) \nacross groups nucleus shapes. test often called test homogeneity.\ntexts suggest using chi-square test homogeneity separate samples selected \none population using chi-square test independence data collected \nsingle sample. Even though study based one sample 37 slides, extended activities \nshow test homogeneity test independence appropriate. calculations\ntests identical; however, hypotheses conclusions different. choose present\ntest homogeneity section hypotheses conclusions coincide \ntwo-sided permutation tests.\nStep 2: Calculate test statistic:chi-square test statistic measure difference observed counts Table 6.1\nexpected counts calculated assumption null hypothesis true (shown\nTable 6.4).study, 7 16 round nuclei malignant (\\(\\hat p_R = 7/16= 0.4375\\)) 17 \n21 concave nuclei malignant (\\(\\hat p_C = 17/21= 0.8095\\)). Assuming proportion malignant\ncells nucleus shapes, best estimate overall proportion cells \nmalignant \\((7 + 17)/(16 + 21)= 24/37= 0.64865\\).expected cell count calculated multiplying estimated proportion times \n­appropriate sample size (total number concave nuclei total number round nuclei).\n16 round nuclei study, expected count round malignant cell nuclei \n\\(16(24/37)= 10.38\\). general, expected counts calculated row total \\(\\times\\) column total/\noverall total.","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"activity-calculating-expected-counts","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"Activity: Calculating Expected Counts","text":"Calculate expected count concave malignant cell nuclei.Assuming null hypothesis true, estimated proportion benign cells population? select sample 16 round cells, expected count round benign cells?Table 6.4 shows table expected counts assumption null hypothesis true: different cell nucleus shapes proportion malignant cells. :\\[\\begin{align}\n\\text{expected count} \\;=\\; \\frac{\\text{row total} \\times \\text{column total}}{\\text{overall total}}.\n\\notag\n\\end{align}\\]example:\\[\\begin{align}\n10.38 = \\frac{16 \\times 24}{37}, \\notag\n\\quad\n5.62 = \\frac{16 \\times 13}{37}. \\notag\n\\end{align}\\]\nchi-square test, expected counts calculated totals observed data. Totals table observed counts (like Table 6.1) table expected values (like Table 6.4) identical, except possible round-error.chi-square test statistic calculated measure observed data consistent null\nhypothesis. chi-square statistic \\[\\begin{align}\n\\chi^2 = \\sum \\frac{(\\text{observed count} - \\text{expected count})^2}{\\text{expected count}}\n\\tag{6.3}\n\\end{align}\\]statistic sum squared difference observed count expected count,\nweighted expected count. observed count represents observed cell count Table 6.1, \nexpected count represents expected cell count Table 6.4.chi-square statistic cancer cell study \\[\\begin{align}\n\\chi^2\n&= \\frac{(9 - 5.62)^2}{5.62} + \\frac{(7 - 10.38)^2}{10.38} + \\frac{(4 - 7.38)^2}{7.38} + \\frac{(17 - 13.62)^2}{13.62} \\\\[6pt] \\notag\n&= 2.03 + 1.10 + 1.55 + 0.84 = 5.52 \\notag\n\\end{align}\\]chi-square test statistic always positive. observed expected counts identical, \ntest statistic \\(\\chi^2 = 0\\). observed data far expected data, test statistic large\nnull hypothesis rejected. chi-square test used show one proportion greater\nless another proportion, show proportions equal. Thus, chi-square test \ntwo-sided test.\nchi-square test can extended categorical data two rows two col-\numns. Notice table expected values chi-square test statistic Equation (6.3) can \ncalculated additional rows columns\nStep 3: Calculate p-value:\np-value help us determine test statistic \\(\\chi^2 = 5.52\\) larger likely occur chance. \nnull hypothesis true, \\(\\chi^2\\) test statistic follow chi-square distribution degrees \nfreedom calculated (number rows - 1) \\(\\times\\) (number columns - 1). cancer cell study,\ntwo rows data (round concave) two columns data (benign malignant); thus,\ntest statistic \\((2- 1) \\times (2- 1) = 1\\) degree freedom.","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"activity-conducting-a-chi-square-test","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"Activity: Conducting a Chi-Square Test","text":"Use statistical software package conduct chi-square test cancer cell data.Submit computer output showing observed expected tables, test statistic, \np-value.Assuming data simple random sample larger population, conclusions can\ndraw population?Can conclude shape cell nucleus causes change likelihood cancer\ncell malignant? ?Step 4: Check model assumptions:Just like hypothesis tests, chi-square test requires observation independent. addition, chi-square test requires large enough sample size cells ensure test statistic can accurately represented chi-square distribution. statisticians slightly different technical assumptions sample size needed, two general rules:\n• 2 × 2 contingency tables, sample size large enough expected count 2 × 2 cells table least 5.\n• tables two rows two columns, expected counts greater 1 average expected count greater equal 5.cancer cell study, expected counts greater 5, appropriate use chi-square test.\nexample, expected counts just slightly greater 5. case, texts might suggest using chi-square test continuity correction. However, simulation studies Fisher’s exact test can now easily calculated computers provide accurate p-values. Thus, really longer need use chi-square continuity correction estimate p-value.Step 5: Draw conclusions within context study:chi-square test provides p-value 0.019 cancer cell study. indicates reject null hypothesis favor alternative. Thus, chi-square test leads conclusion two-sided simulated permutation test two-sided Fisher’s exact test: nucleus shape different proportion malignant cells.p-value 0.019 chi-square test somewhat close result simulation study conducted Section 6.3, found two-sided p-value 0.0347. Every person conducting chi-square test data Table 6.1 get p-value, simulation study provide slightly different p-value. Many students mistakenly assume variation p-value simulation study indicates less accurate p-value chi-square test. , fact, simulation study accurate chi-square test. Fisher’s exact test shows two-sided hypothesis test cancer cell study exact p-value 0.0357. larger sample sizes, p-values chi-square tests closer exact p-values.","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"activity-simulating-the-chi-square-test-statistic","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"Activity: Simulating the Chi-Square Test Statistic","text":"Degrees Freedom Create 2 × 2 contingency table totals margins Table 6.1. Assume counted 16 concave malignant nuclei Question 6. Fill rest table cells. Note can complete three table counts (concave benign, round malignant, round benign) just one known count value. Thus, one table cell count truly free—one cell determined, three fixed. demonstrates 2 × 2 contingency tables 1 degree freedom.Degrees Freedom Create 3 × 2 contingency table row totals 25, 30, 25 column totals 30 50. many table cell counts truly free (.e., smallest number table cell counts , filled , completely specify rest cells)? Describe formula (number rows - 1) × (number columns - 1) relates number free cells two-way contingency table size. may helpful create 3 × 3 table 3 × 4 table convince rule continues hold.","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"what-can-we-conclude-from-the-cancer-study","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"13.5 What Can We Conclude from the Cancer Study?","text":"cancer cell data Table 6.1 analyzed simulation study (specifically, permutation test), chi-square test, Fisher’s exact test. tests suggested null hypothesis rejected, thus conclude concave round cell nuclei associated different proportions malignant cells.study experiment, since random allocation units particular conditions. Thus, expect association nucleus shape malignancy, conclude nucleus shape causes different proportions malignancy.individuals provided 37 slide samples true random sample North Americans, sample patients volunteered part study University Wisconsin hospitals. Subjects medical studies rarely true random samples general population. certain results hold larger population people. However, seems reasonable researchers believe cancer cells patients Wisconsin similar cancer cells patients hospitals. Thus, can cautiously conclude study provides evidence different nucleus shapes associated different proportions malignant cells.","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"relative-risk-and-the-odds-ratio","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"13.6 Relative Risk and the Odds Ratio","text":"proportion malignant nuclei study 24/37 = 0.6486. overall proportion malignancy often called risk (baseline risk) malignancy. conditional proportion proportion malignant cells calculated cell shape. Thus, round nuclei conditional proportion (malignancy) = 7/16 = 0.4375, concave cells conditional proportion (malignancy) = 17/21 = 0.8095.hypothesis tests chapter focused determining whether concave round nuclei proportion malignancy. However, important recognize limitations testing differences proportions. example, assume two studies. first study,\\[\\begin{align}\n\\hat p_1 - \\hat p_2 &= 0.52 - 0.48 = 0.04\n\\notag\n\\end{align}\\]second study,\\[\\begin{align}\n\\hat p_1 - \\hat p_2 &= 0.05 - 0.01 = 0.04\n\\notag\n\\end{align}\\]studies test whether observed difference proportions 0.04 significant. However, second study \\(\\hat p_1\\) five times larger \\(\\hat p_2\\).alternative calculation commonly used data categorical response explanatory variables relative risk. following calculations, arbitrarily decided define malignant cell success.\\[\\begin{align}\n\\text{Relative risk} &= \\frac{\\text{proportion successes group 1}}{\\text{proportion successes group 2}} = \\frac{\\hat p_1}{\\hat p_2} \\tag{6.4}\n\\end{align}\\]cancer cell study, relative risk 0.8095/0.4375 = 1.85. Thus, risk malignancy 1.85 times greater concave group round group.odds (specifically, odds success) can also used compare proportions tend meaning broader range potential outcomes.\\[\\begin{align}\n\\text{Odds} &= \\frac{\\text{number successes}}{\\text{number failures}} \\tag{6.5}\n\\end{align}\\]odds malignancy concave group 17 4, meaning expect 17 successes (malignant cells) every 4 failures (benign cells). often stated follows: odds malignancy concave group 4.25 (17 ÷ 4) 1 (4 ÷ 4). odds malignancy round group 7 9. odds ratio used compare odds two groups.\\[\\begin{align}\n\\text{Odds ratio} &= \\frac{\\text{odds group 1}}{\\text{odds group 2}} = \\frac{\\hat\\theta_1}{\\hat\\theta_2} \\tag{6.6}\n\\end{align}\\]odds ratio cancer cell study \\(\\frac{17/4}{7/9} = 5.5.\\). Thus, odds malignancy 5.5 times greater concave group round group. odds ratio = 1, odds groups equal.\nrelative risk odds ratio calculations, group lower proportion (lower odds) typically considered group 2 (denominator). way, relative risk odds ratio always numbers larger one easier interpret.\n","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"extended-activity-calculating-additional-summary-statistics","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"Extended Activity: Calculating Additional Summary Statistics","text":"Data set: Table 6.1\n22. Use data Table 6.1 define benign cell success round cells group 1. Calculate interpret relative risk odds ratio.\n23. Show null hypothesis \\(H_0: p_1 = p_2\\) mathematically equivalent null hypothesis \\(H_0: \\theta_1/\\theta_2 = 1\\), \\(p\\) represents proportion successful \\(\\theta\\) represents odds success two groups (labeled 1 2).\n24. Shortcut Calculating Odds Ratio Use counts Table 6.1 calculate following:\\[\\begin{align}\n\\frac{\\text{(count round benign)(count concave malignant)}}{\\text{(count round malignant)(count concave benign)}}\n\\notag\n\\end{align}\\]calculation match statistic Question 22? product diagonals can always used shortcut calculate odds ratio.\nodds ratio depend choice success failure. addition, odds ratio provides identical results explanatory response variables switched.","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"cautions-about-relative-risk-reduction-in-medical-studies","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"Cautions About Relative Risk Reduction in Medical Studies","text":"Zocor drug used lower cholesterol order reduce chances heart attack. five-year study conducted investigate effectiveness Zocor, using 4444 people.\\(^6\\) People study aged 35-70 high risk heart attack. addition, subjects Caucasian 81% males. Based study, television print advertisements stated, “clinical study among people high cholesterol heart disease found 41% fewer deaths heart attack among taking Zocor.\\(^7\\)","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"extended-activity-comparing-relative-and-absolute-risk-reduction","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"Extended Activity: Comparing Relative and Absolute Risk Reduction","text":"Data set: Table 6.1Does advertisement claim study shows 41% reduction heart attacks among people use Zocor? Explain.advertisement claim study shows 41% reduction heart attacks among people use Zocor? Explain.41% fewer deaths mean terms chances heart attack? (Select one.)41% fewer deaths mean terms chances heart attack? (Select one.)2222 died heart attacks placebo group, 41% fewer (1289 2222) died heart attacks treatment group.1000 died heart attacks placebo group, 41% fewer (580) died heart attacks treatment group.100 died heart attacks placebo group, 41% fewer (58) died heart attacks treatment group.impossible tell based quote advertisement.actual study found 189 2223 placebo group died heart attacks 111 2221 treatment (Zocor) group died heart attacks.Use Zocor study data create two-way table. Use Placebo Treatment (Zocor) row variables. Use Death Survival column variables.Calculate percentage deaths placebo group percentage deaths treatment group.Calculate difference two percentages calculated Part b.Calculate interpret relative risk death heart attack.difference two percentages calculated Question 27b, 8.5% - 5% = 3.5%, called absolute risk reduction. 41% fewer deaths actually calculated \\[\\begin{align}\n\\frac{8.5\\% - 5\\%}{8.5\\%} \\approx 41\\%\n\\notag\n\\end{align}\\]statistic 41% called relative risk reduction. statistics, 3.5% 41%, appropriate, important recognize easily numbers can misunderstood. advertisement states things make reduction deaths due heart attacks appear greater absolute terms: reduction 8.5% 5% risk death heart attack next five years restricted sample people heart disease.","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"sampling-designs","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"13.7 Sampling Designs","text":"Contingency tables widely used easy interpret. However, important recognize appropriate statistical analysis determined simply looking table data; determined data collected. numerous ways collect data, section focus three key sampling designs often used observational studies.cross‑classification studies, information collected simultaneously variables study. often occurs data collected one sample placed classification table. row totals column totals known prior data collection. total sample size, N, may may known data collected.cohort studies, individuals (units) differ respect certain explanatory variable selected (assigned groups) response variable measured. predetermined groups called cohorts, response variable measured time design called prospective design.* cohort studies, totals corresponding explanatory variable known responses collected.case‑control studies, individuals (units) selected according response variable (often called cases controls). individuals classified according explanatory variable. Case‑control studies often retrospective studies, since historical data typically used collect information explanatory variable. case‑control studies, totals corresponding response variable known data collected explanatory variable.Prospective studies typically provide stronger evidence relationship explanatory variable response variable. However, tend expensive, require long time gather data, sensitive attrition. responses can rare, getting lung cancer, case‑control studies preferred cohort studies case‑control studies can ensure large enough sample size within group responses. However, data selected based response variable, case‑control studies, studies can particularly susceptible difficulties confounding.","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"extended-activity-retrospective-studies-and-the-odds-ratio","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"Extended Activity: Retrospective Studies and the Odds Ratio","text":"retrospective study lung cancer patients 20 London hospitals, Richard Doll identified relationship smoking lung cancer.\\(^8\\) Table 6.5 shows partial set data study. data collected 60 female patients lung cancer 60 control females.proportion females study lung cancer? proportion smokers got lung cancer? proportion nonsmokers got lung cancer? relative risk lung cancer defined success?Notice Doll predetermined distribution response variable (done case‑control studies). Explain proportions Question 28 appropriately extended larger population. (Hint: appropriate conclude 60/120 = 50% female patients 20 London hospitals lung cancer? appropriate assume good estimate percentage female nonsmoking patients hospitals lung cancer 37.2%?)Note tests equal proportions appropriate response totals fixed prior study. addition, next section show since response totals random, test independence used. Fisher’s exact test simulation study used. following questions show appropriate conduct hypothesis test odds ratio.*?Retrospective cohort studies also exist. designs, past (medical) records often used collect data. prospective cohort studies, objective first establish groups based explanatory variable. However, since past records data response variable can collected time.Calculate odds lung cancer smokers. Calculate odds lung cancer nonsmokers. Calculate interpret odds ratio lung cancer. odds ratio indicate relationship smoking lung cancer?Calculate interpret odds ratio smoker. odds ratio depend variable considered response?\n","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"tests-for-the-homogeneity-of-odds","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"Tests for the Homogeneity of Odds","text":"Questions 28–31 showed response variable totals fixed study conducted, proportions representative larger population. Thus, tests homogeneity proportions appropriate. section shows steps test homogeneity odds using data Table 6.5.Step 1: State null alternative hypotheses:\\[\\begin{align}\nH_0: \\frac{\\theta_S}{\\theta_N} &= 1 \\notag \\\\\nH_a: \\frac{\\theta_S}{\\theta_N} &> 1 \\notag\n\\end{align}\\]Step 2: Calculate test statistic:calculate odds cancer smoking group \\[\\begin{align}\n\\hat\\theta_S = \\frac{\\hat p_S}{1 - \\hat p_S} = 1.46 \\notag\n\\end{align}\\]Similarly, odds cancer nonsmoking group \\[\\begin{align}\n\\hat\\theta_N = \\frac{\\hat p_N}{1 - \\hat p_N} = 0.594 \\notag\n\\end{align}\\]odds ratio \\(\\frac{\\hat\\theta_S}{\\hat\\theta_N} = 2.466.\\)can shown (advanced texts) sample sizes large, natural log odds ratio approximately normal following standard deviation:\\[\\begin{align}\nS_{LO}\n&= \\sqrt{\\frac{1}{n_S\\,\\hat p\\,(1 - \\hat p)} + \\frac{1}{n_N\\,\\hat p\\,(1 - \\hat p)}}\n= \\sqrt{\\frac{1}{69(0.5)(0.5)} + \\frac{1}{51(0.5)(0.5)}} = 0.3693\n\\notag\n\\end{align}\\]\\(n_S\\) \\(n_N\\) total number smokers total number nonsmokers study, respectively, \\(\\hat p\\) overall proportion people lung cancer study. Also recall testing whether \\(\\theta_S/\\theta_N = 1\\) equivalent testing whether \\(\\ln(\\theta_S/\\theta_N) = 0\\).Thus, test statistic calculated \\[\\begin{align}\nZ\n&= \\frac{\\ln\\bigl(\\hat\\theta_S / \\hat\\theta_N\\bigr) - 0}{S_{LO}}\n= \\frac{0.90266}{0.3693} = 2.44 \\notag\n\\end{align}\\]Step 3: Calculate p-value:\\[\\begin{align}\nP(Z > 2.44) = 0.0073\n\\notag\n\\end{align}\\]Step 4: Check model assumptions:reasonable assume patient study independent others. general rule long sample size cell greater equal 5, normality assumption appropriate.Step 5: Draw conclusions within context study:study, small p-value leads us reject null hypothesis conclude odds ratio greater one. words, population odds cancer identical smokers nonsmokers, sample odds ratio large larger 2.466 unlikely occur random chance. study experiment; thus, inappropriate conclude smoking causes larger odds cancer. true random sample patients 20 London hospitals. However, reasonable cautiously expect patients representative hospital patients 20 London hospitals.","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"comparing-tests-of-homogeneity-and-independence","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"13.8 Comparing Tests of Homogeneity and Independence","text":"calculations involved tests homogeneity independence identical; however type question asked impact conclusions can drawn.test homogeneity used determine proportions equal across two populations. hypothesis test related cancer cell study\\[\\begin{align}\nH_0: p_L &= p_H \\notag \\\\\nH_a: p_L &\\neq p_H \\notag\n\\end{align}\\]fits situation. Tests homogeneity appropriate whenever one variables clearly defined response variable. shown previous section, test homogeneity proportions acceptable response totals known advance. However, test homogeneity odds appropriate even response totals fixed.test independence used determine two random variables within population independent. necessary determine variable explanatory variable response variable. marginal (.e., row column) totals known (fixed) advance, least one variables random, thus test independence appropriate. null alternative hypotheses can written \nTests homogeneity independence restricted chi-square tests. example, two-proportion z-test also test homogeneity.\ntests, null hypothesis relationship row variable column variable. Tests independence essentially testing probability success one variable depends value second variable. appropriate whenever row column variables random (e.g., column totals predetermined researcher), cross-classification studies.Tests homogeneity appropriate whenever clear one variable treated response explanatory variable. Thus, tests homogeneity can used three study designs listed earlier. However, case-control study designs (response totals fixed) appropriate testing homogeneity odds (homogeneity proportions). Table 6.6 summarizes appropriate hypothesis tests sampling design. Specific examples type sampling design provided end--chapter exercises.cancer cells study, concluded different cell nucleus shapes associated different\nproportions malignant cells. study, researchers clear explanatory variable (\\(Shape\\)) \nresponse variable (\\(Type\\)). Thus, test homogeneity appropriate. However, test independence \nalso appropriate, since subject selected one larger population: patients suspicious tumors\nUniversity Wisconsin hospitals. patients selected, biopsy FNA conducted\nobserved slide classified either malignant benign either round concave.\nchi-square test independence frequently used, conclusions test \ninformative. test prove two variables independent, identifies \nsignificant evidence two variables dependent. measurement degree independence (\nindication whether level dependence practical importance) given.","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"chi-square-goodness-of-fit-tests","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"13.9 Chi-Square Goodness-of-Fit Tests","text":"Many statistical techniques based specific model assumptions. example, many procedures discussed throughout text calculating hypothesis tests confidence intervals assume error terms follow normal distribution. model assumptions violated, tests considered reliable.Goodness--fit tests used determine well observed data “fit” model assumptions. words, want determine “close” observed values values expect specific (theoretical) distribution.goodness--fit test can used, need prior knowledge (benchmark values) theoretical model. chi-square goodness--fit test applied data placed groups. Sample data placed classes (observed groups), theoretical model used calculate expected number observations group. section, use goodness--fit tests determine observed counts (proportions) consistent hypothetical counts (proportions).\n","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"extended-activity-playing-a-dice-game","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"Extended Activity: Playing a Dice Game","text":"friend offers play simple game . six‑sided die roll 30 times. time rolls 5 6, pay $5. time rolls 1, 2, 3, 4, pay $5. results friend’s 30 rolls game shown Table 6.7.agreed play game, owed friend money. Table 6.7 called one-way table six cells. can consider results random sample possible rolls die, can assume roll independent. die fair, assume (null hypothesis ) six outcomes equally likely.visual inspection results, reason suspect die wasn’t fair? Explain.example, G = 6 groups (cells). Assuming die fair, probability roll results 1 \\(p_1 = 1/6\\), probability roll results 2 \\(p_2 = 1/6\\), etc. goodness--fit tests, expected values found hypothesized distribution determines probability G groups.case, assume \\(p_1 = p_2 = p_3 = p_4 = p_5 = p_6 = 1/6\\). expected count 6 groups can found n × pi. Assuming outcomes equally likely, fill expected counts Table 6.8.Conduct chi-square test determine significant difference observed expected.State null alternative hypotheses words.Calculate chi-square statistic using Equation (6.3).goodness--fit tests , parameters well defined, degrees freedom equal number cells minus 1. example, know many 1s, 2s, 3s, 4s, 5s rolled, number 6s fixed. Calculate p-value study.enough observations assume chi-square distribution appropriate?evidence believe friend used unfair die? Clearly state conclusions.\nchi-square goodness--fit test applied grouped data (.e., data put classes). Continuous data can easily placed groups. However, important recognize value chi-square test statistic depends data grouped.\n","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"chapter-summary-1","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"13.10 Chapter Summary","text":"chapter focused analyzing drawing conclusions categorical data. Fisher’s exact test, simulation studies, chi-square tests used analyze various data sets. Like hypothesis tests, tests valid care taken ensure appropriate assumptions met. tests assume observations independent.Fisher’s exact test appropriate sample size sampling design involving 2 × 2 contingency tables. Simulation studies used approximate Fisher’s exact test. simulation studies approximate, conducting 10,000 iterations typically provide precise p‑values.Simulation studies advantage adaptable. example, can easily modified number categories within study. (research project end chapter provides example simulation study independent observation assumption violated.)sample sizes large enough, chi-square test statistic follow chi-square distribution. order p‑values chi-square tests appropriate, following conditions satisfied:2 × 2 contingency tables, sample size large enough expected count cell table least 5.tables two rows two columns, expected counts greater  1 average expected count greater equal 5.Chi-square tests always two‑sided hypothesis tests. However, can easily extended contingency tables two rows two columns. two rows two columns, chi-square test specifically test significance cell. However, end‑‑chapter exercises show reasonable look cell’s contribution chi-square statistic.Three key sampling designs described chapter.cross-classified design, information collected simultaneously variables study. row totals column totals known prior data collection.cohort studies, totals corresponding explanatory variable known responses collected.case-control studies, totals corresponding response variable known data collected explanatory variable.Testing equivalence two proportions (\\(H_0: p_1 = p_2\\)) mathematically equivalent testing whether odds ratio equal 1 (\\(H_0: \\theta_1/\\theta_2 = 1\\)), \\(p\\) represents proportion successful \\(\\theta\\) represents odds success two groups (labeled 1 2). However, proportions interest small, may appropriate look odds ratio proportions. Tests equal proportions appropriate case-control studies, fixed response totals cause sample proportions accurately represent population.Chi-square tests homogeneity independence involve exactly mathematical calculations; however, hypotheses conclusions different. test homogeneity proportions appropriate whenever explanatory response variables clearly defined beginning study. explanatory totals may known advance, response totals fixed data collected. Tests independence appropriate one sample selected one population, thus neither explanatory response totals known advance (.e., explanatory response totals can considered random).chapter limited studies involving one categorical explanatory variable one categorical response variable. Additional tests Mantel–Haenszel procedure can used test equal proportions odds two explanatory variables. following chapters discuss general procedures, using logistic Poisson regression analyze data categorical response variable multiple categorical quantitative explanatory variables.","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"exercises-1","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"Exercises","text":"","code":""},{"path":"activity-descriptive-statistics-and-graphs.html","id":"endnotes","chapter":"13 Activity: Descriptive Statistics and Graphs","heading":"Endnotes","text":"","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"logistic-regression-the-space-shuttle-challenger","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14 Logistic Regression: The Space Shuttle Challenger","text":"best thing statistician get play everyone’s backyard.\n—John Tukey\\(^1\\)many investigations researcher interested developing regression model response variable dichotomous (two categories). Dichotomous responses can represented binary data (data values zero one). Logistic regression used examine relationship one explanatory variables binary response variable.chapter, look several studies, including O-ring failure data provided National Aeronautics Space Administration (NASA) space shuttle Challenger disaster, order introduce following logistic regression techniques:Calculating interpreting logistic regression modelUsing Wald statistic likelihood ratio tests determine significance individual explanatory variablesCalculating log-odds function maximum likelihood estimatesConducting goodness--fit tests evaluate model appropriatenessAssessing regression model performance looking classification table, showing correct incorrect classification response variableExtending logistic regression cases multiple explanatory variables","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"investigation-did-temperature-influence-the-likelihood-of-an-o-ring-failure","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14.1 Investigation: Did Temperature Influence the Likelihood of an O-Ring Failure?","text":"January 28, 1986, NASA space shuttle program launched 25th shuttle flight Kennedy Space Center Florida. Seventy-three seconds flight, external fuel tank collapsed spilled liquid oxygen hydrogen. chemicals ignited, destroying shuttle killing seven crew members board. Reports President Reagan videos event available Kennedy Space Center website.*Investigations showed O-ring seal right solid rocket booster failed isolate fuel supply. Figure 7.1 shows space shuttle Challenger just ignition fuel tank two 149.16-foot-long solid rocket boosters. Figure 7.2 shows diagram solid rocket booster. size, rocket boosters built shipped separate sections. forward, center aft field joint connected sections. Two O-rings (one primary one secondary), resemble giant rubber bands 0.28 inch thick 37 feet diameter, used seal field joints sections.O-ring seal used stop gases inside solid rocket booster escaping. However, cold outside air temperature caused O-rings become brittle fail seal properly. Gases 5800 °F escaped burned hole side rocket booster.[[[Figure 7.1]]]\nPicture space shuttle Challenger just ignition. solid rocket booster six O-rings, two field joint. O-rings right aft field joint failed.Report Presidential Commission Space Shuttle Challenger Accident, also known Rogers’ Commission Report, states:““O-ring resiliency directly related temperature. . . . warm O-ring compressed\nreturn original shape much quicker cold O-ring compression relieved.\n. . . compressed O-ring 75 degrees Fahrenheit five times responsive returning \nuncompressed shape cold O-ring 30 degrees Fahrenheit. . . . cold launch temperature\nexperienced, O-ring slow returning normal rounded shape. . . . \nremain compressed position O-ring channel provide space \nupstream channel wall. Thus, probable O-ring . . . seal gap time preclude\njoint failure due blow-erosion hot combustion gases. . . . 21 launches ambient\ntemperatures 61 degrees Fahrenheit greater, four showed signs O-ring thermal distress:\n.e., erosion blow-soot. launches 61 degrees Fahrenheit resulted one\nO-rings showing signs thermal distress.”\\(^2\\)[[[Figure 7.2]]]\nDiargam solid rocket boosterA lamentable aspect disaster problem O-rings already understood \nengineers prior Challenger launch. February 1984, Marshall Configuration Control\nBoard sent memo O-ring erosion occurred STS 41-B (10th space shuttle flight \n4th mission Challenger shuttle). Messages continued increase intensity, evidenced \n1985 internal memo Thiokol Corporation, company designed O-ring. Employees \nThiokol wrote following Vice President Engineering: “letter written ensure \nmanagement fully aware seriousness current O-Ring erosion problem SRM joints\nengineering standpoint.”\\(^3\\)temperature January 28, 1986, expected 31°F, Thiokol Corporation recommended\nChallenger launch. However, flight getting significant publicity high school\nteacher, Christa McAuliffe, flight. flight already delayed several times, \nquick solution O-ring concern. engineers overruled, decision made go\nahead launch. eventual presidential investigation stated,\n>“decision launch Challenger flawed. made decision unaware \nrecent history problems concerning O-rings joint unaware initial\nwritten recommendation contractor advising launch temperatures 53\ndegrees Fahrenheit continuing opposition engineers Thiokol management\nreversed position. clear understanding Rockwell’s concern \nsafe launch ice pad. decision makers known facts, highly\nunlikely decided launch 51-L January 28, 1986.”\\(^4\\)seems even though engineers comprehend severity problem, unable\nproperly communicate results. Prior ill-fated Challenger flight, solid rocket boosters 24\nshuttle launches recovered inspected damage. Even though O-ring damage present \nflights, O-rings damaged enough allow gas escape. Since damage \nminimal, 24 prior flights considered success NASA.Flight 4 missing data point rockets lost sea.Table 7.1 shows temperature time launch whether damage visible O‑rings. chapter, define successful launch one evidence O‑ring damage. Table 7.1, Successful Launch categorical variable, 0 representing launch O‑ring damage occurred 1 indicating successful launch O‑ring damage. Throughout rest investigation, relatively small data set Table 7.1 used demonstrate techniques can used determine likelihood O‑ring damage related temperature.","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"activity-describing-the-data-1","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"Activity: Describing the Data","text":"Based description Challenger disaster O‑ring concerns, identify variable space shuttle data set Table 7.1 explanatory variable response variable.Imagine engineer working Thiokol Corporation prior January 1986. Create graphs data Table 7.1. obvious temperature related success O‑rings? Submit charts graphs created show potential relationship temperature O‑ring damage.chapter, develop regression model using binary response variable, Successful Launch. \nspace shuttle data set, y = 1 represents successful flight O-ring damage y = 0 represents \nflight O-ring damage. Binary response data occur many fields; example, may want knowwhether disease present absentwhether person good credit risk loanwhether high school student admitted particular collegewhether individual involved substance abuseThe next section describes least squares regression model appropriate response \nbinary. Logistic regression used examine relationship one explanatory variables\nbinary response variable. Like regression models, logistic regression models often explana-\ntory variables quantitative, can categorical well","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"review-of-the-least-squares-regression-model","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14.2 Review of the Least Squares Regression Model","text":"Chapters 2 3, saw ordinary least squares regression model form\n\\[\\begin{align}\ny_i &= \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\quad\\text{} = 1, 2, 3, \\dots, n\n\\tag{7.1}\n\\end{align}\\]\\(n\\) number observations, \\(y_i\\) \\(\\)th value , \\(\\beta_0\\) \\(\\beta_1\\) regression coefficients, \\(x_i\\) \\(\\)th value explanatory variable, \\(\\epsilon_i\\) represents normally distributed errors constant variance. Equation (7.2) states mean response (expected response particular \\(x_i\\)) equal linear predictor \\(\\beta_0 + \\beta_1 x_i\\) observed value \\(x_i\\):\\[\\begin{align}\nE(Y_i \\mid x_i) &= \\beta_0 + \\beta_1 x_i\n\\quad\\text{} = 1, 2, 3, \\dots, n\n\\tag{7.2}\n\\end{align}\\]\\(\\beta_0\\) \\(\\beta_1\\) parameters can estimated sample data. addition assuming regression model linear predictor, assume error terms least squares regression model independent follow normal distribution zero mean fixed standard deviation:\\[\\begin{align}\n\\epsilon_i &\\overset{\\mathrm{iid}}{\\sim} N(0, \\sigma^2)\n\\quad\\text{} = 1, 2, 3, \\dots, n\n\\tag{7.3}\n\\end{align}\\]Equation (7.3) states independent identically distributed error term follows normal probability\ndistribution centered zero constant variance.\none explanatory variable, Equation (7.1), ordinary least squares regression often called simple linear regression. shown Chapter 3, model called least squares regression line minimizes sum squared residuals (difference observed value expected response). Least squares estimates \\(\\beta_0\\) \\(\\beta_1\\) (represented \\(b_0 = \\hat\\beta_0\\) \\(b_1 = \\hat\\beta_1\\)) can calculated even normality equal variance assumptions violated. However, assumptions error terms needed conduct hypothesis tests construct confidence intervals \\(\\beta_0\\) \\(\\beta_1\\).\n","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"activity-building-a-least-squares-regression-model","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"Activity: Building a Least Squares Regression Model","text":"Use data Table 7.1 create scatterplot least squares regression line space shuttle data. Calculate predicted response values \\(\\hat y = b_0 + b_1 x\\) temperature 60°F temperature 85°F.","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"the-logistic-regression-model","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14.3 The Logistic Regression Model","text":"response variable binary, response typically defined probability success, instead 0 1. example, Question 3, temperature 60°F, least squares regression line estimates probability successful launch 0.338. expected response particular \\(x_i\\) defined \\[\\begin{align}\n\\pi_i &= P(Y_i = 1) = \\text{probability launch O-ring damage temperature } x_i \\notag \\\\\n      &= E(Y_i \\mid x_i) \\notag \\\\\n      &= \\beta_0 + \\beta_1 x_i \\quad \\text{} = 1,2,3,\\dots,n \\tag{7.4}\n\\end{align}\\]linear model (\\(\\beta_0 + \\beta_1 x_i\\)) Equation (7.4) simple, appropriate use, since probabilities must 0 1. example, temperature value \\(x_i = 50\\), least squares regression model Question 3 predict probability \\(-0.036\\). order restrict predictions values 0 1, S-shaped function called log-odds function used.Logistic regression uses following model fit S-shaped relationship \\(\\pi\\) \\(x\\):\\[\\begin{align}\n\\ln\\bigl(\\frac{\\pi_i}{1 - \\pi_i}\\bigr) &= \\beta_0 + \\beta_1 x_i\n\\tag{7.5}\n\\end{align}\\]ln represents natural log, \\(\\beta_0\\) \\(\\beta_1\\) regression parameters, \\(\\pi_i\\) probability successful launch given temperature (\\(x_i\\)). ratio \\(\\pi/(1 - \\pi)\\) called odds, probability success probability failure. Thus, function ln[\\(\\pi/(1 - \\pi)\\)] called log-odds \\(\\pi\\) logistic logit transformation \\(\\pi\\).* Figure 7.3 shows least squares regression model logistic regres-\nsion model space shuttle data.\nChapter 6, odds outcome defined \\(\\pi/(1 - \\pi)\\), probability success (O-ring damage) probability failure (O-ring damage). example, computer randomly selects day week, odds selecting Saturday (Saturday considered success) 1 6, since\\[\\begin{align}\n\\text{odds} = \\frac{\\pi}{1 - \\pi} = \\frac{1/7}{(1 - (1/7))} = \\frac{1}{6}.\n\\notag\n\\end{align}\\]Similarly, odds 6 1 Saturday selected (day Saturday success).\n\n(#fig:fig7.3)Figure 7.3 Space shuttle data simple linear regression model logistic regression model.\n*?Throughout chapter, use terms , actually use natural logs (ln) \ncalculations.Equation (7.5) can solved \\(\\pi_i\\) show \\[\\begin{align}\n\\pi_i &= \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}} \\tag{7.6}\n\\end{align}\\]\nBinary logistic regression assumes \\(x_i\\) value, response variable \\(Y_i\\) follows Bernoulli distribution (described extended activities). means assume (1) \\(Y_i\\) independent, (2) \\(Y_i\\) falls exactly one two categories represented either zero one, (3) \\(x_i\\), \\(P(Y_i = 1) = \\pi_i\\) \\(P(Y_i = 0) = 1 - \\pi_i\\) (specifically written \\(P(Y_i = 1 \\mid x_i) = \\pi_i\\) \\(P(Y_i = 0 \\mid x_i) = 1 - \\pi_i\\)). third assumption states given explanatory variable (specific temperature value), probability success (O-ring failures) constant.\npossible use least squares regression techniques estimate \\(\\beta_0\\) \\(\\beta_1\\) logistic regression models. However, assumptions needed hypothesis tests confidence intervals using ordinary least squares regression model met. Specifically, even log-odds transformation, Figure 7.3 demonstrates residuals normally distributed variability residuals depends explanatory variable.Residuals (observed values minus expected values) used estimate error terms. Visual inspection residual plots often used check normality. Recall previous work regression residuals normally distributed, scatterplot residuals versus explanatory variable resemble randomly scattered oval points. example, resemble random scatter see happened drop 23 coins (one residual value).logistic regression, residuals \\(y_i - \\hat\\pi_i\\). observed response \\(y_i = 0\\), residual value \\(-\\hat\\pi_i\\). observed response \\(y_i = 1\\), residual value \\(1 - \\hat\\pi_i\\). leads two curves shown Figure 7.4. temperature low space shuttle data (around 55°F, seen Figure 7.3), observed responses tend zero predicted responses (\\(\\hat\\pi_i\\)’s) small positive numbers. Thus, residual values (\\(-\\hat\\pi_i\\)’s) negative close zero. temperature high, observed responses tend one, predicted responses close one, residual values positive close zero.\n(#fig:fig7.4)Figure 7.4 scatterplot residuals space shuttle logistic regression model sample scatterplot normally distributed residuals might look like.\n\nresponse regression model binomial, \\(\\pi_i = P(Y_i = 1)\\) probability success (launch O-ring damage temperature \\(x_i\\)). simple linear regression models binomial response,\\[\\begin{align}\ny_i &= \\pi_i + \\epsilon_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\quad \\text{} = 1,2,3,\\dots,n \\tag{7.7}\n\\end{align}\\]logit transformation, logistic regression models binomial response following form:\\[\\begin{align}\ny_i &= \\pi_i + \\epsilon_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}} + \\epsilon_i \\quad \\text{} = 1,2,3,\\dots,n \\tag{7.8}\n\\end{align}\\]logit transformation results nice S-shaped curve, error terms Equations (7.7) (7.8) constant normally distributed. Thus, hypothesis tests confidence intervals calculated using least squares regression.\n","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"the-logistic-regression-model-using-maximum-likelihood-estimates","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14.4 The Logistic Regression Model Using Maximum Likelihood Estimates","text":"Logistic regression special case known generalized linear model. Generalized linear models expand linear regression models cases normal assumptions hold. generalized linear models three components:Generalized linear models can also used response variable follows distributions. example, \\(y\\) may follow Poisson gamma distribution. Textbooks generalized linear models derive link functions types response variables. least squares regression response normal distribution, Equation (7.1), link function simply identity function. words, response needs transformation simple linear regression models.Clearly logistic regression model Figure 7.3 nonlinear. may seem somewhat surprising consider logistic regression generalized linear model. reason still call model linear link function, log-odds transformation, modeled linear predictor, \\(\\beta_0 + \\beta_1 x\\).Instead using least squares estimates, generalized linear models use method maximum likelihood estimate coefficients \\(\\beta_0\\) \\(\\beta_1\\). extended activities provide detail calculating maximum likelihood estimates logistic regression. space shuttle example, simply use computer software package find maximum likelihood estimates \\(\\beta_0\\) \\(\\beta_1\\). least squares regression, often transform response variable (\\(y\\)) data fit model assumptions. addition linearizing data, transforming \\(y\\) impacts variability distribution error terms. generalized linear models, link function transforms expected response (\\(\\pi\\)) fit linear predictor. calculus, link functions also differentiable invertible.\n","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"activity-using-software-to-calculate-maximum-likelihood-estimates","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"Activity: Using Software to Calculate Maximum Likelihood Estimates","text":"Solve Equation (7.5) \\(\\pi_i\\) show Equation (7.6) true.Use Equation (7.6) create six graphs. graph, plot explanatory variable (\\(x\\)) versus expected probability success (\\(\\pi\\)) using \\(\\beta_0 = -10\\) \\(-5\\) \\(\\beta_1 = 0.5\\), \\(1\\), \\(1.5\\). Repeat process \\(\\beta_0 = 10\\) \\(5\\) \\(\\beta_1 = -0.5\\), \\(-1\\), \\(-1.5\\).submit graphs, explain impact changing \\(\\beta_0\\) \\(\\beta_1\\).graphs, value \\(\\pi\\) appears steepest slope?Use statistical software calculate maximum likelihood estimates \\(\\beta_0\\) \\(\\beta_1\\). Compare maximum likelihood estimates least squares estimates Question 3.Figure 7.3 shows logistic regression model using maximum likelihood estimates \\(\\beta_0\\) \\(\\beta_1\\). Using Equation (7.6) maximum likelihood estimates Question 6, can estimate probability launch O-ring damage temperature \\(x_i\\):\\[\\begin{align}\n\\hat\\pi_i &= \\frac{e^{b_0 + b_1 x_i}}{1 + e^{b_0 + b_1 x_i}}\n           = \\frac{e^{-15.043 + 0.232 x_i}}{1 + e^{-15.043 + 0.232 x_i}}\n           \\quad \\text{} = 1,2,3,\\dots,n \\tag{7.9}\n\\end{align}\\]Notice \\(\\pi\\) Equation (7.6) replaced \\(\\hat\\pi\\) Equation (7.9) parameters linear regression model (\\(\\beta_0\\) \\(\\beta_1\\)) estimated sample data; \\(b_0 = \\hat\\beta_0 = -15.043\\) \\(b_1 = \\hat\\beta_1 = 0.232\\).","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"activity-estimating-the-probability-of-success-with-maximum-likelihood-estimates","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14.4.1 Activity: Estimating the Probability of Success with Maximum Likelihood Estimates","text":"Use Equation (7.9) predict probability launch O-ring damage temperature 31°F, 50°F, 75°F.point, seems reasonable question O-rings considered higher risk time 1986 Challenger launch. , odds successful launch (O-ring damage) expected temperature 31°F 1 2555 predicted odds change dramatically based temperature. important recognize previous launches result disaster Challenger launch O-rings showed “minor” damage. wasn’t enough gas escape—indicator O-rings might resilient expected. Estimating value temperature 31°F extrapolating beyond data set. Just least squares regression, caution used making predictions outside range explanatory variables available.\n","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"the-logistic-regression-model-using-maximum-likelihood-estimates-1","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14.5 The Logistic Regression Model Using Maximum Likelihood Estimates","text":"Logistic regression special case known generalized linear model. Generalized linear models expand linear regression models cases normal assumptions hold. generalized linear models three components:Generalized linear models can also used response variable follows distributions. example, \\(y\\) may follow Poisson gamma distribution. Textbooks generalized linear models derive link functions types response variables. least squares regression response normal distribution, Equation (7.1), link function simply identity function. words, response needs transformation simple linear regression models.Clearly logistic regression model Figure 7.3 nonlinear. may seem somewhat surprising consider logistic regression generalized linear model. reason still call model linear link function, log-odds transformation, modeled linear predictor, \\(\\beta_0 + \\beta_1 x\\).Instead using least squares estimates, generalized linear models use method maximum likelihood estimate coefficients \\(\\beta_0\\) \\(\\beta_1\\). extended activities provide detail calculating maximum likelihood estimates logistic regression. space shuttle example, simply use computer software package find maximum likelihood estimates \\(\\beta_0\\) \\(\\beta_1\\). least squares regression, often transform response variable (\\(y\\)) data fit model assumptions. addition linearizing data, transforming \\(y\\) impacts variability distribution error terms. generalized linear models, link function transforms expected response (\\(\\pi\\)) fit linear predictor. calculus, link functions also differentiable invertible.\n","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"activity-using-software-to-calculate-maximum-likelihood-estimates-1","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"Activity: Using Software to Calculate Maximum Likelihood Estimates","text":"Solve Equation (7.5) \\(\\pi_i\\) show Equation (7.6) true.Use Equation (7.6) create six graphs. graph, plot explanatory variable (\\(x\\)) versus expected probability success (\\(\\pi\\)) using \\(\\beta_0 = -10\\) \\(-5\\) \\(\\beta_1 = 0.5\\), \\(1\\), \\(1.5\\). Repeat process \\(\\beta_0 = 10\\) \\(5\\) \\(\\beta_1 = -0.5\\), \\(-1\\), \\(-1.5\\).submit graphs, explain impact changing \\(\\beta_0\\) \\(\\beta_1\\).graphs, value \\(\\pi\\) appears steepest slope?Use statistical software calculate maximum likelihood estimates \\(\\beta_0\\) \\(\\beta_1\\). Compare maximum likelihood estimates least squares estimates Question 3.Figure 7.3 shows logistic regression model using maximum likelihood estimates \\(\\beta_0\\) \\(\\beta_1\\). Using Equation (7.6) maximum likelihood estimates Question 6, can estimate probability launch O-ring damage temperature \\(x_i\\):\\[\\begin{align}\n\\hat\\pi_i\n&= \\frac{e^{b_0 + b_1 x_i}}{1 + e^{b_0 + b_1 x_i}} \\\\\n&= \\frac{e^{-15.043 + 0.232 x_i}}{1 + e^{-15.043 + 0.232 x_i}} \\\\\n&\\quad \\text{} = 1,2,3,\\dots,n \\tag{7.9}\n\\end{align}\\]Notice \\(\\pi\\) Equation (7.6) replaced \\(\\hat\\pi\\) Equation (7.9) parameters linear regression model (\\(\\beta_0\\) \\(\\beta_1\\)) estimated sample data; \\(b_0 = \\hat\\beta_0 = -15.043\\) \\(b_1 = \\hat\\beta_1 = 0.232\\).","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"activity-estimating-the-probability-of-success-with-maximum-likelihood-estimates-1","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14.6 Activity: Estimating the Probability of Success with Maximum Likelihood Estimates","text":"Use Equation (7.9) predict probability launch O-ring damage temperature 31°F, 50°F, 75°F.point, seems reasonable question O-rings considered higher risk time 1986 Challenger launch. , odds successful launch (O-ring damage) expected temperature 31°F 1 2555 predicted odds change dramatically based temperature. important recognize previous launches result disaster Challenger launch O-rings showed “minor” damage. wasn’t enough gas escape—indicator O-rings might resilient expected. Estimating value temperature 31°F extrapolating beyond data set. Just least squares regression, caution used making predictions outside range explanatory variables available.\n","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"interpreting-the-logistic-regression-model","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14.7 Interpreting the Logistic Regression Model","text":"Interpretation logistic regression models often done terms odds success (odds launch O-ring damage). temperature 59°F, odds successful launch O-ring damage \\(\\hat\\pi/(1 - \\hat\\pi) = 0.2066/(1 - 0.2066) = 0.2605 \\approx 0.25 = 1/4\\). Thus, 59°F, state odds successful launch 1 4. temperature 60°F, odds successful launch \\(\\hat\\pi/(1 - \\hat\\pi) = 0.3285 \\approx 0.333 \\approx 1/3\\). 60°F, state odds successful launch 1 3.slope easy interpret logistic regression model simple linear regression model. ordinary least squares regression focuses \\(\\beta_1\\), logistic regression measures change odds success term \\(e^{b_1}\\), called odds ratio. increase \\(x_i\\) 1 unit logistic regression model, predicted odds \\(y = 1\\) (.e., launch O-ring damage) multiplied \\(e^{b_1}\\). example, temperature changes 59°F 60°F, odds increase multiplicative factor \\(e^{b_1} = e^{0.232} = 1.2613\\). words,\\[\\begin{align}\n\\text{odds success 59°F} \\times e^{b_1}\n&= 0.2605(1.2613) \\notag \\\\\n&= 0.3285 \\notag \\\\\n&= \\text{odds success 60°F} \\notag\n\\end{align}\\]temperature value \\(x_i\\), relationship can also stated \\[\\begin{align}\n\\text{odds ratio} = e^{b_1} &= \\frac{\\text{odds}(x_i + 1)}{\\text{odds}(x_i)} \\tag{7.10}\n\\end{align}\\]\nTaking exponent Equation (7.5), can write odds success \n\\[\\begin{align}\n\\text{odds} = \\biggl(\\frac{\\pi_i}{1 - \\pi_i}\\biggr) &= e^{\\beta_0 + \\beta_1 x_i} = e^{\\beta_0}(e^{\\beta_1})^{x_i} \\tag{7.11}\n\\end{align}\\]Thus, \\(x_i\\) increases 1,\n\\[\\begin{align}\ne^{\\beta_0}(e^{\\beta_1})^{x_i + 1} &= e^{\\beta_0}(e^{\\beta_1})^{x_i}(e^{\\beta_1}) \\tag{7.12}\n\\end{align}\\]\nslope logistic regression model typically described terms odds ratio \\(e^{b_1}\\). increase \\(x_i\\) 1 unit, predicted odds multiplied \\(e^{b_1}\\). example, temperature increases one degree, increase odds successful launch \\(e^{b_1} = e^{0.232} = 1.2613\\) times. Similarly, decrease \\(x_i\\) 1 unit, predicted odds multiplied \\(e^{-b_1} = 1/e^{b_1}\\).\n","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"activity-interpreting-a-logistic-regression-model","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14.8 Activity: Interpreting a Logistic Regression Model","text":"Calculate odds launch O-ring damage temperature 60°F temperature 70°F.\\(x_i\\) increases 10, state terms \\(e^{b_1}\\) much expect odds change.difference odds success 60°F 59°F 0.3285 – 0.2605 = 0.068. expect difference odds 52°F 51°F also 0.068? Explain .Create plot two logistic regression models. Plot temperature versus estimated probability using maximum likelihood estimates Question 6, plot temperature versus estimated probability using least squares estimates Question 3.Thus far, developed model estimate odds successful launch O-ring failures. However, yet discussed variability estimates confident can results. next section, discuss two hypothesis tests can used determine odds successful launch related temperature. words, can conclude logistic regression coefficient \\(b_1\\) equal zero?\nprobability, odds, log-odds three closely related calculations. Even though \nthree used express concepts interest, log-odds often used estimate \ncoefficients, interpretation logistic regression models typically relies expected probabilities\nodds easier interpret.\n","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"inference-for-the-logistic-regression-model","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14.9 Inference for the Logistic Regression Model","text":"","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"assumptions-for-logistic-regression-models","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"Assumptions for Logistic Regression Models","text":"Inference logistic regression uses statistical theory based limits sample size approaches infinity. techniques, based called asymptotic theory, work well large sample sizes, approximate outliers small sample sizes. common logistic regression models developed data sets size, savvy statisticians always use caution interpreting results data sets small sample sizes (space shuttle example).","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"the-wald-statistic","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"The Wald Statistic","text":"Wald’s test often used test significance logistic regression coefficients. Just least squares regression, set hypothesis test determine relationship explanatory response variables:\\[\\begin{align}\nH_0: b_1 = 0 \\quad \\text{vs.}\\quad H_a: b_1 \\neq 0\n\\end{align}\\]Wald’s test similar one-sample Z-test seen introductory statistics courses. Wald statistic calculated \\[\\begin{align}\nZ = \\frac{b_1 - 0}{\\text{se}(b_1)} = \\frac{0.232}{0.108} = 2.14 \\tag{7.13}\n\\end{align}\\]\ntexts use chi-square statistic instead Z-statistic given Equation (7.13). probability textbooks explain square test statistic Equation (7.13) follows chi-square distribution 1 degree freedom. techniques provide identical p-values.\n\\(b_1\\) maximum likelihood estimate \\(\\beta_1\\) se(\\(b_1\\)) standard error \\(b_1\\). maximum likelihood estimate, \\(b_1\\), asymptotically normally distributed (.e., \\(b_1\\) normally distributed sample size large). Thus, Z-statistic Equation (7.13) follow standard normal distribution null hypothesis true sample size large. model, see estimated slope coefficient \\(b_1 = 0.232\\) p-value \\(P(|Z| \\geq 2.14) = 0.032\\).Wald confidence intervals can also created. logistic regression, confidence interval often discussed terms odds ratio. example, 95% confidence interval \\(\\beta_1\\) given \\[\\begin{align}\n(e^{b_1 - Z^* \\text{se}(b_1)}, e^{b_1 + Z^* \\text{se}(b_1)}) = (e^{0.232 - 1.96(0.108)}, e^{0.232 + 1.96(0.108)}) = (e^{0.02}, e^{0.44}) = (1.02, 1.56) \\tag{7.14}\n\\end{align}\\]1.96 = Z^* represents value corresponding 95% confidence interval normal distribution mean 0 standard deviation 1. \\(b_1 = 0\\), thus odds ratio \\(e^{b_1} = 1\\), odds success temperature \\(x_i\\) odds success temperature. Thus, \\(e^{b_1} = 1\\) tells us association explanatory variable response.\n95% Wald confidence interval odds ratio contain 1, reject null hypothesis \\(H_0: \\beta_1 = 0\\) (using alpha-level 0.05) conclude odds success depend explanatory variable \\(x_i\\). interval contain 1, fail reject \\(H_0: \\beta_1 = 0\\).\nMinitab output Figure 7.5 shows Wald’s test corresponding confidence interval \nodds ratio. space shuttle example, 95% confidence interval include \\(e^{\\beta_1} = 1\\); thus, can\nreject null hypotheses conclude odds successful launch depend temperature.\nEven though computer software provided small p-value confidence interval include 1, \nimportant note 23 observations study. Wald’s test reasonable \nlarge sample sizes, smaller sample sizes known tendency result type II error—failing\nreject null hypothesis rejected.\\(^5\\)can also calculate odds ratio successful launch 60°F 70°F. \\(x_i\\)\nincreases 10°F, odds multiplied \\((e^{b_1})10 = 1.2613^{10} = 10.19\\). Thus, approximately\n10 times higher odds successful launch temperature 70°F 60°F. 95%\nconfidence interval odds ratio successful launch 60°F 70°F can given \n\\((1.02^10, 1.56^10) = (1.22, 85.40)\\). 95% confidence interval wide range; odds success\n70°F just slightly larger odds 60°F 85 times large odds 60°F. \nlarge range suggests estimate odds ratio, 10.19, highly variable. data needed \nbetter understand true odds ratio.","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"activity-wald-confidence-intervals-and-hypothesis-tests","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"Activity: Wald Confidence Intervals and Hypothesis Tests","text":"Calculate odds ratio successful launch 31°F 60°F. Provide confidence interval\nodds ratio interpret results.coefficients Equation (7.9) calculated successful launch given value 1. Con-\nduct logistic regression analysis 1 indicates O-ring failure 0 represents successful launch.Explain relationships model shown Equation (7.9) new model.regression coefficients change?odds ratio change?Create 95% Wald confidence interval new odds ratio interpret results.[[[Figure 7.5]]]?Recall Z Equation (7.13) statistic calculated sample data Z* Equation (7.11) called critical\nvalue. Z* represents value based desired level confidence. Z* known data collected, Z \nbased sample data","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"the-likelihood-ratio-test","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14.10 The Likelihood Ratio Test","text":"likelihood ratio test (LRT) derived calculating difference adequacy full restricted log-likelihood models. full model (sometimes called unrestricted model) includes parameters consideration model. example, two parameters, \\(\\beta_0\\) \\(\\beta_1\\), full model include parameters explanatory variables model. restricted model (also called reduced model) model fewer terms full model. example, \\(\\beta_0\\) restricted model (explanatory variables model). explanatory variables restricted model, restricted model also called null model. full model significantly better fit (expected values closer observed values) restricted model, reject null hypothesis \\(H_0: \\beta_1 = 0\\) conclude \\(H_a: \\beta_1 \\neq 0\\).log-likelihood (restricted) function, described extended activities, measure fit model includes intercept:\\[\\begin{align}\n\\text{Restricted Model:} \\quad \\pi_i = \\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}\n\\notag\n\\end{align}\\]restricted model, null hypothesis true \\(\\pi_i\\) constant x-value. log-likelihood (full) function measures fit model includes parameters interest:\\[\\begin{align}\n\\text{Full Model:} \\quad \\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\n\\notag\n\\end{align}\\]null hypothesis \\(H_0: \\beta_1 = 0\\) true, can shown \\[\\begin{align}\nG = 2 \\times \\text{log-likelihood(full model)} - 2 \\times \\text{log-likelihood(restricted model)} \\sim \\chi^2_1\n\\tag{7.15}\n\\end{align}\\]G-statistic measures difference fits restricted full models. essence, \nmeasuring much better fit explanatory variable (temperature) added logistic\nmodel. p-value corresponding G-statistic small, difference fits large unlikely\noccur chance, thus conclude Ha: b1 0 (fit full model significantly better \nrestricted model).Degrees freedom LRT equal number parameters full model minus number \nparameters restricted model. case, 2 - 1 = 1. Different software packages present\ntest slightly different ways. Minitab output Figure 7.5, log-likelihood full model\n(-10.158) G-statistic (7.952) provided.statistics packages may give G-statistic, give enough information \nLRT can calculated. R output shown Figure 7.6 gives null deviance [K - 2 * log-\nlikelihood (restricted model)] residual deviance [K - 2 * log-likelihood (full model)], \nK constant value.Note \n\\[\\begin{align}\nG &= 2 \\times \\text{log-likelihood(full model)} - 2 \\times \\text{log-likelihood(restricted model)} \\notag \\\\\n&= \\text{null (restricted model) deviance} - \\text{residual (full model) deviance} \\notag \\\\\n&= 7.952. \\notag\n\\end{align}\\]","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"activity-the-likelihood-ratio-test","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"Activity: The Likelihood Ratio Test","text":"Use statistical software calculate LRT space shuttle data. Submit p-value state conclusions.G-statistic relatively large, indicating evidence reject \\(H_0: beta_1 = 0\\) conclude temperature related odds successful launch O-ring damage. sample sizes large one explanatory variable model, p-values LRT Wald’s test approximately . space shuttle example, LRT Wald’s test somewhat different p-values.likelihood ratio test reliable often preferred Wald’s test small sample sizes. However, unlike LRT, Wald’s test can one-sided alternative hypothesis tests well nonzero hypothesized values. difficult determine actual sample size needed Wald’s test LRT perform well. statisticians suggest minimum sample size 100 observations.\\(^7\\) Thus, best label p-value approximate using tests small sample size.\nlarge sample size, Wald’s test likelihood ratio test provide accurate tests \\(H_0: \\beta_1 = 0\\) versus \\(H_a: \\beta_1 \\neq 0\\). LRT test tends reliable smaller sample sizes, use caution interpreting results data sets small sample sizes (space shuttle example).\n","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"what-can-we-conclude-from-the-space-shuttle-study","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14.11 What Can We Conclude from the Space Shuttle Study?","text":"space shuttle example observational study, since launches “randomly assigned” temperature groups, conclude solely data set low temperatures caused O-ring damage. Wald’s test likelihood ratio test provided evidence odds successful launch related temperature. However, sample size 23 large enough us confident p-values reliable. logistic regression model provides indication probability successful launch related temperature. information, scientists understanding cold temperatures cause O-rings brittle, also strengthens conclusion.","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"logistic-regression-with-multiple-explanatory-variables","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14.12 Logistic Regression with Multiple Explanatory Variables","text":"Wolberg Mangasarian developed technique accurately diagnose breast masses using visual characteristics cells within tumor. sample placed slide, characteristics cellular nuclei within tumor, size, shape, texture, examined microscope determine whether cancer cells benign malignant. Benign tumors scar tissue abnormal growths spread typically harmless. Malignant (invasive) cancer cells cells can travel, typically bloodstream lymph nodes, begin replace normal cells parts body. tumor malignant, essential remove destroy cancerous cells order keep spreading. tumor benign, surgery typically needed harmless tumor can remain.Chapter 6, used contingency tables two variables, cell shape type, better understand analyze two categorical variables. section describe process variable selection logistic regression, using radius concavity cell nuclei estimate probability tumor malignant. data set, radius actually average radius (micrometers, \\(\\mu\\)m) visible cell nuclei slide, refer variable simply cell radius tumor. concavity cell nuclei indicator whether visible cell nuclei sample nice round shape typical healthy cells whether cells appear grown way perimeters cell nuclei tend concave points.","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"extended-activity-estimating-the-probability-of-malignancy-in-cancer-cells","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"Extended Activity: Estimating the Probability of Malignancy in Cancer Cells","text":"Data set: Cancer2Create logistic regression model using Radius Concave explanatory variables estimate probability mass malignant.Using Radius first explanatory variable, \\(x_1\\), Concave second explanatory variable, \\(x_2\\), submit logistic regression model. words, find coefficients model\\[\\begin{align}\n   y_i &= \\frac{e^{\\beta_0 + \\beta_1 x_{1,} + \\beta_2 x_{2,}}}{1 + e^{\\beta_0 + \\beta_1 x_{1,} + \\beta_2 x_{2,}}} + \\epsilon_i \\quad \\text{} = 1,2,\\dots,n\n\\end{align}\\]Submit likelihood ratio test results, including log-likelihood (deviance) values.Concave = 0 represents round cells Concave = 1 represents concave cells. Calculate event probability Radius = 4 cells concave. Also calculate event probability Radius = 4 cells concave.Create logistic regression model using Radius explanatory variable estimate probability mass malignant.Submit logistic regression model likelihood ratio test results, including log-likelihood (deviance) values.Calculate event probability Radius = 4.multiple explanatory variables logistic regression model, model created Question 15, likelihood ratio test compares full model null model, excludes radius concavity terms. Thus, null hypothesis coefficient corresponding explanatory variables zero. Question 15, LRT testing\\[H_0: \\beta_1 = \\beta_2 = 0 \\quad \\text{vs.}\\quad H_a: \\text{least one coefficients zero}\\]G-statistic hypothesis test 527.42 3 - 1 = 2 degrees freedom (number parameters full model minus number parameters restricted model) corresponding p-value < 0.001. Thus, can reject \\(H_0\\) conclude least one explanatory variables significantly related probability cells malignant.coefficients multiple logistic regression models discussed terms odds success. coefficient (\\(b_j\\)) indicates response change corresponding \\(j\\)th explanatory variable, conditional explanatory variables model. \\(j\\)th explanatory variable increased one unit, odds success multiplied \\(e^{b_j}\\). explanatory variable (\\(x_j\\)) binary, Concave , \\(e^{b_j}\\) represents odds ratio two groups. However, just ordinary least squares regression multiple explanatory variables, coefficients conditional terms model.","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"the-drop-in-deviance-test","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14.13 The Drop-in-Deviance Test","text":"Figure 7.7 provides expected probabilities model created Question 15. probability cell malignant appears depend Radius. addition, appears concavity important variable model, since concave cells tend higher estimated probability malignant. Chapter 3, variable selection described process determining explanatory variables included regression model. Ideally, like simplest model (.e., model fewest terms) best explains response (.e., model smallest residuals). example, want determine model Question 16 can estimate probability malignancy just accurately slightly complex model Question 15. models similar abilities estimate probability malignancy, prefer simpler model.logic determining whether additional terms logistic regression model essentially LRT discussed connection space shuttle study. log-likelihood (deviance) can used measure well model fits data. full model two explanatory variables, \\(x_1\\) \\(x_2\\), much better fit restricted model just one explanatory variable, \\(x_1\\), can conclude \\(x_2\\) significant included model.LRT shows significant difference full restricted model, coefficient \nsecond variable, \\(x_2\\), can set zero can conclude including additional variable \nmodel improve ability estimate probability success (case, success represents \nmalignant cell).cancer study, can compare full log-likelihood (deviance) two-term model \nQuestion 15 restricted log-likelihood (deviance) one-term model Question 16.\\[\\begin{align}\nG &= 2 \\times \\text{log-likelihood (full model)} - 2 * \\text{log-likelihood (restricted model)} \\notag \\\\\n&= 2(-112.008) - 2(-165.005) \\notag \\\\\n&= \\text{null (restricted model) deviance} - \\text{residual (full model) deviance} \\notag \\\\\n&= 105.994 \\tag{7.16}\n\\end{align}\\]Just LRT described connection shuttle example, degrees freedom calculated \nnumber parameters full model minus number parameters restricted model. Thus, can\ntest whether \\(Concave\\) (\\(x_2\\)) included model finding \\(p\\)-value, percentage \n\\(\\chi^2\\) distribution 3 - 2 = 1 degree freedom exceeds \\(G\\). \\(p\\)-value corresponding Equation\n(7.16) less 0.0001. strong evidence explanatory variable, \\(Concave\\), important \nmodel \\(Radius\\) already included. Thus, logistic regression model Question 15 preferred\nmodel Question 16.\n(#fig:fig7.7)Figure 7.7 scatterplot observed data estimated probabilities round cells (Concave= 0) concave cells (Concave= 1).\nLRT used variable selection, often called change--deviance test drop--deviance test. test valid restricted model nested within full model. \nrestricted model nested full model every explanatory variable restricted model also\nfull model.\n\n\ndrop--deviance can also used simultaneously test multiple variables. example, let’s assume\nfour Shape measurements (round, slightly concave, moderately concave, severely\nconcave). four levels used create three indicator variables. interested \ntesting whether \\(Shape\\) important, test three coefficients simultaneously. steps \ntesting three coefficients simultaneously identical steps listed except instead \njust testing \\(x_j\\), simultaneously testing \\(x_2\\), \\(x_3\\), \\(x_4\\). null hypothesis \n\\(H0: \\beta_2 = \\beta_3 = \\beta_4 = 0\\), coefficients corresponds one \\(Shape\\) indicator variable\nfull model.\nDrop--deviance tests often used combination stepwise regression techniques order \nidentify best model prediction. end--chapter exercises, backward elimination procedures\nanalogous used multiple least squares regression used find appropriate model. \nprocedure starts explanatory variables interest model. Variables appear \nsignificant (significantly reduce size residuals) removed stepwise process. \nprocess continued variables model significant (model consists variables\nimportant reducing size residuals).Just least squares regression, often “best” multivariate logistic regression model. Differ-\nent stepwise procedures, sampling variability, desired balance accuracy simplicity\nmodel, choice terms tested can influence final model selected. \ncareful justification selecting final model, caution used stating selected\nmodel “best.”\nRecall stepwise techniques useful developing models goal estimate predict \nresponse. However, appropriate goal developing regression model theory test-\ning. Stepwise procedures involve multiple testing variables. leads unreliable p-values\ntesting coefficients individual explanatory variables. Thus, inappropriate use stepwise\nprocedures find good model test coefficient final model determine \nindividual explanatory variables significant.\n\nsoftware packages programs automatically suggest model logistic regression\n(just done stepwise procedures least squares regression). chapter focuses \ndrop--deviance test, techniques can used variable selection.\n\\[\\begin{align}\n\\textbf{Akaike’s Information Criterion (AIC)} &= -2 \\times \\text{log-likelihood} + 2 \\times p \\notag \\\\\n\\textbf{Bayesian Information Criterion (BIC)} &= -2 \\times \\text{log-likelihood} + p \\times ln(n) \\notag\n\\end{align}\\]\np number estimated parameters (number explanatory variables plus 1) n \nsample size. AIC BIC adjust number parameters model \nlikely select models fewer variables drop--deviance test. techniques suggest choos-\ning model smallest AIC BIC value\n","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"measures-of-association","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14.14 Measures of Association","text":"sample sizes small, model may strong association (clear pattern visible) enough evidence show independent variable significant. Conversely, thousands observations data set, hypothesis test might conclude independent variable significant even though weak association. Thus, researchers typically report significance tests measure association discussing results. widely accepted equivalent \\(R^2\\) logistic regression, section describe calculations can used measure strength association.measure strength association space shuttle logistic regression model, pair observed success every observed failure. shuttle example, 16 successes 7 failures; thus, \\(16 \\times 7 = 112\\) pairs. pair, use logistic regression model estimate probability success observed success observed failure. observation corresponding success higher estimated probability, pair called concordant pair. observation corresponding success lower estimated probability, pair called discordant pair. Tied pairs occur observed success estimated probability observed failure.possible pairs concordant, Somers’ \\(D\\) equal 1. model predictive power, expect half pairs concordant half discordant. correspond Somers’ \\(D = 0\\). Thus, value 0 Somers’ \\(D\\) (well Goodman Kruskal’s gamma) indicates effect explanatory variable response variable.","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"extended-activity-measures-of-association","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"Extended Activity: Measures of Association","text":"Data set: \\(Shuttle\\)first eleventh launches form pair, since 63°F O-ring failure 66°F success (O-ring failure). concordant pair, since probability success higher observed success. Estimate probability success temperature.Calculate expected probabilities first (66°F) 22nd (75°F) observations. concordant discordant pair?Identify two launches space shuttle data represent tied pair.Various statistical software packages tend provide different measures association. Use statistical software calculate Goodman-Kruskal gamma, Somers’ \\(D\\), Kendall’s tau-space shuttle data.Minitab output space shuttle data provided Figure 7.8. output shows 75.9% pairs\nconcordant, 19.6% pairs discordant. provides evidence association\ntemperature probability successful launch.\\[\\begin{align}\n\\text{Somers’ D} &= (85 - 22)/112 = 0.56 \\notag \\\\\n\\text{Goodman-Kruskal gamma} &= (85 - 22)/(112 - 5) = 0.59 \\notag \\\\\n\\text{Kendall’s Tau-} &= (85 - 22)/(253) = 0.25 \\notag \\\\\n\\text{} 253 &= 23 \\times 22/2, \\text{total number pairs} \\notag\n\\end{align}\\]Somers’ D Goodman-Kruskal gamma close \ntied pairs. \\(p\\)-values corresponding measures, useful compar-\ning different models different explanatory variables comparing models based different link\nfunctions.[[[Figure 7.8]]]","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"review-of-means-and-variances-of-binary-and-binomial-data","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14.15 Review of Means and Variances of Binary and Binomial Data","text":"worked discrete probability models, recognize binary data follow Bernoulli distribution following conditions true:observation, \\(y_i\\), independent.\\(y_i\\) falls exactly one two categories represented either zero one.probability success, \\(P(Y_i = 1) = \\pi_i\\), constant observation.Bernoulli distribution can displayed Table 7.2.\nTable 7.2 often represented following mathematical function model Bernoulli distribution:\\[\\begin{align}\nP(Y = k) &= \\pi^k (1 - \\pi)^{1-k} \\quad \\text{} k = 0, 1 \\tag{7.17}\n\\end{align}\\]expected value (mean) \\(y\\) average outcome:\\[\\begin{align}\nE(Y) &= 0 \\times P(Y = 0) + 1 \\times P(Y = 1) = 0 \\times (1 - \\pi) + 1 \\times \\pi = \\pi \\tag{7.18}\n\\end{align}\\]Note outcome equally likely. Thus, outcome weighted probability. variance \\(y\\) average value squared deviation observed value, \\(y\\), expected value, \\(\\pi\\):\\[\\begin{align}\n\\mathrm{Var}(Y) &= E[(Y - \\pi)^2] = (0 - \\pi)^2 \\times P(Y = 0) + (1 - \\pi)^2 \\times P(Y = 1) = \\pi (1 - \\pi) \\tag{7.19}\n\\end{align}\\]equations, \\(\\pi\\) every observational unit results observational unit independent . However, regression focus expected value \\(y\\) given \\(x\\), \\(E(Y \\mid x_i)\\). represents expectation probability success depends explanatory variable. space shuttle example, expected probability successful launch change depending temperature. given temperature value, \\(x_i\\), probability success constant, \\(\\pi_i\\), observations independent. Thus, particular \\(x_i\\), corresponding mean variance \\[\\begin{align}\nE(Y \\mid x_i) &= \\pi_i \\quad \\text{} \\quad \\mathrm{Var}(Y \\mid x_i) = \\pi_i (1 - \\pi_i)\n\\tag{7.20}\n\\end{align}\\]Thus, logistic regression Bernoulli response variables, variance \\(Y\\) depend \\(x\\). violates\nkey assumption constant variance least squares regression models.Logistic regression also appropriate response count number successes. count follows binomial distribution following conditions true:\\(n_i\\) independent observations given level \\(x\\) (\\(x_i\\)).\\(\\pi_i = P(Y_i = 1 \\mid x_i)\\) probability success, probability constant given \\(x_i\\) (\\(0 \\le \\pi_i \\le 1\\)).response two possible outcomes. However, instead recording 0 1 value outcome, typically record \\(Y\\) count successes (proportion successes) particular \\(x_i\\) value.Many introductory textbooks show data follow binomial distribution, probability \\(k\\) successes \\(n_i\\) independent observations :\\[\\begin{align}\nP(Y = k \\mid x_i) &= \\binom{n_i}{k} \\,\\pi_i^k (1 - \\pi_i)^{n_i - k} \\quad \\text{} k = 0, 1, \\dots, n_i \\tag{7.21}\n\\end{align}\\]\\(\\displaystyle \\binom{n_i}{k} = \\frac{n_i!}{k!\\,(n_i - k)!}\\) called binomial coefficient read “\\(n_i\\) choose \\(k\\).”Using technique similar Bernoulli distribution, can shown conditional mean (probability success) variance binomial response variables \\[\\begin{align}\nE(Y \\mid x_i) &= n_i \\times \\pi_i \\quad \\text{} \\quad \\mathrm{Var}(Y \\mid x_i) = n_i \\times \\pi_i (1 - \\pi_i)\n\\tag{7.22}\n\\end{align}\\]","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"extended-activity-understanding-the-binomial-distribution","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"Extended Activity: Understanding the Binomial Distribution","text":"Assume particular temperature \\(x_i = 70^\\circ\\mathrm{F}\\) true probability success \\(\\pi_i = 0.75\\). four launches made \\(x_i = 70^\\circ\\mathrm{F}\\), use Equation (7.21) find probability four launches successful, \\(P(Y_i = 4 \\mid x_i = 70)\\). Also find probability one four launches successful, \\(P(Y_i = 1 \\mid x_i = 70)\\).Assume particular temperature \\(x_i = 70^\\circ\\mathrm{F}\\) true probability success \\(\\pi_i = 0.75\\). four launches made \\(x_i = 70^\\circ\\mathrm{F}\\), use Equation (7.21) find probability four launches successful, \\(P(Y_i = 4 \\mid x_i = 70)\\). Also find probability one four launches successful, \\(P(Y_i = 1 \\mid x_i = 70)\\).four observations (\\(n_i = 4\\)) \\(\\pi_i = 0.75\\), use basic formula calculating means\nfindWhen four observations (\\(n_i = 4\\)) \\(\\pi_i = 0.75\\), use basic formula calculating means\nfindWhen \\(n_i = 4\\) \\(\\pi_i = 0.75\\), find \\(\\mathrm{Var}(Y_i \\mid x_i)\\).\\(n_i = 4\\) \\(\\pi_i = 0.75\\), find \\(\\mathrm{Var}(Y_i \\mid x_i)\\).","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"calculating-logistic-regression-models-for-binomial-counts","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14.16 Calculating Logistic Regression Models for Binomial Counts","text":"previous examples, \\(y\\) considered binary random variable (either 0 1). example, logistic regression used response count number successes (.e., response binomial). Table 7.3 shows cancer cell data \\(Radius\\) variable grouped five levels. Clearly grouping necessary logistic regression, grouping done demonstrate conduct logistic regression response binomial.","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"extended-activity-binomial-logistic-regression","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"Extended Activity: Binomial Logistic Regression","text":"Data set: Table 7.3Create logistic regression model based Equation (7.8) predict probability malignant cell grouped radius data Table 7.3.maximum likelihood estimates \\(b_0\\) \\(b_1\\)?Substitute \\(b_0\\) \\(b_1\\) Equation (7.9) estimate probability malignancy radius 4.5.Use Wald’s test \\(G\\)-statistic determine model provides evidence probability malignancy related cell radius.Figure 7.9 plots observed percentages malignant cells corresponding logistic regression model.\nNotice observed expected probabilities fairly close. However, observed percentage \nmalignant cells higher expected cell radius 4.5.\n(#fig:fig7.9)Figure 7.9 logistic regression model, \\(\\hat \\pi_i\\) estimated Equation (7.8) using maximum likelihood estimates, plotted observed probability malignancy grouped data Table 7.3.\n","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"calculating-residuals-for-logistic-models-with-binomial-counts","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14.17 Calculating Residuals for Logistic Models with Binomial Counts","text":"response variable follows binomial distribution Table 7.3, Pearson residuals \ndeviance residuals often calculated. residuals calculated evaluate well model\nfits data.Using estimate \\(\\hat\\pi_i = \\frac{e^{b_0 + b_1 x_i}}{1 + e^{b_0 + b_1 x_i}}\\) \\(\\pi_i\\), find Pearson residual taking number observed successes (\\(y_i\\)) minus estimated number expected successes (\\(n_i \\times \\hat\\pi_i\\)) dividing estimated standard deviation:\\[\\begin{align}\n\\text{Pearson residual $$th radius value}\n= \\frac{y_i - n_i \\,\\hat\\pi_i}{\\sqrt{n_i \\,\\hat\\pi_i\\,(1 - \\hat\\pi_i)}}\n\\tag{7.23}\n\\end{align}\\]Since variance constant, residual weighted estimated standard deviation.\ncan calculate Pearson residual using count data know individual radius, \\(x_i\\), response variable, \\(y_i\\), follows binomial distribution estimated mean \\(n_i \\times \\hat\\pi_i\\) variance \\(n_i \\times \\hat\\pi_i \\times (1 - \\hat\\pi_i)\\).","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"extended-activity-evaluating-residuals-in-binomial-logistic-regression","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"Extended Activity: Evaluating Residuals in Binomial Logistic Regression","text":"Data set: Table 7.3Create logistic regression model predict probability malignant cell grouped radius data Table 7.3.Use software calculate Pearson residuals.Create histogram /normal probability plot residuals. residuals normally distributed?Create scatterplot Pearson residuals versus radius. accurate estimates radius 2.5 radius 4.5?observation, deviance residual measures change deviance full null model. deviance residual square root deviance goodness––fit statistic cell (distinct radius value). deviance residual \n\\[\\begin{align}\nD_i \\,\\pm\\, \\sqrt{2 \\Bigl[\ny_i \\ln\\bigl(\\tfrac{y_i}{n_i\\,\\hat\\pi_i}\\bigr)\n\\;+\\;(n_i - y_i)\\ln\\bigl(\\tfrac{n_i - y_i}{n_i - n_i\\,\\hat\\pi_i}\\bigr)\n\\Bigr]}\n\\tag{7.24}\n\\end{align}\\]\nsign positive observed \\(y_i\\) higher estimated mean negative \nobserved \\(y_i\\) less estimated mean. Repeat Question 27 use deviance residual instead\nPearson residual.","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"assessing-the-fit-of-a-logistic-regression-model-with-binomial-counts","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14.18 Assessing the Fit of a Logistic Regression Model with Binomial Counts","text":"goodness‐‐fit test measures well model fits observed data. discuss three goodness‐‐fit tests logistic regression based chi‐square distribution. discussed Chapter 6, chi‐square goodness‐‐fit test asymptotic test measures accumulated distance observed values expected values (.e., values predicted model). three tests, null alternative hypotheses \\[\\begin{align}\nH_0:\\;&\\text{logistic regression model provides adequate fit data} \\notag \\\\\nH_a:\\;&\\text{model adequately fit data} \\notag\n\\end{align}\\]predicted values model relatively close observed data values (.e., model good fit data), test statistic small \\(p\\)-value large. test statistic large, suggests “lack fit”; \\(H_0\\) rejected models tried better fit data.\nGoodness‐‐fit tests assess overall fit logistic regression model. \\(H_0\\) rejected, model good fit. Failing reject \\(H_0\\) guarantee model “best fit” even good fit, rather simply don’t enough evidence prove ’s poor fit.Test 1: Pearson chi‐square goodness‐‐fit test traditional chi‐square goodness‐‐fit test seen many introductory statistics courses. Degrees freedom calculated number groups (classes) minus number parameters estimated. case, groups classified median radius; five groups estimating two parameters (\\(\\beta_0\\) \\(\\beta_1\\)), \\(5 - 2 = 3\\) degrees freedom.Test 2: deviance goodness‐‐fit test (also called likelihood ratio chi‐square test) based sum squared deviance residuals. test statistic follows chi‐square probability distribution degrees freedom calculated number groups minus number parameters estimated. Pearson test statistic deviance test statistic tend similar. determining model, deviance goodness‐‐fit test preferred Pearson test.\\(^9\\)\nsum squared Pearson residuals equal Pearson chi‐square test statistic. one explanatory variable, likelihood ratio test equivalent deviance goodness‐‐fit test. residual deviance degrees freedom given R identical given deviance goodness‐‐fit test statistic Minitab.Test 3: Hosmer Lemeshow developed test similar Pearson chi‐square goodness‐‐fit test. key distinction groups formed differently. Pearson test uses explanatory variable form groups, Hosmer‐Lemeshow test uses predicted values sort data form groups (default 10 groups). Table 7.3, predetermined five groups, Pearson Hosmer‐Lemeshow tests identical example. However, explanatory variable continuous (grouped), Hosmer‐Lemeshow test reliable Pearson chi‐square goodness‐‐fit test.","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"extended-activity-calculating-residuals-by-hand","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"Extended Activity: Calculating Residuals by Hand","text":"Data set: \\(Cancercells\\)Calculate Pearson chi‐square goodness‐‐fit test hand Cancercells data.Complete Table 7.4 using logistic regression model Question 26 calculate expected (predicted) cell counts.Calculate Pearson chi‐square test statistic:\\[\\begin{align}\n\\chi^2 &= \\sum \\frac{(\\text{observed count} - \\text{expected count})^2}{\\text{expected count}}\n\\tag{7.25}\n\\end{align}\\]\nusing observed count expected count Table 7.4. Note 10 observed 10 expected cells.Find \\(p\\)-value using software looking statistic chi‐square probability distribution table.Interpret results clearly state conclusions impacted random sampling random allocation treatments original study design. Remember small \\(p\\)-value provides evidence model good fit.Calculate deviance chi‐square goodness‐‐fit test hand Cancercells data given Table 7.3. Expected counts, degrees freedom, \\(p\\)-values found way Pearson test. difference calculation test statistic:\\[\\begin{align}\n   \\text{Deviance chi-square test statistic} = D^2 = 2 \\times \\sum \\biggl[\\text{observed count} \\times \\ln\\bigl(\\frac{\\text{observed count}}{\\text{expected count}}\\bigr)\\biggr]\n   \\tag{7.26}\n   \\end{align}\\]\nDescribe differences () conclusions Pearson test Question 29.Figure 7.10 shows Minitab output logistic regression model fit three goodness--fit tests.\nNote three goodness--fit tests show small p-values, indicating can reject \\(H_0\\) con-\nclude model good fit. addition, cell counts shown Question 29 reveal sample\nsize large enough us believe tests reliable. Thus, models tried. logistic\nregression model shown Figure 7.10 looks reasonable, many possible explanations \ndata considered good fit:groups based median radius may appropriate. example, radii 1.51 2.49 \nconsidered part group. create groups smaller class sizes. Clearly lose\ninformation whenever data grouped categories, using original data likely best option.groups based median radius may appropriate. example, radii 1.51 2.49 \nconsidered part group. create groups smaller class sizes. Clearly lose\ninformation whenever data grouped categories, using original data likely best option.Additional explanatory variables may need included model. Just ordinary regres-\nsion, may appropriate include interaction terms transformations (square, cube,\nlog) explanatory variable(s).Additional explanatory variables may need included model. Just ordinary regres-\nsion, may appropriate include interaction terms transformations (square, cube,\nlog) explanatory variable(s).binomial model may appropriately model response variable transformation \nlogit transformation may need tried. Notice logit model assumes symmetry; curves S shape symmetric around midpoint data.\n[[[Figure 7.10]]]binomial model may appropriately model response variable transformation \nlogit transformation may need tried. Notice logit model assumes symmetry; curves S shape symmetric around midpoint data.\n[[[Figure 7.10]]]outliers may significantly influencing results. example, median radius \n4.5, observed probability close expected value. point greatly contributes \nlarge chi-square statistics Questions 29 30.outliers may significantly influencing results. example, median radius \n4.5, observed probability close expected value. point greatly contributes \nlarge chi-square statistics Questions 29 30.Since chi-square tests based asymptotic theory, p-values reliable unless large\nenough sample size. general rule analyzing 2 \\(\\times\\) 2 table counts expected count \ncell must least 5. larger tables, expected counts cells must least 1, average\nexpected cell counts greater 5.data sparse (observed, therefore expected, cell counts small), chi-square\ntests may tend fail reject null hypothesis states model good fit data.\\(^10\\) \nwords, tests may good power detecting particular types lack fit. Hosmer-Lemeshow\ntest designed correct problem (continuous explanatory variables) grouping\ndata. Hosmer Lemeshow suggest sample size least 400 using test.\\(^11\\)","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"extended-activity-goodnessoffit-tests-for-continuous-explanatory-variables","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"Extended Activity: Goodness‐of‐Fit Tests for Continuous Explanatory Variables","text":"Data set: \\(Cancer2\\)Use computer software create logistic regression model predict probability malignant cell using continuous variable \\(Radius\\) explanatory variable. Conduct three goodness‐‐fit tests model.goodness--fit tests Question 31 look different calculated grouped data\nFigure 7.10. Remember goodness--fit tests, degrees freedom calculated number \ngroups minus number parameters estimated. two parameters estimated, stated 454\ndegrees freedom Pearson deviance tests indicate 456 groups formed (based distinct\nradius value given data). Since 569 observations data set, Pearson deviance goodness-\n-fit tests meet sample size requirements reliable test (cells one observation).\nQuestion 31, Pearson deviance goodness--fit tests fail reject \\(H_0\\): logistic regression\nmodel provides adequate fit data. However, violation sample size requirement makes \ntests inappropriate use.Hosmer-Lemeshow test Question 31 8 degrees freedom, since default form 10\ngroups based fitted values. form groups, statistical software sorts estimated probabilities \nattempts create 10 groups equal size. observed expected values given software\noutput. Notice still two cells expected values less 1. Recent studies shown \nHosmer-Lemshow test somewhat sensitive way groups formed, specific tests\ndeveloped.\\(^12\\) However, since p-value much bigger \\(\\alpha = 0.05\\), appear \nstrong evidence model good fit.Hosmer Lemeshow state slight violations sample size requirements acceptable.\\(^13\\) \nsuggest sample size concern, simply group columns. example, Question\n31, grouping columns 1 2 grouping columns 9 10 satisfy sample size requirements.","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"extended-activity-goodnessoffit-tests-for-continuous-explanatory-variables-1","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"Extended Activity: Goodness‐of‐Fit Tests for Continuous Explanatory Variables","text":"Data set: \\(Cancer2\\)Conduct Hosmer‐Lemeshow goodness‐‐fit test , time adjust group size (number groups) sample size requirement violated. \\(p\\)-value new Hosmer‐Lemeshow test change conclusions?\nPearson, deviance, Hosmer‐Lemeshow tests assess model fit chi‐square tests. one explanatory variables continuous (often case), deviance Pearson chi‐square tests useful enough observations observed expected cell (number distinct values explanatory variable nearly equal number observations). Hosmer‐Lemeshow test can used continuous explanatory variables uses predicted values group data.\n","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"diagnostic-plots","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14.19 Diagnostic Plots","text":"Just least squares regression, residual plots useful understanding logistic models. Goodness‐‐fit tests useful, , stated earlier, tend fail reject null hypothesis even model appropriate. chi‐square tests fail conclude model inappropriate, residual plots can used verify model fit appropriate. addition, goodness‐‐fit tests reject null hypothesis (conclude model appropriate), residual plots can help identify issues model fit occur.Large residual values useful identifying observations explained well model. addition residuals, several scatterplots can used identify outliers influential observations. creating plots, need define additional terms.covariate pattern set observations identical explanatory variables. space shuttle example, four observations temperature 70°F. four observations form covariate pattern. Cancer2 data set, covariate pattern group observations Radius value Concave value.Delta chi-square (\\(\\Delta \\chi^2\\)) measure change Pearson goodness--fit statistic (x2) \nparticular observation (covariate pattern one observation explanatory\nvalues) eliminated. words, particular xi value, delta chi-square \\(\\Delta \\chi^2_i = \\chi^2 \\;-\\;\\chi^2_{()}\\), \\(\\chi^2_{()}\\) Pearson goodness‐‐fit statistic \\(\\)th observation (covariate pattern) eliminated.Delta deviance (\\(\\Delta D^2\\)) measure change deviance goodness--fit statistic (D2) \nparticular observation (covariate pattern) eliminated. words, particular \\(x_i\\) value, delta chi-\nsquare \\(\\Delta D^2_i = D^2 \\;-\\; D^2_{()}\\), \\(D^2_{()}\\) deviance goodness‐‐fit statistic \\(\\)th observation (covariate pattern) eliminated.Delta beta measures difference regression coefficient particular observation (covariate pattern) removed.Figure 7.11 shows delta chi‐square, delta deviance, delta beta values plotted expected probabilities (\\(\\hat\\pi_i\\)). determination whether observation (covariate pattern) outlier overly influential somewhat subjective. Outliers typically appear extreme values upper corners scatterplot. rough estimate, \\(\\Delta\\chi^2\\) \\(\\Delta D^2\\) greater 4 delta beta values greater 1 may considered unusual observations. large sample sizes, \\(\\Delta\\chi^2\\) \\(\\Delta D^2\\) approximately follow chi‐square distribution, 95th percentile chi‐square distribution 1 degree freedom equals 3.84. Figure 7.11 one covariate pattern fairly large \\(\\Delta\\chi^2\\) \\(\\Delta D^2\\) values. corresponds two launches occurred 75°F. Notice Figure 7.11 failure 75°F appears somewhat unusual. example, \\(\\Delta\\chi^2\\) \\(\\Delta D^2\\) values extreme enough major concern (values close less 4).\n(#fig:fig7.11)Figure 7.11 Scatterplots delta deviance, delta chi-square, delta beta values versus expected probabilities space shuttle data. Circled values represent launches 75°F.\nFigure 7.12 shows delta chi‐square delta beta values plotted leverage values. Leverages values 0 1 depend explanatory variables (response). Large leverage values indicate observation (covariate pattern) extreme explanatory values may large influence regression coefficients.\n(#fig:fig7.12)Figure 7.12 Scatterplots delta chi-square delta beta values versus leverage space shuttle data. extreme value along y-axis represents launches 75°F. appear extreme leverage values.\n\ncontribution single observation depends residual leverage. delta chi‐square delta deviance values can used detect observations strong influence goodness‐‐fit statistics. large delta beta value indicates covariate pattern large leverage /large residual values.","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"extended-activity-identifying-outliers-and-influential-observations","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"Extended Activity: Identifying Outliers and Influential Observations","text":"Data set: \\(Cancer2\\)Run logistic regression model explanatory variables Radius Concave.Create histograms standardized Pearson residuals deviance residuals.Create scatterplots delta deviance, delta chi‐square, delta beta (standardized) versus expected probabilities.Create scatterplots delta deviance, delta chi‐square, delta beta (standardized) versus leverage.Identify observations (covariate patterns) appear outliers influential observations.","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"maximum-likelihood-estimation-in-logistic-regression","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14.20 Maximum Likelihood Estimation in Logistic Regression*","text":"Maximum likelihood estimation fairly complex topic. goal section simply provide example calculate maximum likelihood estimates binomial data. completing section, better understanding LRT deviance test calculated. However, stated earlier, maximum likelihood estimates computationally intensive best left computer algorithms.\nassumptions residuals least squares regression model described Section 7.2 satisfied, maximum likelihood estimates \\(\\beta_0\\) \\(\\beta_1\\) identical least squares estimates.","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"maximum-likelihood-estimator-for-binary-data","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"Maximum Likelihood Estimator for Binary Data","text":"Consider set independent binary responses \\(y_1, y_2, \\dots, y_n\\). Since observed response independent follows Bernoulli distribution shown Equation (7.17), probability particular outcome can found \\[\\begin{align}\nP(Y_1 = k_1,\\,Y_2 = k_2,\\,\\dots,\\,Y_n = k_n)\n&= P(Y_1 = k_1)\\,P(Y_2 = k_2)\\,\\cdots\\,P(Y_n = k_n) \\notag \\\\\n&= \\pi^{k_1}(1 - \\pi)^{1-k_1}\\,\\pi^{k_2}(1 - \\pi)^{1-k_2}\\,\\cdots\\,\\pi^{k_n}(1 - \\pi)^{1-k_n} \\notag \\\\\n&= \\pi^{\\sum_{=1}^n k_i}\\,(1 - \\pi)^{\\sum_{=1}^n (1 - k_i)}\n\\tag{7.27}\n\\end{align}\\]\\(k_1, k_2, \\dots, k_n\\) represent particular observed series 0 1 outcomes \\(\\pi\\) probability, \\(0 \\le \\pi \\le 1\\). \\(k_1, k_2, \\dots, k_n\\) observed, fixed values. Maximum likelihood estimates functions sample data derived finding value \\(\\pi\\) maximizes likelihood function. given observed data set \\(y_1, y_2, \\dots, y_n\\), Equation (7.27) function \\(\\pi\\), called likelihood function denoted \\(L(\\pi)\\).\nEquation (7.27) often called joint probability function considered function data. However, data assumed fixed Equation (7.27) considered function \\(\\pi\\), called likelihood function.maximum likelihood estimate value \\(\\pi = P(Y = 1)\\) maximizes Equation (7.27). simplicity, common find value \\(\\pi\\) maximizes log likelihood function. Recall value \\(\\pi\\) maximizes likelihood function, \\(L(\\pi)\\), also maximize log-likelihood function, \\(\\ln L(\\pi)\\).\\[\\begin{align}\n\\ln L(\\pi)\n&= \\ln\\!\\bigl(\\pi^{\\sum_{=1}^n k_i}\\,(1 - \\pi)^{\\sum_{=1}^n (1 - k_i)}\\bigr) \\notag \\\\\n&= \\sum_{=1}^n k_i \\ln(\\pi)\\;+\\;\\bigl(n - \\sum_{=1}^n k_i\\bigr)\\ln(1 - \\pi)\n\\tag{7.28}\n\\end{align}\\]\nprinciple maximum likelihood estimation choose value \\(\\pi\\) observed data set likely occur (.e., likelihood function maximized).find maximum value \\(\\ln L(\\pi)\\), take derivative \\(\\ln L(\\pi)\\) set first derivative equal 0:\\[\\begin{align}\n\\frac{d[\\ln L(\\pi)]}{d\\pi}\n&= \\sum_{=1}^n k_i \\,\\frac{1}{\\pi} \\;+\\;\\bigl(n - \\sum_{=1}^n k_i\\bigr)\\,\\frac{-1}{(1 - \\pi)} = 0\n\\tag{7.29}\n\\end{align}\\]*?Calculus requiredThen solve following equivalent equation terms \\(\\pi\\):\\[\\begin{align}\n(1 - \\pi)\\sum_{=1}^n k_i \\;-\\;\\pi\\bigl(n - \\sum_{=1}^n k_i\\bigr) = 0\n\\tag{7.30}\n\\end{align}\\]provides maximum likelihood estimator \\(\\pi = P(Y = 1)\\):\\[\\begin{align}\n\\displaystyle \\hat{\\pi} = \\frac{\\sum_{=1}^n k_i}{n}\n\\tag{7.31}\n\\end{align}\\]example, maximum likelihood estimator well-known frequentist approach estimating \\(\\pi = P(Y = 1)\\). However, always case.\nsecond derivative \\(\\ln L(\\pi)\\) can also calculated show function concave . Thus, \\(\\hat{\\pi}\\) local maximum local minimum. Likelihood functions just one parameter typically one critical value, maximum value. one parameter involved, care needs taken ensure critical values actually maximum likelihood estimates.","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"extended-activity-calculating-maximum-likelihood-estimates","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"Extended Activity: Calculating Maximum Likelihood Estimates","text":"binomial response data follow binomial distribution, log-likelihood function one observation (\\(y_1\\)) \\[\\begin{align}\n\\ln[P(Y_1 = k)]\n&= \\ln\\!\\bigl(\\binom{n_1}{k}\\,\\pi_1^k\\,(1 - \\pi_1)^{n_1 - k}\\bigr) \\notag \\\\\n&= C \\;+\\;k\\ln(\\pi_1)\\;+\\;(n_1 - k)\\ln(1 - \\pi_1)\n\\tag{7.32}\n\\end{align}\\]\n\\(C\\) constant influence \\(\\pi_1\\), can drop \\(C\\) equation without impact maximum likelihood estimate. Assume observed \\(y_1 = 5\\) malignant cells sample \\(n_1 = 12\\). Substituting values Equation (7.32) dropping \\(C\\) simplifies log-likelihood function \\(\\ln[P(Y_1 = 5)] = 5\\ln(\\pi_1) + 7\\ln(1 - \\pi_1).\\)Plot log-likelihood function several values \\(\\pi_1\\) 0 1. Use plot estimate value \\(\\pi_1\\) maximize log-likelihood function.calculus, set derivative log-likelihood function 0 solve \\(\\pi_1\\). maximum likelihood estimate \\(\\pi_1\\)?","code":""},{"path":"logistic-regression-the-space-shuttle-challenger.html","id":"maximum-likelihood-estimator-for-logistic-regression-models","chapter":"14 Logistic Regression: The Space Shuttle Challenger","heading":"14.20.1 Maximum Likelihood Estimator for Logistic Regression Models","text":"previous example address cases logistic regression observations depend one explanatory variables. use maximum likelihood estimation logistic regression, Equation (7.6) see \\[\n\\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}.\n\\]Replacing value \\(\\pi\\) Equation (7.28) gives\\[\\begin{align}\n\\ln L(\\beta_0, \\beta_1)\n&= \\sum_{=1}^n k_i \\ln\\!\\bigl(\\tfrac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\bigr)\n\\;+\\;\\bigl(n - \\sum_{=1}^n k_i\\bigr)\\ln\\!\\Bigl[1 - \\bigl(\\tfrac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\bigr)\\Bigr]\n\\tag{7.33}\n\\end{align}\\]find maximum likelihood estimates \\(\\beta_0\\) \\(\\beta_1\\), take derivative Equation (7.33) respect \\(\\beta_0\\) respect \\(\\beta_1\\). provide two equations two unknowns. However, two equations linear solved directly.\nFinding estimates \\(\\beta_0\\) \\(\\beta_1\\) maximize log-likelihood function iterative technique quickly becomes complex even small data sets typically done hand. Iterative techniques start initial estimates \\(\\beta_0\\) \\(\\beta_1\\). iterative technique Newton–Raphson method repeatedly provides new estimates \\(\\beta_0\\) \\(\\beta_1\\) increase log-likelihood log-likelihood notably change.\\(^{15}\\)","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"survival-analysis-melting-chocolate-chips","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"15 Survival Analysis: Melting Chocolate Chips","text":"Far better approximate answer right question, often vague, exact answer wrong question, can always made precise.\n— John TukeySurvival analysis methods used investigate time target event interest (e.g., death, drug relapse, college graduation) occurs, used variety disciplines medicine, sociology, education. Although survival analysis techniques get amount exposure literature statistical methods regression analysis analysis variance, important consider use whenever response variable interest time event occurs.chapter, opportunity perform simple experiment investigate time required different types chocolate chips melt. set activities related experiment introduce variety methods exploring times target event occurs, also referred survival time--event data. Upon completion activities, able following:Recognize special characteristics survival dataSummarize survival data estimate survival probabilities using Kaplan–Meier estimatorCompute descriptive statistics, including mean percentiles, sample event timesConstruct confidence intervals survival probabilitiesCompare survival experiences different groups subjectsInvestigate periods time subjects low high risk experiencing event interestThe extended activities research project provide opportunities evaluate research articles discussing applications survival analysis, hazard functions, various types incomplete data.","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"investigation-how-long-does-it-take-for-chocolate-chips-to-melt","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"15.1 Investigation: How Long Does It Take for Chocolate Chips to Melt?","text":"enjoy eating chocolate chips, can probably appreciate sweet flavor smooth texture\nmelt mouth. Chocolatiers food scientists well aware material composition\nchocolate affects flavor, texture, duration melting process, thereby resulting “good” \n“bad” tasting experience. individuals continually strive improve manufacturing process, well\nproperties chocolate. developing heat-resistant varieties can withstand higher temperatures,\nwork increase time chocolate melts.2 food scientists chocolate confectioners,\nmelting chocolate can serious business.purpose following investigation explore time required chocolate chips melt. \nperform study, conduct experiment requires students place chip mouth \nwait melt. experiment somewhat restrictive time allowed run study\nlimited. Suppose allow 60 seconds study. possible chocolate chips\nmelt. chips melted time 60 seconds elapsed, partial\ninformation melting times, indicate times incomplete. ’ll see, incomplete\ntimes can create difficulties trying compute simple quantities like average melting time \ntrying estimate proportion chips take longer particular time melt.obtain larger set chip melting times can use activities extended activities\nchapter, class might participate following experiment collect chocolate chip melting\ntimes. addition, able examine differences melting times type chocolate\nchip (white milk chocolate). experiment, place chocolate chip mouth, hold \ntongue roof mouth, record time required completely dissolve\n(.e., melt), without actually biting chip. Although activity may appear rather trivial, meant\nserve simple, yet illustrative, approach generating real time--event data exploring methods\nused investigate survival data. hope examine additional examples real survival data\nlater chapter, able relate features examples back features chip\nmelting time data.melting times different types chips methods learn chapter, able following:Estimate proportion chips remain unmelted beyond specific point timeEstimate average time takes white milk chocolate chips meltDetermine type chip related chip melting experience. example, milk white chocolate chips melt rates time?chocolate chips melted particular time, determine rate melting next “instant” time\nactivity, need time-keeping device every student can clearly see, bag white chocolate chips, bag milk chocolate chips.","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"activity-melting-chocolate-chips","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"Activity: Melting Chocolate Chips","text":"Perform chocolate chip melting activity outlined . sure record chip color, melting time, whether chip completely melted 60 seconds. Combine class data one data file future analysis purposes, name file MeltingChips.\nDevise system randomly assigning student white milk chocolate chip (can done flipping coin, example).\ninstructor gives approval, place white milk chocolate chip mouth record time completely melts. Note group come reasonable consensus clear definition “completely melts” prior activity ensure consistency recorded times.\nCreate data set following manner: Treat study done specified period time (may need experiment, 60 seconds worked well). actual time required chip melt less 60 seconds, actual time complete submit (chip type, actual time, 1). chip melted 60 seconds, regard observation incomplete submit (chip type, 60, 0). Observations chips swallowed prior 60 seconds regarded incomplete well, submit (chip type, actual swallowed time, 0).\nDevise system randomly assigning student white milk chocolate chip (can done flipping coin, example).instructor gives approval, place white milk chocolate chip mouth record time completely melts. Note group come reasonable consensus clear definition “completely melts” prior activity ensure consistency recorded times.Create data set following manner: Treat study done specified period time (may need experiment, 60 seconds worked well). actual time required chip melt less 60 seconds, actual time complete submit (chip type, actual time, 1). chip melted 60 seconds, regard observation incomplete submit (chip type, 60, 0). Observations chips swallowed prior 60 seconds regarded incomplete well, submit (chip type, actual swallowed time, 0).\nclass perform chip melting activity, can use data set MeltingChipsJS\nsupplied authors complete activities extended activities. certain verify \ndata set (MeltingChips MeltingChipsJS) instructor like use following\nactivities.18\n","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"overview-of-survival-analysis-studies-and-data","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"15.2 Overview of Survival Analysis Studies and Data","text":"collection lengths times required chips melt example survival data, also called time--event data failure-time data. Survival data times individuals experience event interest. specific event interest can , example, death, graduation, test completion, individual experiencing event may living, person animal, inanimate, light bulb, computer, chocolate chip.Survival analysis field statistics covering methods techniques examining investigating survival data, used diverse fields including medicine, education, psychology. studies use survival analysis techniques, response variable interest, denoted \\(T\\), time event interest occurs, also called failure time, survival time, time--event random variable. example, time taken chocolate chip melt time--event random variable, recorded melting times observed values \\(T\\). material follows, discuss many ways summarize describe observed values \\(T\\) might get experiment study. Additional examples survival time random variables (related fields parentheses) discussed include:Time drivers blocked traffic honk horn (sociology psychology)Time students graduate college (education)Age first alcoholic drink taken (public health)Time former inmates rearrested (criminology)study involves measuring time target event occurs possesses clearly\ndefined beginning time well meaningful scale measuring time, appropriate use\nsurvival analysis methods techniques. beginning time point individual study\nyet experience event (e.g., date student enrolls post-secondary institution \ninvestigation concerns time college graduation), meaningful scale might seconds,\nminutes, days, weeks, \nTime--event data fundamentally different time series data. Time series data measurements observational units collected different time points. example, time series data may number chips melt 40 seconds, 50 seconds, . Time--event data time durations observational unit experiences target event.\nresponse variable survival analysis study time event interest occurs. Survival analysis methods appropriate data experiments studies possess well-defined event interest, clearly defined beginning time, meaningful scale measuring time.","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"incomplete-event-times-censoring","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"Incomplete Event Times: Censoring","text":"One feature common many survival data sets needs appropriately addressed event times incomplete; .e., partial information time event interest occurs. ’ll refer event times known exactly complete. several reasons observations may incomplete, chapter focus mechanism called right censoring, occurs observation begins defined starting time ends outcome interest observed. chocolate chip data right-censored observations chip melt allotted time student accidentally swallowed chip. example, chip melt 60 seconds right-censored (incomplete) time 60 seconds. types incomplete data discussed extended activities.written reports journals, common display right-censored event time + placed right observed time. example, recorded melting time 60+ seconds indicate chip observed 60 seconds, (completely) melt. Another way record event time assign pair numbers consisting observed value survival time variable, \\(T\\), value censoring status variable, \\(C\\). censoring variable might, example, take value 0 event interest observed value 1 (0–1 coding choice arbitrary, although common). Although formal, method recording survival data particularly relevant times need entered data files. coding scheme adopted -class chip melting activity performed earlier.Incomplete observations can introduce systematic error, also called bias, estimated quantities (like mean median) handled appropriately—example, right-censored observations treated complete removed study. Descriptive statistics survival time, mean, may grossly underestimated right censoring present ignored. Survival analysis methods techniques accommodate data incomplete observations developed, covered sections follow.\nsure read description file containing survival data know value censoring variable corresponds right-censored time value corresponds complete time.","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"activity-melting-chocolate-chips-1","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"Activity: Melting Chocolate Chips","text":"chip melting study, describe event interest, time--event random variable \\(T\\), beginning time, scale measuring time.Examine class data. many melting times complete? many censored?\nSurvival data may contain right-censored observations; , individual may experience event end study, individual may drop study event time observed. event time right censored can displayed + right recorded time represented using pair values include observed recorded time value indicate time censored.","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"the-survival-function","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"15.3 The Survival Function","text":"Now discussed particular features time--event data looked notation terminology, turn methods examining summarizing survival data. primary function used characterize values time--event random variable \\(T\\) survival function \\(S(t)\\), given \\[\\begin{align}\nS(t) &= P(T > t)\n\\notag\n\\end{align}\\]\\(S(t)\\) provides probability randomly selected individual population survive (experience event interest) beyond time \\(t\\). Another interpretation \\(S(t)\\) provides proportion subjects population yet experience event interest time \\(t\\).context chocolate chip melting times, survival function \\(S(t) = P(T > t)\\) provides probability randomly selected chip population chips take longer specified time \\(t\\) melt. , example, \\(S(45) = P(T > 45)\\) gives probability randomly selected chip take longer 45 seconds melt. Equivalently, \\(S(45)\\) provides proportion chips population melted 45 seconds.beginning time, one experienced event, proportion subjects population yet experience target event 100% \\(S(0) = 1\\). time progresses, individuals experience event (e.g., chocolate chips melt, college students graduate, former inmates arrested ), survival function decline toward lower bound 0 (although may actually reach value).\nsurvival function provides probability individual survive beyond given time \\(t\\)—, probability individual experience event interest time \\(t\\). Important: \\(S(t)\\) indicate probability individual experiences event time \\(t\\).","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"the-empirical-survival-function","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"The Empirical Survival Function","text":"determine exact proportion chips take longer \\(t\\) seconds melt, need know melting times entire population chips know exact probability distribution \\(T\\). practice, hardly ever know exact probability distribution \\(T\\). real world, collect (given) sample survival times, need find estimator \\(S(t)\\).\nsimilar done first statistics course. exact value, population mean \\(\\mu\\), estimated using function sample data, \\(\\bar x\\).illustrate calculation various quantities following sections, use small sample\nmelting times (seconds) milk chocolate chips 7 students, maximum time allowed \nexperiment 60 seconds. times displayed Table 9.1. Statistical software used\n-class chip melting activity data, well additional data sets described extended\nactivities.estimate proportion chocolate chips melted 45 seconds, \\(\\hat S(45)\\), simply\ncalculate sample proportion:\\[\\begin{align}\n\\hat S(45) &= \\frac{\\text{number chips melted 45 seconds}}{\\text{total number chips sample}} \\notag \\\\\n&= \\frac{2}{7} \\notag\n\\end{align}\\]observations complete, can generalize calculation time \\(t\\) using estimator \\(S(t)\\) called empirical survival function, denoted \\(\\hat S(t)_{E}\\) given \\[\\begin{align}\n\\hat S(t)_{E}\n&= \\frac{\\text{number individuals yet experience event time }t}{\\text{total number individuals study}} \\notag \\\\\n&= \\frac{\\text{number event times greater }t}{\\text{total number individuals study}}\n\\tag{9.1}\n\\end{align}\\]observations complete, empirical survival function works well. However, estimator may precise data contain censored (.e., incomplete) observations.Now suppose milk chocolate chip melting times displayed Table 9.1 incomplete. Let’s assume students 1 7 withdrew study (accidentally swallowed chips melted), student 3 experienced melted chip end experiment. students 1, 3, 7 censored times, melting times can displayed shown Table 9.2, + denotes right-censored observations.","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"activity-empirical-survival-function","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"Activity: Empirical Survival Function","text":"Use Equation (9.1) 7 milk chocolate melting times Table 9.1 compute \\(\\hat S(25)_{E}\\), \\(\\hat S(30)_{E}\\), \\(\\hat S(40)_{E}\\), \\(\\hat S(60)_{E}\\).melting times provided Table 9.2, use following two approaches calculate estimated probability takes 45 seconds chocolate chip melt, based empirical survival function \\(\\hat S(45)_{E}\\):\nTreat censored times complete (actual observed) times, use Equation (9.1) calculate \\(\\hat S(45)_{E}\\).\nEliminate censored observations, use Equation (9.1) remaining complete observations calculate \\(\\hat S(45)_{E}\\).\nTreat censored times complete (actual observed) times, use Equation (9.1) calculate \\(\\hat S(45)_{E}\\).Eliminate censored observations, use Equation (9.1) remaining complete observations calculate \\(\\hat S(45)_{E}\\).Note different answers obtained Parts () (b) Question 5. treating censored times complete times, assume event times shorter actually , thereby underestimating true probability survival (melting). disregarding censored times, lose information melting times sample (consider extreme case melting times 60+). Treating censored observations complete ignoring bias estimates based remaining complete times.","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"the-kaplanmeier-estimator","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"15.4 The Kaplan–Meier Estimator","text":"data set contains incomplete observations, best estimator survival function Kaplan–Meier estimator, \\(\\hat S(t)_{\\text{KM}}\\). estimator typically calculated statistical software, section describe logic behind Kaplan–Meier estimator put together.first step creating Kaplan-Meier estimator establish series time intervals. Order \ncomplete event times smallest largest label smallest complete time \\(t_1\\), second smallest\n\\(t_2\\), . denote number distinct complete event times m, m less \nequal n, total number observed event times (complete incomplete).complete times, \\(t_1\\) \\(t_m\\), used define intervals beginning one complete event time \nending just prior next complete event time, minor modifications first last intervals\noutlined :convention, 0th interval begins time \\(t_0 = 0\\) ends just prior time first event occurs, time \\(t_1\\). interval given \\([0,\\,t_1)\\).next interval begins complete time \\(t_i\\) ends just prior next complete event time \\(t_{+1}\\). Time intervals form \\([t_i,\\,t_{+1})\\) created \\(= 1,2,\\dots,m-1\\).largest observed event time censored, time denoted \\(t_m\\) interval extends \\(t_m\\) open right; .e., interval given \\([t_m,\\,t_n)\\). largest observed event time complete, last interval technically interval just consists single point; , interval given \\([t_m,\\,t_m]\\).","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"activity-time-intervals-for-the-chip-melting-times","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"Activity: Time Intervals for the Chip Melting Times","text":"Consider chocolate chip melting time data Table 9.2. \\(m\\)? List \\(t_1\\) \\(t_m\\) chip melting times.first two intervals chocolate chip melting times \\([0,25)\\) \\([25,30)\\). Write remaining intervals. Notice incomplete times, 30+ 35+, ignored creating intervals.Determine \\(d_i\\), number melted chips interval, \\(n_i\\), number chips risk melting interval (chips complete censored times yet occurred), \\(= 0,1,2,3,4\\).Table 9.3 displays quantities required compute Kaplan–Meier estimates. time intervals, number events interest (\\(d_i\\)) number risk (\\(n_i\\)) calculated, three calculations still needed interval: \\(\\hat p_i\\), \\(1 - \\hat p_i\\), \\(\\hat S(t_i)_{\\text{KM}}\\).time intervals appropriately defined, estimate \\(\\hat p_i\\), conditional probability experiencing event \\(\\)th time interval, given event occurred start interval. , compute\\[\\begin{align}\n\\hat p_i &= \\frac{d_i}{n_i}\n\\notag\n\\end{align}\\]\\(d_i\\) number subjects experienced target event interval \\(\\) \\(n_i\\) total number subjects (complete censored times) eligible (risk) experience target event beginning \\(\\)th time interval.Now \\(\\hat p_i\\) probability individual experiencing event \\(\\)th time interval, given \nindividual experienced event previous time intervals, \\(1 - \\hat p_i\\)\nprobability experiencing event (.e., surviving) ith time interval, given individual experienced event prior \\(\\)th time interval.example, estimate conditional probability chip melt 25th second\n30th second, given remained unmelted 25th second, given \\[\\begin{align}\n1 - \\hat p_1 &= 1 - \\frac{d_1}{n_1} \\notag \\\\\n&= 1 - \\frac{1}{7} \\notag \\\\\n&= \\frac{6}{7} \\notag\n\\end{align}\\], \\(6/7\\), 86%, chips melted just prior 25th second remain unmelted (survive) 25th 30th second.","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"activity-estimated-conditional-melting-probabilities","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"Activity: Estimated Conditional Melting Probabilities","text":"Use chocolate chip data Table 9.2 answer following questions:value \\(\\hat p_1\\)? Interpret value.\\(\\hat p_1\\) estimate conditional probability chip melt 25th second 30th second, given remained unmelted 25th second. Show 14% chips melted just prior 25th second melt 25th 30th second.Calculate remaining estimated conditional probabilities \\(\\hat p_3\\) \\(\\hat p_4\\). Place values appropriate cells Table 9.3 interpret values.Calculate remaining estimated conditional probabilities \\(1 - \\hat p_3\\) \\(1 - \\hat p_4\\). Place values appropriate cells Table 9.3 interpret values.final step constructing Kaplan–Meier estimated survival probabilities multiply together conditional probability surviving \\(\\)th time interval get unconditional probability surviving \\(\\)th time interval.example, Kaplan–Meier estimate probability chip remain unmelted 25th second given \\[\\begin{align}\n\\hat S(25)_{\\mathrm{KM}} = (1 - \\hat p_0)\\,(1 - \\hat p_1) &= (1)\\,\\bigl(1 - d_1/n_1\\bigr) \\notag \\\\\n&= 1 - \\tfrac{1}{7} \\notag \\\\\n&= \\tfrac{6}{7} \\notag\n\\end{align}\\]Therefore, 6/7, 86%, chips remain unmelted (survive) beyond 25th second.\nKaplan–Meier estimator provides proportions subjects sample survive beyond given time. compute Kaplan–Meier estimator data set \\(n\\) individuals, define following quantities:\\(m\\): number distinct uncensored event times, \\(m \\le n\\). distinct mean two identical times contribute determine \\(m\\).\\(t_1, t_2, \\dots, t_m\\): ordered complete times (.e., times event interest actually occurred), ordered smallest largest. convention, \\(t_0 = 0\\).\\(n_i\\): number risk experiencing event time \\(t_i\\) (.e., just prior start time interval \\([t_i,\\,t_{+1})\\)), \\(= 0,1,\\dots,m-1\\).\\(d_i\\): number experiencing event time \\(t_i\\) (.e., time interval \\([t_i,\\,t_{+1})\\)), \\(= 0,1,\\dots,m-1\\).Kaplan–Meier estimator survival function given \\[\\begin{align}\n\\hat S(t)_{\\mathrm{KM}} &= \\prod_{t_i \\le t} \\bigl(1 - d_i/n_i\\bigr)\n\\tag{9.2}\n\\end{align}\\]\\(\\prod\\) symbol taking products terms \\((1 - d_i/n_i)\\) \\(\\) complete event times \\(t_1,\\dots,t_m\\) less equal time interest \\(t\\). Also, \\(\\hat S(t)_{\\mathrm{KM}} = 1\\) times \\(t < t_1\\).\n\nKaplan–Meier estimator derived using multiplication rule introductory probability. Let \\(A_i\\) defined event occurring interval \\(\\). , example,\n\\[\\begin{align}\n\\hat S(25)_{\\mathrm{KM}} &= (1 - \\hat p_1)\\,(1 - \\hat p_0) \\\\\n&= P(A_1 \\mid A_0)\\,P(A_0)\n\\end{align}\\]\n\n\\[\\begin{align}\n\\hat S(30)_{\\mathrm{KM}} &= (1 - \\hat p_2)\\,(1 - \\hat p_1)\\,(1 - \\hat p_0) \\\\\n&= P(A_2 \\mid A_1)\\,P(A_1 \\mid A_0)\\,P(A_0)\n\\end{align}\\]\n","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"activity-kaplanmeier-estimates","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"Activity: Kaplan–Meier Estimates","text":"Refer entries Table 9.3 answer following questions.\n13. Use remaining chocolate chip melting times complete Table 9.3.\n14. estimate \\(S(45)\\) Table 9.3? , proportion chips sample melted 45 seconds?\n15. Use entries Table 9.3 estimate proportion chips melted 35 seconds.\n16. Use entries Table 9.3 estimate proportion chips melted 50 seconds.\n17. Assume censoring present melting times (see entries Table 9.1). Estimate \\(S(25)\\), \\(S(30)\\), \\(S(45)\\), \\(S(55)\\) using empirical survival function Kaplan–Meier estimator, compare answers. answers suggest Kaplan–Meier estimator censoring present?","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"graphing-the-kaplanmeier-curve","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"Graphing the Kaplan–Meier Curve","text":"survival probabilities estimated, graph Kaplan–Meier curve can constructed display relationship time estimated probability surviving. Kaplan–Meier curve approximation true survival curve, graphical representation \\(S(t)\\). values \\(\\hat S(t)_{\\mathrm{KM}}\\) plotted complete event times \\(t_1, t_2, \\dots, t_m\\). Figure 9.1 shows Kaplan–Meier curve chocolate chip melting times displayed Table 9.2.Notice value \\(\\hat S(30)_{\\mathrm{KM}}\\) remains across time interval. Table 9.3, see , example, \\(\\hat S(30)_{\\mathrm{KM}} = 5/7\\); .e., proportion chips sample melted 30 seconds 5/7, 71%. value remains constant time \\(t\\) interval \\([30,45)\\). Figure 9.1 can observe Kaplan–Meier curve decreasing series steps, drops occurring complete event time \\(t_i\\). type plot called step function looks like series steps. height step corresponds value \\(\\hat S(t_i)_{\\mathrm{KM}}\\) \\(t\\) inside \\([t_i,\\,t_{+1})\\), \\(= 0,\\dots,m-1\\) (convention \\(t_0 = 0\\)).can see estimated probability randomly selected chip remains unmelted decreases time increases , stated another way, proportion unmelted chips decreases time. Another feature Figure 9.1 proportion chips remain unmelted 60 seconds nonzero, last step Kaplan–Meier curve extends right 60 seconds.’ll notice box right graph, labeled “Table Statistics,” contains mean \nmedian survival times sample, well interquartile range (IQR). ’ll discuss descriptive\nstatistics Section 9.4.[[[Figure 9.1]]]","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"activity-kaplanmeier-curves","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"Activity: Kaplan–Meier Curves","text":"Compare values \\(\\hat S(t)_{\\mathrm{KM}}\\) Table 9.3 Figure 9.1. Kaplan–Meier curve Figure 9.1 change largest observed melting time censored?Use technology instructions provided CD construct Kaplan–Meier curve white milk chocolate chips chocolate chip melting activity. Compare two curves. melting proportions different types chips appear similar time? ’ll discuss formal methods comparing survival curves populations white milk chocolate chips Section 9.6.","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"additional-chip-melting-experiences","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"Additional Chip Melting Experiences","text":"class data, observed two possible chip melting experiences time, corresponding milk white chocolate chips. Figure 9.2 presents four additional Kaplan–Meier curves corresponding different chip melting experiences.Kaplan–Meier curve corresponding first scenario, displayed Part (), reveals chips melt 49th second. time, say \\(t\\), 49th 87th second, estimated proportion chocolate chips remain unmelted decreases somewhat slowly, indicating possible resistance chocolate chips melt. period time 87th 96th second, estimated proportion chips remain unmelted decreases rapidly, suggesting chocolate chips susceptible melting. time beyond 96th second, estimated proportion chips remain unmelted remains constant 20%. means chips melted end observation period (.e., melting times censored); however, note 20% necessarily refer proportion chips censored melting times.Kaplan–Meier curve corresponding second melting scenario, displayed Part (b), shows chips begin completely melting 30 seconds, proportion chips remain unmelted beyond time 30 65 seconds decreases rapidly, indicating period time chips melting quite quickly. estimated probability chip remains unmelted beyond 65th second 0, since chips melted point.Part (c) displays Kaplan–Meier curve third melting scenario, can observe chips begin completely melting 35 seconds. next 5 seconds, estimated proportion chips remain unmelted declines rapidly (approximately 50% chips melt 35th 40th second). extended period time 40th 85th second, melting stabilizes, proportions chips survive identical. melting resumes next 15 seconds. time beyond 100th second, estimated 9% chips melted.[[[Figure 9.2]]]","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"activity-chip-melting-experiences","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"Activity: Chip Melting Experiences","text":"Discuss fourth chip melting scenario, illustrated Kaplan–Meier curve Part (d) Figure 9.2.","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"descriptive-statistics-for-survival-data","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"15.5 Descriptive Statistics for Survival Data","text":"far focused using Kaplan–Meier curve estimate proportion chips remain unmelted 25 seconds, 30 seconds, . survival function useful, may also interested finding estimate mean median time--event population individuals. example, typical time required chocolate chip melt? long take half chips melt?may seem odd spent great deal time discussing survival probabilities estimates discussing descriptive statistics. primary reason ordering topics survival analysis descriptive measures survival data require Kaplan–Meier estimated survival probabilities.may tempting use calculations means medians learned previous statistics course. However, consider mean time chocolate chip melts. use sample mean \\(\\bar x\\) estimate quantity, run problem estimated chip melting rates—censored observations treated? example, censored observations treated complete, resulting estimate mean melting time underestimate true average melting time. need incorporate Kaplan–Meier estimated survival probabilities deal censored observations.","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"estimating-the-mean-survival-time","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"Estimating the Mean Survival Time","text":"estimated mean survival time total area Kaplan–Meier curve. can found adding areas bars formed height curve two adjacent complete survival times, slight adjustment largest observed event time censored. Consider rectangular bars displayed Figure 9.3. area bar found taking width interval \\(t_{+1} - t_i\\) (duration time period) multiplying estimated probability surviving interval \\(\\hat S(t_i)_{\\mathrm{KM}}\\) (height bar). largest observed event time censored (.e., \\(t_n > t_m\\)), last interval extends \\(t_m\\) \\(t_n\\). Hence, following expressions computing estimated mean survival time:largest observed event time complete (.e., \\(t_n = t_m\\)), estimator mean survival time, denoted \\(\\hat \\mu\\), given \\[\\begin{align}\n\\hat \\mu\n&= \\sum_{=0}^{m-1} \\hat S(t_i)_{\\mathrm{KM}} \\,\\bigl(t_{+1} - t_i\\bigr) \\notag \\\\\n&= \\hat S(t_0)_{\\mathrm{KM}}(t_1 - t_0) + \\hat S(t_1)_{\\mathrm{KM}}(t_2 - t_1) + \\dots + \\hat S(t_{m-1})_{\\mathrm{KM}}(t_m - t_{m-1})\n\\tag{9.3}\n\\end{align}\\]largest event time \\(t_n\\) censored (.e., \\(t_n > t_m\\)), estimator mean survival time given \\[\\begin{align}\n\\hat \\mu\n&= \\sum_{=0}^{m-1} \\hat S(t_i)_{\\mathrm{KM}} \\,(t_{+1} - t_i) + \\hat S(t_m)_{\\mathrm{KM}}(t_n - t_m)\n\\tag{9.4}\n\\end{align}\\]Equations (9.3) (9.4) can thought weighted average time--event data, Kaplan–Meier curve \\(\\hat S(t_i)_{\\mathrm{KM}}\\) acts weights (probabilities) time--event intervals. term summation Equations (9.3) (9.4) represents area rectangle formed width \\(\\)th time interval \\(t_{+1} - t_i\\) height Kaplan–Meier curve \\(\\hat S(t_i)_{\\mathrm{KM}}\\). example, width left-rectangle Figure 9.3 outlined vertical dotted line 25 seconds, height rectangle 1. first term sum Equation (9.3) \\(\\hat S(t_0)_{\\mathrm{KM}}(25 - 0) = (1)(25) = 25\\).[[[Figure 9.3]]]\nmean survival time estimated using time intervals Kaplan–Meier estimated probabilities.","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"activity-estimated-mean-chip-melting-time","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"Activity: Estimated Mean Chip Melting Time","text":"Examine Figure 9.3. Visually estimate mean survival time (.e., estimated average time taken chocolate chips sample melt) computing rough approximation area Kaplan–Meier curve.sample chocolate chip melting times Table 9.2, equation, (9.3) (9.4), appropriate estimating mean survival time chips? Based answer, calculate estimated mean using quantities Table 9.3.","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"estimating-percentiles-of-the-survival-time-distribution","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"15.6 Estimating Percentiles of the Survival Time Distribution","text":"Suppose want time 50% chocolate chips melted. quantity called median survival time (, precisely, estimated median survival time, since working sample survival times). times certain percentage subjects experienced event interest known percentiles survival time distribution.\\(p\\)th percentile distribution survival time random variable \\(T\\) defined time \\(p\\%\\) subjects population experienced target event. estimate \\(p\\)th percentile, denoted \\(\\hat t_{(p)}\\), defined smallest complete event time sample least \\(p\\%\\) subjects sample experienced event interest \\(\\hat t_{(p)}\\), \\((100 - p)\\%\\) subjects sample experience event \\(\\hat t_{(p)}\\). Depending number distinct complete event times, value \\(\\hat t_{(p)}\\) can found either inspecting Kaplan–Meier curve finding solution following equation:\\[\\begin{align}\n\\hat t_{(p)} &= \\text{smallest complete event time }t_i\\text{ sample } \\hat S(t_i)_{\\mathrm{KM}} \\le 1 - \\frac{p}{100}\n\\tag{9.5}\n\\end{align}\\]example, find estimate 70th percentile chocolate chip melting times Figure 9.1, find smallest complete event time \\(t_i\\) least 70% chips melted: Since \\(\\hat S(t_i)_{\\mathrm{KM}} \\le 1 - 0.7 = 0.3\\), draw horizontal line \\(\\hat S(t) = 0.3\\), eventually reach vertical step occurs \\(t_4 = 55\\). Since \\(t_4 = 55\\) smallest complete event time satisfies \\(\\hat S(t_i)_{\\mathrm{KM}} \\le 0.3\\), \\(\\hat t_{(70)} = 55\\). See Figure 9.4.\nPercentile estimates survival time distribution found using Kaplan–Meier estimated probabilities.","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"activity-estimated-median-chip-melting-time","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"Activity: Estimated Median Chip Melting Time","text":"chocolate chip melting time data Table 9.2, calculate estimated median survival time \\(\\hat t_{(50)}\\). Since right-censored data typically right skewed, median survival time usually preferred mean.Refer back Kaplan–Meier curve displayed Figure 9.1. mean median survival times sample chip melting times provided “Table Statistics” box graph. interquartile range (IQR) also provided. Recall IQR third quartile (75th percentile) minus first quartile (25th percentile). Verify hand IQR sample times 25 seconds.Use technology instructions provided CD determine estimated mean median survival times separately milk white chocolate chip melting time data class activity. Discuss differences observe two descriptive measures two types chips.[[[Figure 9.4]]]\nBased definition estimated percentile Equation (9.5), possible particular estimated percentiles exist. \\(p\\)th percentile exist (software may return NA) exist complete time \\(t_p\\) \\(\\hat S(t_p)_{\\mathrm{KM}} \\le 1 - \\tfrac{p}{100}\\). example, MeltingChipsJS data set, 25% milk chocolate chips melted end study (75 seconds). Thus, estimate 75th percentile IQR calculated.\n","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"confidence-intervals-for-survival-probabilities","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"15.7 Confidence Intervals for Survival Probabilities","text":"Just like sample statistic (e.g., sample mean \\(\\bar x\\)), estimates survival probabilities subject sampling variability. example, different samples chocolate chips (/students) likely lead different melting times, lead different Kaplan–Meier estimates. Although point estimate, \\(\\hat S(25)_{\\mathrm{KM}}\\), useful descriptive purposes, may also want construct range possible values estimated survival probability can take—.e., range can reasonably sure contains true survival probability \\(S(25)\\). words, want construct confidence interval true survival probability \\(S(t)\\) time \\(t = 25\\) seconds.first course statistics, saw confidence interval population parameter (e.g., population mean \\(\\mu\\)) following form:\\[\\begin{align}\n\\text{point estimate} \\;\\pm\\; \\text{critical value}\\;\\times\\;\\text{standard error estimate}\n\\tag{9.6}\n\\end{align}\\]point estimate single estimate parameter (\\(\\bar x\\) \\(\\mu\\)), critical value taken reference distribution like standard normal distribution \\(t\\)-distribution, standard error estimate measure variability point estimate. expression confidence interval \\(S(t)\\) fixed time \\(t\\) similar form.know sampling distribution \\(\\bar x\\) approximately normal large sample sizes. similar result can stated concerning sampling distribution \\(\\hat S(t)_{\\mathrm{KM}}\\) fixed \\(t\\). advanced theory tells us , larger sample sizes, sampling distribution \\(\\hat S(t)_{\\mathrm{KM}}\\) fixed time \\(t\\) approximately normal. fact allows us use critical values standard normal distribution.Using critical value standard normal distribution standard error \\(\\hat S(t)_{\\mathrm{KM}}\\), can put together confidence interval \\(S(t)\\) fixed time \\(t\\). usual procedure construct confidence interval \\(S(t)\\) complete event times \\(t_1,\\dots,t_m\\).100\\((1-\\alpha)\\%\\) confidence interval \\(S(t)\\) fixed time \\(t\\) given \\[\\begin{align}\n\\hat S(t)_{\\mathrm{KM}} \\;\\pm\\; Z_{\\alpha/2}\\,\\mathrm{se}\\bigl(\\hat S(t)_{\\mathrm{KM}}\\bigr)\n\\tag{9.7}\n\\end{align}\\]\\(Z_{\\alpha/2}\\) critical value standard normal distribution \\(\\alpha/2\\) area curve right (.e., corresponding confidence level \\(100(1-\\alpha)\\%\\)) \\(\\mathrm{se}\\bigl(\\hat S(t)_{\\mathrm{KM}}\\bigr)\\) standard error Kaplan–Meier estimator time \\(t\\), discussed following section.","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"the-standard-error-of-the-kaplanmeier-estimator","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"The Standard Error of the Kaplan–Meier Estimator","text":"variability values \\(\\hat S(t)_{\\mathrm{KM}}\\) fixed time \\(t\\) interval \\([t_{-1},\\,t_i)\\) measured estimated variance Kaplan–Meier estimator, denoted \\(\\widehat{\\mathrm{Var}}\\bigl(\\hat S(t)_{\\mathrm{KM}}\\bigr)\\) calculated using expression\\[\\begin{align}\n\\widehat{\\mathrm{Var}}\\bigl(\\hat S(t)_{\\mathrm{KM}}\\bigr)\n&= \\bigl(\\hat S(t)_{\\mathrm{KM}}\\bigr)^2 \\;\\biggl(\\sum_{t_i \\le t}\\frac{d_i}{n_i(n_i - d_i)}\\biggr)\n\\tag{9.8}\n\\end{align}\\]\\(d_i\\) number subjects experienced event interest time interval \\([t_{-1},\\,t_i)\\) \\(n_i\\) number risk complete event time \\(t_i\\).\\(^3\\)standard error Kaplan–Meier estimator time \\(t\\), denoted \\(\\mathrm{se}\\bigl(\\hat S(t)_{\\mathrm{KM}}\\bigr)\\), square root \\(\\widehat{\\mathrm{Var}}\\bigl(\\hat S(t)_{\\mathrm{KM}}\\bigr)\\):\\[\\begin{align}\n\\mathrm{se}\\bigl(\\hat S(t)_{\\mathrm{KM}}\\bigr)\n&= \\sqrt{\\widehat{\\mathrm{Var}}\\bigl(\\hat S(t)_{\\mathrm{KM}}\\bigr)}\n\\tag{9.9}\n\\end{align}\\]see, construct confidence intervals \\(S(t)\\), estimated variance \\(\\widehat{\\mathrm{Var}}\\bigl(\\hat S(t)_{\\mathrm{KM}}\\bigr)\\) [standard error \\(\\mathrm{se}\\bigl(\\hat S(t)_{\\mathrm{KM}}\\bigr)\\)] need calculated complete event time \\(t_1,\\dots,t_m\\); , need calculate\\[\\begin{align}\n\\widehat{\\mathrm{Var}}\\bigl(\\hat S(t_1)_{\\mathrm{KM}}\\bigr),\\;\\widehat{\\mathrm{Var}}\\bigl(\\hat S(t_2)_{\\mathrm{KM}}\\bigr),\\;\\dots,\\;\\widehat{\\mathrm{Var}}\\bigl(\\hat S(t_m)_{\\mathrm{KM}}\\bigr).\\notag\n\\end{align}\\]example, Kaplan–Meier estimated survival probability complete time \\(t=25\\) data Table 9.2 \\(\\hat S(25)_{\\mathrm{KM}} = 6/7\\). Using Equation (9.8) quantities Table 9.3, find variance estimate \\(\\widehat{\\mathrm{Var}}\\bigl(\\hat S(25)_{\\mathrm{KM}}\\bigr)\\) time \\(t=25\\) given \\[\\begin{align}\n\\widehat{\\mathrm{Var}}\\bigl(\\hat S(25)_{\\mathrm{KM}}\\bigr)\n&= \\bigl(\\hat S(25)_{\\mathrm{KM}}\\bigr)^2 \\;\\biggl(\\sum_{t_i \\le 25}\\frac{d_i}{n_i(n_i - d_i)}\\biggr) \\\\\n&= \\Bigl(\\tfrac{6}{7}\\Bigr)^2 \\;\\biggl(\\frac{0}{7(7-0)} + \\frac{1}{7(7-1)}\\biggr) \\\\\n&= 0.0175\n\\notag\n\\end{align}\\]standard error \\(\\hat S(25)_{\\mathrm{KM}}\\) time \\(t=25\\) \\(\\mathrm{se}\\bigl(\\hat S(25)_{\\mathrm{KM}}\\bigr) = \\sqrt{0.0175} = 0.1323\\).computed standard error, can use Formula (9.7) construct confidence interval survival probability. example, 95% confidence interval \\(S(25)\\)—probability randomly selected milk chocolate chip takes longer 25 seconds melt—calculated \\[\\begin{align}\n\\hat S(25)_{\\mathrm{KM}} \\;\\pm\\; Z_{0.025}\\,\\mathrm{se}\\bigl(\\hat S(25)_{\\mathrm{KM}}\\bigr)\n\\quad=\\quad\n\\frac{6}{7}\\;\\pm\\;1.96\\,(0.1323)\n\\quad=\\quad\n(0.60,\\;1.12).\n\\notag\n\\end{align}\\]Observe upper limit confidence interval \\(S(25)\\) exceeds 1. Since confidence interval given Formula (9.7) survival probability, interval limits technically fall outside range [0,1]. However, possible obtain lower limit less 0 /upper limit greater 1, especially smaller sample sizes. drawback using Formula (9.7) construct confidence intervals. confronted situation (case ) interval limit(s) () outside range [0,1], common practice truncate limit(s) appropriate minimum value 0 maximum value 1.interpretation confidence interval limits survival probabilities similar standard interpretation confidence intervals parameters interest (think back confidence intervals population mean proportion). chip melting times provided Table 9.2, can state 95% confidence probability chocolate chip take longer 25 seconds melt 0.60 1.00. , equivalently, 95% confident true proportion chips melted 25 seconds 60% 100%.Kaplan–Meier curve associated 95% confidence intervals chocolate chip melting time data displayed Figure 9.5. Observe times upper limit constrained equal 1 lower limit constrained equal 0.[[[Figure 9.5]]]\nalternative formulas computing confidence intervals survival probabilities constrain limits lie 0 1, ’ll leave interested readers investigate .\nConfidence intervals provide range values can reasonably sure contain true survival probabilities.","code":""},{"path":"survival-analysis-melting-chocolate-chips.html","id":"activity-standard-errors-and-confidence-intervals-for-survival-probabilities","chapter":"15 Survival Analysis: Melting Chocolate Chips","heading":"Activity: Standard Errors and Confidence Intervals for Survival Probabilities","text":"Use estimated survival probabilities entries Table 9.3 answer Questions 26 28, use MeltingChips MeltingChipsJS answer Questions 29 31.\n26. Provide brief explanation estimated variance \\(\\hat S(0)_{\\mathrm{KM}}\\), hence standard error \\(\\hat S(0)_{\\mathrm{KM}}\\), equal 0.\n27. Find remaining standard errors \\(\\hat S(t)_{\\mathrm{KM}}\\) times \\(t = 30\\), \\(t = 45\\), \\(t = 55\\) chocolate chip melting time data.\n28. Using answers appropriate entries Table 9.3, construct 95% confidence intervals survival probabilities \\(S(t)\\) remaining complete melting times, interpret values.\n29. Use software construct 95% confidence intervals survival probabilities milk chocolate chip melting time data class activity. Graph Kaplan–Meier curve confidence intervals \\(S(t)\\).\n30. Based width confidence intervals (upper limit minus lower limit), useful think intervals providing estimates true melting times? improve usefulness intervals? (Hint: Refer back Equation (9.8). change make intervals narrower?)\n31. graph, plot Kaplan–Meier estimates 95% confidence intervals white milk chocolate chip melting time data class activity. Explain whether two types chocolate appear different survival functions.\nconfidence interval \\(S(t)\\) valid single fixed time inference made. reason, confidence intervals sometimes referred pointwise intervals. interval constructed using Formula (9.7) relevant \\(t\\) within \\(\\)th interval. Therefore, correct state 95% confident entire true survival curve \\(S(t)\\) falls confidence bands Figure 9.5. want confidence band entire survival function \\(S(t)\\) within can guarantee specified level confidence entire curve falls, alternative confidence bands need computed.","code":""}]
