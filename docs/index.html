<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 1 An Introduction to Nonparametric Methods: Schistosomiasis | A Book Chapter Example</title>
<meta name="author" content="Your Name">
<meta name="generator" content="bookdown 0.43 with bs4_book()">
<meta property="og:title" content="Chapter 1 An Introduction to Nonparametric Methods: Schistosomiasis | A Book Chapter Example">
<meta property="og:type" content="book">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 1 An Introduction to Nonparametric Methods: Schistosomiasis | A Book Chapter Example">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
          margin-bottom: 0em;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
<meta name="description" content="A book created with bookdown.">
<meta property="og:description" content="A book created with bookdown.">
<meta name="twitter:description" content="A book created with bookdown.">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Book Chapter Example</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled"><li><a class="active" href="index.html"><span class="header-section-number">Chapter 1</span> An Introduction to Nonparametric Methods: Schistosomiasis</a></li></ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/rstudio/bookdown-demo">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><!--bookdown:title:end--><!--bookdown:title:start--><div id="an-introduction-to-nonparametric-methods-schistosomiasis" class="section level1" number="1">
<h1>
<span class="header-section-number">1</span> An Introduction to Nonparametric Methods: Schistosomiasis<a class="anchor" aria-label="anchor" href="#an-introduction-to-nonparametric-methods-schistosomiasis"><i class="fas fa-link"></i></a>
</h1>
<p><em>Using statistics is no substitute for thinking about the problem</em>
-Douglas Montgomery<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Douglas Montgomery, Design and Analysis of Experiments, Fifth edition, Wiley, 2003, page 21.&lt;/p&gt;"><sup>1</sup></a></p>
<p>Randomization tests, permutation tests, and bootstrap methods are quickly gaining in popularity as methods for conduct statistical inference. Why? These nonparametric methods require fewer assumptions and provide results that are often more accurate than those from traditional techniques using well-known distributions (such as the normal, t, or F distribution). These methods are based on computer simulations instead of distributional assumptions and thus are particularly useful when the sample data are skewed or if the sample size is small. In addition, nonparametric methods can be extended to other parameters of interest, such as the median or standard deviation, while the well known parametric methods described in introductory statistics courses are often restricted to just inference for the population mean.</p>
<div class="line-block">    We begin this chapter by comparing two treatments for a potentially deadly disease called Schistosomiasis (shis-tuh-soh-mahy-uh-sis). We illustrate the basic concepts behind nonparametric methods by using randomization tests to:</div>
<ul>
<li>Provide an intuitive description of statistical inference.</li>
<li>Conduct a randomization test by hand</li>
<li>Use software to conduct a randomization test</li>
<li>Compare one-sided and two-sided hypothesis tests</li>
<li>Making connections between randomization tests and conventional terminology</li>
</ul>
<p>After working through the schistosomiasis investigation, you will have the opportunity to
analyze several other data sets using randomization tests, permutation tests, bootstrap methods,
and rank-based nonparametric tests.</p>
<div id="investigation-can-a-new-drug-reduce-the-spread-of-schistosomiasis" class="section level2" number="1.1">
<h2>
<span class="header-section-number">1.1</span> <strong>Investigation: Can a New Drug Reduce the Spread of Schistosomiasis?</strong><a class="anchor" aria-label="anchor" href="#investigation-can-a-new-drug-reduce-the-spread-of-schistosomiasis"><i class="fas fa-link"></i></a>
</h2>
<p>Schistosomiasis is a disease occurring in humans caused by parasitic flatworms called schistosomes (skis’-tuhsohms).
Schistosomiasis affects about 200 million people worldwide and is a serious problem in sub-Saharan
Africa, South America, China, and Southeast Asia. The disease can cause death, but more commonly results
in chronic and debilitating symptoms, arising primarily from the body’s immune reaction to parasite eggs
lodged in the liver, spleen, and intestines.</p>
<div class="line-block"> Currently there is one drug, praziquantel (prā’zĭ-kwän’těl’), in common use for treatment of schistosomiasis; it is cheap and effective. However many organizations are worried about relying on a single drug to treat a serious disease which affects so many people worldwide. Drug resistance may have prompted a 1990s outbreak in Senegal, where cure rates were low. In 2007, several researchers published work involving a promising drug called K11777 that, in theory, might also treat schistosomiasis.</div>
<div class="line-block"> In this chapter, we will analyze data from this study where the researchers wanted to find out whether K11777 helps to stop schistosome worms from growing. In one phase of the study, 10 female laboratory mice and 10 male laboratory mice were deliberately infected with the schistosome parasite. Seven days after being infected with schistosomiasis, each mouse was given injections every day for 28 days. Within each sex, 5 mice were randomly assigned to a treatment of K11777 whereas the other 5 mice formed a control group injected with an equal volume of plain water. At day 49, the researchers euthanized the mice and measured both the number of eggs and the numbers of worms in the mice livers. Both numbers were expected to be lower if the drug was effective.</div>
<div class="line-block">Table 1.1 gives the worm count for each mouse. An individual value plot of the data is shown in Figure 1.1. Notice that the treatment group has fewer worms than the control group for both females and males.</div>
<div class="figure">
<span style="display:block;" id="fig:graph1"></span>
<img src="index_files/figure-html/graph1-1.png" alt="Individual value plot of the worm count data" width="672"><p class="caption">
Figure 1.1: Individual value plot of the worm count data
</p>
</div>
<div style="page-break-after: always;"></div>
<blockquote>
<p><strong>NOTE</strong>
There is a difference between individual value plots and dotplots. In dotplots (such as Figures 1.3 and
1.4 shown later in this chapter), each observation is represented by a dot along a number line (x-axis).
When values are close or the same, the dots are stacked. Dotplots can be used in place of histograms
when the sample size is small. Individual value plots, as shown in Figure 1.1, are used to simultaneously
display each observation for multiple groups. They can be used instead of boxplots to identify outliers and distribution shape, especially when there are relatively few observations.</p>
</blockquote>
</div>
<div id="activity-describing-the-data" class="section level2 unnumbered">
<h2>Activity: <em>Describing the Data</em><a class="anchor" aria-label="anchor" href="#activity-describing-the-data"><i class="fas fa-link"></i></a>
</h2>
<blockquote>
<ol style="list-style-type: decimal">
<li>Use Figure 1.1 to visually compare the number of worms for the treatment and control groups for both
the male and the female mice. Does each of the four groups appear to have a similar center and a similar
spread? Are there any outliers (extreme observations that don’t seem to fit with the rest of the data)?</li>
<li>Calculate appropriate summary statistics (e.g., the median, mean, standard deviation, and range) for
each of the four groups. For the female mice, calculate the difference between the treatment and control
group means. Do the same for the male mice.</li>
</ol>
</blockquote>
<div class="line-block">The descriptive analysis in Questions 1 and 2 points to a positive treatment effect: K11777 appears to have</div>
<p>reduced the number of parasitic worms in this sample. But descriptive analysis is usually only the first step
in ascertaining whether an effect is real; we often conduct a significance test or create a confidence interval
to determine if chance alone could explain the effect.</p>
<div class="line-block">Most introductory statistics courses focus on hypothesis tests that involve using a normal, t-, chi-square or F-distribution to calculate the p-value. These tests are often based on the central limit theorem. In the</div>
<p>schistosomiasis study, there are only five observations in each group. This is a much smaller sample size
than is recommended for the central limit theorem, especially given that Figure 1.1 indicates that the data
may not be normally distributed. Since we cannot be confident that the sample averages are normally distributed,
we will use a distribution-free test, also called a nonparametric test. Such tests do not require
the distribution of our sample statistic to have any specific form and are often useful in studies with very
small sample sizes.</p>
<p>
For any population with mean m and finite standard deviation s, the central limit theorem states that
the sample mean x from an independent and identically distributed sample tends to follow the normal
distribution if the sample size is large enough. The mean of x is the same as the population mean, m, while
the standard deviation of x is s/1n, where n is the sample size.</p>
<div class="line-block">We will use a form of nonparametric statistical inference known as a randomization hypothesis test to analyze the data from the schistosomiasis study. <strong>Randomization hypothesis</strong> tests are significance tests that simulate the random allocation of units to treatments many times in order to determine the likelihood of observing an outcome at least as extreme as the one found in the actual study.</div>
<p>
</p>
<div class="line-block">We will introduce the basic concepts of randomization tests in a setting where units (mice in this example) are randomly allocated to a treatment or control group. Using a significance test, we will decide if an observed treatment effect (the observed difference between the mean responses in the treatment and control) is “real” or if “random chance alone” could plausibly explain the observed effect. The null hypothesis states that “random chance alone” is the reason for the observed effect. In this initial discussion, the alternative hypothesis will be onesided because we want to show that the true treatment mean (<span class="math inline">\(\mu\)</span>treatment) is less than the true control mean (<span class="math inline">\(\mu\)</span>control). Later, we will expand the discussion to consider modifications needed to deal with two-sided alternatives.</div>
</div>
<div id="statistical-inference-through-a-randomization-test" class="section level2" number="1.2">
<h2>
<span class="header-section-number">1.2</span> <strong>Statistical Inference Through a Randomization Test</strong><a class="anchor" aria-label="anchor" href="#statistical-inference-through-a-randomization-test"><i class="fas fa-link"></i></a>
</h2>
<p>Whether they take the form of significance tests or confidence intervals, inferential procedures rest on the fundamental question for inference: “What would happen if we did this many times?” Let’s unpack this
question in the context of the female mice in the schistosomiasis study. We observed a difference in means
of 7.6 = 12.00 - 4.40 worms between control and treatment groups. While we expect that this large difference
reflects the effectiveness of the drug, it is possible that chance alone could explain this difference. This
“chance alone” position is usually called the null hypothesis and includes the following assumptions:</p>
<ul>
<li>The number of parasitic worms found in the liver naturally varies from mouse to mouse.</li>
<li>Whether or not the drug is effective, there clearly is variability in the responses of mice to the infestation
of schistosomes.</li>
<li>Each group exhibits this variability, and even if the drug is not effective, some mice do better than
others.</li>
<li>The only explanation for the observed difference of 7.6 worms in the means is that the random
allocation randomly placed mice with larger numbers of worms in the control group and mice with
smaller numbers of worms in the treatment group.</li>
</ul>
<div class="line-block">In this study, the null hypothesis is that the treatment has no effect on the average worm count, and it</div>
<p>is denoted as
| <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\mu\)</span>control = <span class="math inline">\(\mu\)</span>treatment
Another way to write this null hypothesis is
<span class="math inline">\(H_0\)</span>: the treatment has no effect on average worm count</p>
<p>The research hypothesis (the treatment causes a reduction in the average worm count) is called the alternative
hypothesis and is denoted <span class="math inline">\(H_a\)</span> (or <span class="math inline">\(H_1\)</span>). For example,
<span class="math inline">\(H_a\)</span>: mcontrol 7 mtreatment
Another way to write this alternative hypothesis is
Ha: the treatment reduces the average worm count
Alternative hypotheses can be “one-sided, greater than” (as in this investigation), “one-sided, less-than”
(the treatment causes an increase in worm count), or “two-sided” (the treatment mean is different, in one
direction or the other, from the control mean). We chose to test a one-sided hypothesis because there is a
clear research interest in one direction. In other words, we will take action (start using the drug) only if we
can show that K11777 reduces the worm count.</p>
<p>
</p>
</div>
<div id="activity-conducting-a-randomization-test-by-hand" class="section level2 unnumbered">
<h2>Activity: <em>Conducting a Randomization Test by Hand</em><a class="anchor" aria-label="anchor" href="#activity-conducting-a-randomization-test-by-hand"><i class="fas fa-link"></i></a>
</h2>
<ol start="3" style="list-style-type: decimal">
<li>To get a feel for the concept of a p-value, write each of the female worm counts on an index card.
Shuffle the 10 index cards, and then draw five cards at random (without replacement). Call these five
cards the treatment group and the five remaining cards the control group. Under the null hypothesis
(i.e. the treatment has no effect on worm counts), this allocation mimics precisely what actually happened
in our experiment, since the only cause of group differences is the random allocation.
| Calculate the mean of the five cards representing the treatment group and the mean of the five
cards representing the control group. Then find the difference between the control and treatment group means that you obtained in your allocation. To be consistent, take the control group mean minus the
treatment group mean. Your work should look similar to the following simulation:</li>
</ol>
<p>[[[Fig_CT]]]</p>
<ol start="4" style="list-style-type: decimal">
<li><p>If you were to do another random allocation, would you get the same difference in means? Explain.</p></li>
<li><p>Now, perform nine more random allocations, each time computing and writing down the difference in
mean worm count between the control group and the treatment group. Make a dotplot of the 10 differences.
What proportion of these differences are 7.6 or larger?</p></li>
<li><p>If you performed the simulation many times, would you expect a large percentage of the simulations to
result in a mean difference greater than 7.6? Explain.</p></li>
</ol>
<div class="line-block">The reasoning in the previous activity leads us to the randomization test and an interpretation of the</div>
<p>fundamental question for inference. The fundamental question for this context is as follows: “If the null
hypothesis were actually true and we randomly allocated our 10 mice to treatment and control groups many
times, what proportion of the time would the observed difference in means be as big as or bigger than 7.6?”
This long-run proportion is a probability that statisticians call the <strong>p-value</strong> of the randomization test. The
p-values for most randomization tests are found through simulations. Despite the fact that simulations do
not give exact p-values, they are usually preferred over the tedious and time-consuming process of listing
all possible outcomes. Researchers usually pick a round number such as 10,000 repetitions of the simulation
and approximate the p-value accordingly. Since this p-value is an approximation, it is often referred to as
the <strong>empirical p-value</strong>.</p>
<p>
</p>

<p>
Many researchers include the observed value as one of the possible outcomes. In this case, N = 9999
iterations are typically used and the p-value is calculated as (X + 1)/(9999 + 1). The results are very
similar whether X/10,000 or (X + 1)/(9999 + 1) is used. Including the observed value as one of the
possible allocations is a more conservative approach and protects against getting a p-value of 0. Our
observation from the actual experiment provides evidence that the true p-value is greater than zero.</p>
</div>
<div id="performing-a-randomization-test-using-a-computer-simulation" class="section level2" number="1.3">
<h2>
<span class="header-section-number">1.3</span> <strong>Performing a Randomization Test Using a Computer Simulation</strong><a class="anchor" aria-label="anchor" href="#performing-a-randomization-test-using-a-computer-simulation"><i class="fas fa-link"></i></a>
</h2>
<p>While physical simulations (such as the index cards activity) help us understand the process of computing an
empirical p-value, using computer software is a much more efficient way of producing an empirical p-value
based on a large number of iterations. If you are simulating 10 random allocations, it is just as easy to use index cards as a computer. However, the advantage of a computer simulation is that 10,000 random allocations
can be conducted in almost the same amount of time it takes to simulate 10 allocations. In the following
steps, you will develop a program to calculate an empirical p-value.</p>
</div>
<div id="activity-using-computer-simulations-to-conduct-a-hypothesis-test" class="section level2 unnumbered">
<h2>Activity: <em>Using Computer Simulations to Conduct a Hypothesis Test</em><a class="anchor" aria-label="anchor" href="#activity-using-computer-simulations-to-conduct-a-hypothesis-test"><i class="fas fa-link"></i></a>
</h2>
<ol start="7" style="list-style-type: decimal">
<li><p>Use the technology instructions provided on the CD to insert the schistosomiasis data into a statistical
software package and randomly allocate each of the 10 female worm counts to either the treatment or the
control group.</p></li>
<li><p>Take the control group average minus the K11777 treatment group average.</p></li>
<li><p>Use the instructions to write a program, function, or macro to repeat the process 10,000 times. Count
the number of simulations where the difference between the group averages (control minus K11777) is
greater than or equal to 7.6, divide that count by 10,000, and report the resulting empirical p-value.</p></li>
<li><p>Create a histogram of the 10,000 simulated differences between group means and comment on the
shape of the histogram. This histogram, created from simulations of a randomization test, is called an
empirical randomization distribution. This distribution describes the frequency of each observed
difference (between the control and treatment means) when the null hypothesis is true.</p></li>
<li><p>Based on your results in Questions 9 and 10 and assuming the null hypothesis is true, about how frequently
do you think you would obtain a mean difference as large as or larger than 7.6 by random allocation alone?</p></li>
<li><p>Does your answer to Question 11 lead you to believe the “chance alone” position (i.e., the null hypothesis
that the mean worm count is the same for both the treatment and the control), or does it lead you to
believe that K11777 has a positive inhibitory effect on the schistosome worm in female mice? Explain.</p></li>
</ol>
<div class="line-block">Figure 1.2 shows a histogram resulting from the previous activity. A computer simulation of Question 9</div>
<p>resulted in a p-value of 281/10,000 = 0.0281. This result shows that random allocation alone would produce
a mean group difference as large as or larger than 7.6 only about 3% of the time, suggesting that something
other than chance is needed to explain the difference in group means. Since the only other distinction between
the groups is the presence or absence of treatment, we can conclude that the treatment causes a reduction in
worm counts.</p>
<div class="line-block">We conducted four more simulations, each with 10,000 iterations, which resulted in p-values of 0.0272,</div>
<p>0.0282, 0.0268, and 0.0285. When the number of iterations is large, the empirical randomization distribution
(such as the histogram created in Question 10) provides a precise estimate of the likelihood of all possible values of the difference between the control and treatment means. Thus, when the number of iterations is large,
well-designed simulation studies result in empirical p-values that are fairly accurate. The larger the number
of iterations (i.e., randomizations) within a simulation study, the more precise the p-value is.</p>
<div class="figure">
<span style="display:block;" id="fig:graph2"></span>
<img src="index_files/figure-html/graph2-1.png" alt="Histogram showing the results of a schistosomiasis simulation study. In this simulation, 281 out of 10,000 resulted in a difference greater than or equal to 7.6. " width="672"><p class="caption">
Figure 1.2: Histogram showing the results of a schistosomiasis simulation study. In this simulation, 281 out of 10,000 resulted in a difference greater than or equal to 7.6.
</p>
</div>
<div class="line-block"> Because the sample sizes in the schistosomiasis study are small, it is possible to apply mathematical</div>
<p>methods to obtain an <strong>exact p-value</strong> for this randomization test. An exact p-value can be calculated by writing
down the set of all possibilities (assuming each possible outcome is equally likely under the null hypothesis)
and then calculating the proportion of the set for which the difference is at least as large as the observed difference.
In the schistosomiasis study, this requires listing every possible combination in which five of the 10
female mice can be allocated to the treatment (and the other five assigned to the control). There are 252 possible
combinations. For each of these combinations, the difference between the treatment and control means
is then calculated. The exact p-value is the proportion of times in which the difference in the means is at least
as large as the observed difference of 7.6 worms. Of these 252 combinations, six have a mean difference of
7.6 and one has a mean difference greater than 7.6 (namely 8.8). Since all 252 of these random allocations are
equally likely, the exact p-value in this example is 7/252 = 0.0278. However, most real studies are too large
to list all possible samples. Randomization tests are almost always adequate, providing approximate p-values
that are close enough to the true p-value.</p>
<p>
Conducting a two-sample t-test on the female mice provides a p-value of 0.011. This p-value of 0.011 is
accurate only if the observed test statistic (i.e., the difference between means) follows appropriate assumptions
about the distribution. Figure 1.2 demonstrates that the distributional assumptions are violated. While
the randomization test provides an approximate p-value “close to 0.0278,” it provides a much better estimate
of the exact p-value than does the two-sample t-test. Note that each of the five simulations listed gave a
p-value closer to the exact p-value than the one given by the two-sample t-test. </p>
<p>
</p>
<div class="line-block">Sometimes we have some threshold p-value at or below which we will reject the null hypothesis and</div>
<p>conclude in favor of the alternative. This threshold value is called a significance level and is usually denoted
by the Greek letter alpha (<span class="math inline">\(\alpha\)</span>). Common values are <span class="math inline">\(\alpha\)</span> = 0.05 and <span class="math inline">\(\alpha\)</span> = 0.01, but the value will depend heavily
on context and on the researcher’s assessment of the acceptable risk of stating an incorrect conclusion. When
the study’s p-value is less than or equal to this significance level, we state that the results are statistically
significant at level A. If you see the phrase “statistically significant” without a specification of <span class="math inline">\(\alpha\)</span> the writer
is most likely assuming <span class="math inline">\(\alpha\)</span> = 0.05, for reasons of history and convention alone. However, it is best to show
the p-value instead of simply stating a result is significant at a particular <span class="math inline">\(\alpha\)</span>-level.</p>
</div>
<div id="two-sided-tests" class="section level2" number="1.4">
<h2>
<span class="header-section-number">1.4</span> <strong>Two-Sided Tests</strong><a class="anchor" aria-label="anchor" href="#two-sided-tests"><i class="fas fa-link"></i></a>
</h2>
<p>The direction of the alternative hypothesis is derived from the research hypothesis. In this K11777 study, we
enter the study expecting a reduction in worm counts and hoping the data will bear out this expectation. It is
our expectation, hope, or interest that drives the alternative hypothesis and the randomization calculation. Occasionally,
we enter a study without a firm direction in mind for the alternative, in which case we use a two-sided
alternative. Furthermore, even if we hope that the new treatment will be better than the old treatment or better
than a control, we might be wrong—it may be that the new treatment is actually worse than the old treatment
or even harmful (worse than the control). Some statisticians argue that a conservative objective approach is to
always consider the two-sided alternative. For a <strong>two-sided test</strong>, the p-value must take into account extreme
values of the test statistic in either direction (no matter which direction we actually observe in our sample data)</p>
<p>
</p>
<div class="line-block">We will now make our definition of the p-value more general to allow for a wider variety of significance</div>
<p>testing situations. The <strong>p-value</strong> is the probability of observing a group difference as extreme as or more extreme
than the group difference actually observed in the sample data, assuming that there is nothing creating group
differences except the random allocation process.</p>
</div>
<div id="activity-a-two-sided-hypothesis-test" class="section level2 unnumbered">
<h2>Activity: <em>A Two-Sided Hypothesis Test</em><a class="anchor" aria-label="anchor" href="#activity-a-two-sided-hypothesis-test"><i class="fas fa-link"></i></a>
</h2>
<ol start="13" style="list-style-type: decimal">
<li>Run the simulation study again to find the empirical p-value for a two-sided hypothesis test to determine
if there is a difference between the treatment and control group means for female mice.</li>
<li>Is the number of simulations resulting in a difference greater than or equal to 7.6 identical to the number
of simulations resulting in a difference less than or equal to -7.6? Explain why these two values
are likely to be close but not identical.</li>
<li>Explain why you expect the p-value for the two-sided alternative to be about double that for the onesided
alternative. Hint: You may want to look at Figure 1.2</li>
<li>Using the two-sided alternative hypothesis, the two-sample t-test provides a p-value of 0.022.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;When we do not assume equal variances Minitab uses 7 degrees of freedom providing a p-value of 0.022 while R uses
7.929 degrees of freedom resulting in a p-value of 0.0194.&lt;/p&gt;"><sup>2</sup></a> This
p-value would provide strong evidence for rejecting the assumption that there is no difference between
the treatment and the control (null hypothesis). However, this p-value should not be used to draw
conclusions about this study. Explain why.</li>
</ol>
<div class="line-block">For the above study, a simulation involving 100,000 iterations provided an empirical p-value of 0.0554.</div>
<p>Again, because this particular data set is small, all 252 possible random allocations can be listed to find that
the exact two-sided p-value is 14/252 = 0.0556.</p>
</div>
<div id="what-can-we-conclude-from-the-schistosomiasis-study" class="section level2" number="1.5">
<h2>
<span class="header-section-number">1.5</span> <strong>What Can We Conclude from the Schistosomiasis Study?</strong><a class="anchor" aria-label="anchor" href="#what-can-we-conclude-from-the-schistosomiasis-study"><i class="fas fa-link"></i></a>
</h2>
<p>The key question in this study is whether K11777 will reduce the spread of a common and potentially deadly
disease. The result that you calculated from the one-sided randomization hypothesis test should have been
close to the exact p-value of 0.0278. This small p-value allows you to reject the null hypothesis and conclude
that the worm counts are lower in the female treatment group than in the female control group. In every study,
it is important to consider how random allocation and random sampling impact the conclusions.</p>
<div class="line-block">
<em>Random allocation</em>: The schistosomiasis study was an <strong>experiment</strong> because the units (female mice)</div>
<p>were randomly allocated to treatment or control groups. To the best of our knowledge this experiment
controlled for any outside influences and allows us to state that there is a cause and effect relationship
between the treatment and response. Therefore, we can conclude that K11777 did cause a reduction in
the average number of schistosome parasites in these female mice.</p>
<div class="line-block">
<em>Random sampling</em>: Mice for this type of study are typically ordered from a facility that breeds and raises lab</div>
<p>mice. It is possible that the mice in this study were biologically related or were exposed to something that
caused their response to be different from that of other mice. Similarly, there are risks in simply assuming
that male mice have the same response as females, so the end-of-chapter exercises provide an opportunity to conduct a separate test on the male mice. Since our sample of 10 female mice was not selected at random
from the population of all mice, we should question whether the results from this study hold for all mice.</p>
<p>More importantly, the results have not shown that this new drug will have the same impact on humans
as it does on mice. In addition, even though we found that K11777 does cause a reduction in worm counts,
we did not specifically show that it will reduce the spread of the disease. Is the disease less deadly if only two
worms are in the body instead of 10? Statistical consultants aren’t typically expected to know the answers to
these theoretical, biological, or medical types of questions, but they should ask questions to ensure that the
study conclusions match the hypothesis that was tested. In most cases, drug tests require multiple levels of
studies to ensure that the drug is safe and to show that the results are consistent across the entire population of
interest. While this study is very promising, much more work is needed before we can conclude that K11777
can reduce the spread of schistosomiasis in humans.</p>
</div>
<div id="a-closer-look-nonparametric-methods" class="section level2 unnumbered">
<h2>
<em>A Closer Look: Nonparametric Methods</em><a class="anchor" aria-label="anchor" href="#a-closer-look-nonparametric-methods"><i class="fas fa-link"></i></a>
</h2>
</div>
<div id="permutation-tests-versus-randomization-tests" class="section level2" number="1.6">
<h2>
<span class="header-section-number">1.6</span> <strong>Permutation Tests versus Randomization Tests</strong><a class="anchor" aria-label="anchor" href="#permutation-tests-versus-randomization-tests"><i class="fas fa-link"></i></a>
</h2>
<p>The random allocation of experimental units (e.g., mice) to groups provides the basis for statistical inference in
a randomized comparative experiment. In the schistosomiasis K11777 treatment study, we used a significance
test to ascertain whether cause and effect was at work. In the context of the random allocation study design,
we called our significance test a randomization test.
| In <strong>observational studies</strong>, subjects are not randomly allocated to groups. In this context, we apply the
same inferential procedures as in the previous experiment, but we commonly call the significance test a
<strong>permutation test</strong> rather than a randomization test.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;This text defines a randomization test as a permutation test that is based on random allocation. Some statisticians do not
distinguish between permutation tests and randomization tests. They call simulation studies permutation tests, whether
they are based on observational studies or experiments.&lt;/p&gt;"><sup>3</sup></a> More importantly, in observational studies, the results
of the test cannot typically be used to claim cause and effect; a researcher should exhibit more caution in the
interpretation of results.</p>
<p>
</p>
<p>
</p>
</div>
<div id="age-discrimination-study" class="section level2 unnumbered">
<h2>Age Discrimination Study<a class="anchor" aria-label="anchor" href="#age-discrimination-study"><i class="fas fa-link"></i></a>
</h2>
<p>Westvaco is a company that produces paper products. In 1991, Robert Martin was working in the engineering
department of the company’s envelope division when he was laid off in Round 2 of several rounds of layoffs
by the company.3 He sued the company, claiming to be the victim of age discrimination. The ages of the 10
workers involved in Round 2 were: 25, 33, 35, 38, 48, 55, 55, 55, 56, and 64. The ages of the three people
laid off were 55, 55, and 64.</p>
<div class="line-block">Figure 1.3 shows a comparative dotplot for age by layoff category. This dotplot gives the impression that</div>
<p>Robert Martin may have a case: It appears as if older workers were more likely to be laid off. But we know
enough about variability to be cautious.</p>
<div class="figure">
<span style="display:block;" id="fig:graph3"></span>
<img src="index_files/figure-html/graph3-1.png" alt="Dotplot of age in years of worker versus layoff (whether he or she was laid off)" width="672"><p class="caption">
Figure 1.3: Dotplot of age in years of worker versus layoff (whether he or she was laid off)
</p>
</div>
</div>
<div id="extended-activity-is-there-evidence-of-age-discrimination" class="section level2 unnumbered">
<h2>Extended Activity: <em>Is There Evidence of Age Discrimination?</em><a class="anchor" aria-label="anchor" href="#extended-activity-is-there-evidence-of-age-discrimination"><i class="fas fa-link"></i></a>
</h2>
<p>Data set: <code>Age</code>
17. Conduct a permutation test to determine whether the observed difference between means is likely to
occur just by chance. Use <code>Age</code> as the response variable and <code>Layoff</code> as the explanatory variable. Here
we are interested in only a one-sided hypothesis test to determine if the mean age of people who were
laid off is higher than the mean age of people who were not laid off.</p>
<ol start="18" style="list-style-type: decimal">
<li>Modify the program/macro you created in Question 17 to conduct a one-sided hypothesis test to determine
if the median age of people who were laid off is higher than the median age of people who were
not laid off. Report the p-value and compare your results to those in Question 17.</li>
</ol>
<div class="line-block"> Since there was no random allocation (i.e., people were not randomly assigned to a layoff group),</div>
<p>statistical significance does not give us the right to assert that greater age is <em>causing</em> a difference in being
laid off. The null hypothesis in this context becomes “The observed difference could be explained as if
by random allocation alone.” That is, we proceed as any practicing social scientist must when working
with observational data. We “imagine” an experiment in which workers are randomly allocated to a
layoff group and then determine if the observed average difference between the ages of laid-off workers
and those not laid off is significantly larger than would be expected to occur by chance in a randomized
comparative experiment.
| While age could be the cause for the difference—hence proving an allegation of age discrimination—
there are many other possibilities (i.e., extraneous variables), such as the educational levels of the
workers, their competence to do the job, and ratings on past performance evaluations. Rejecting the
“as if by random allocation” hypothesis in the nonrandomized context can be a useful step toward
establishing causality; however, it cannot establish causality unless the extraneous variables have been
properly accounted for.
| In the actual court case, data from all three rounds of layoffs were statistically analyzed. The analysis
showed some evidence that older people were more likely to be laid off; however, Robert Martin ended up
settling out of court.</p>
</div>
<div id="permutation-and-randomization-tests-for-matched-pairs-designs" class="section level2" number="1.7">
<h2>
<span class="header-section-number">1.7</span> <strong>Permutation and Randomization Tests for Matched Pairs Designs</strong><a class="anchor" aria-label="anchor" href="#permutation-and-randomization-tests-for-matched-pairs-designs"><i class="fas fa-link"></i></a>
</h2>
<p>The ideas developed in this chapter can be extended to other study designs, such as a basic two-variable design
called a matched pairs design. In a matched pairs design, each experimental unit provides both measurements
in a study with two treatments (one of which could be a control). Conversely, in the completely randomized
situation of the schistosomiasis K11777 treatment study, half the units were assigned to control and half to
treatment; no mouse received both treatments.</p>
</div>
<div id="music-and-relaxation" class="section level2 unnumbered">
<h2>Music and Relaxation<a class="anchor" aria-label="anchor" href="#music-and-relaxation"><i class="fas fa-link"></i></a>
</h2>
<p>Grinnell College students Anne Tillema and Anna Tekippe conducted an experiment to study the effect of
music on a person’s level of relaxation. They hypothesized that fast songs would increase pulse rate more
than slow songs. The file called Music contains the data from their experiment. They decided to use a person’s
pulse rate as an operational definition of the person’s level of relaxation and to compare pulse rates for two selections of music: a fast song and a slow song. For the fast song they chose “Beyond” by Nine Inch
Nails, and for the slow song they chose Rachmaninoff’s “Vocalise.” They recruited 28 student subjects for
the experiment.</p>
<div class="line-block">Anne and Anna came up with the following experimental design. Their fundamental question</div>
<p>involved two treatments: (1) listening to the fast song and (2) listening to the slow song. They could
have randomly allocated 14 subjects to hear the fast song and 14 subjects to hear the slow song, but
their more efficient approach was to have each subject provide both measurements. That is, each subject
listened to both songs, giving rise to two data values for each subject, called a matched pairs. Randomization
came into play when it was decided by a coin flip whether each subject would listen first to the
fast song or the slow song.</p>
<p>
</p>
<div class="line-block"> Specifically, as determined by coin flips, half the subjects experienced the following procedure:</div>
<p>[one minute of rest; measure pulse (prepulse)] <span class="math inline">\(&gt;\)</span> [listen to fast song for 2 minutes; measure pulse
for second minute (fast song pulse)] <span class="math inline">\(&gt;\)</span> [rest for one minute] <span class="math inline">\(&gt;\)</span> [listen to slow song for 2 minutes;
measure pulse for second minute (slow song pulse)].</p>
<div class="line-block">The other half experienced the procedure the same way except that they heard the slow song first and</div>
<p>the fast song second.
| Each subject gives us two measurements of interest for analysis: (1) fast song pulse minus prepulse
and (2) slow song pulse minus prepulse. In the data file, these two measurements are called <code>Fastdiff</code> and
<code>Slowdiff</code>, respectively.</p>
<div class="line-block">Figure 1.4 shows a dotplot of the 28 <code>Fastdiff</code>-minus-<code>Slowdiff</code> values. Notice that positive numbers</div>
<p>predominate and the mean difference is 1.857 beats per minute, both suggesting that the fast song does indeed
heighten response (pulse rate) more than the slow song. We need to confirm this suspicion with a randomization
test.</p>
<div class="line-block">To perform a randomization test, we mimic the randomization procedure of the study design. Here,</div>
<p>the randomization determined the order in which the subject heard the songs, so randomization is applied
to the two measurements of interest for each subject. To compute a p-value, we determine how frequently
we would obtain an observed difference as large as or larger than 1.857.</p>
<pre><code>#&gt; Bin width defaults to 1/30 of the range of the data. Pick
#&gt; better value with `binwidth`.</code></pre>
<div class="figure">
<span style="display:block;" id="fig:graph4"></span>
<img src="index_files/figure-html/graph4-1.png" alt="Dotplot of the difference in pulse rates for each of the 28 subjects." width="672"><p class="caption">
Figure 1.4: Dotplot of the difference in pulse rates for each of the 28 subjects.
</p>
</div>
</div>
<div id="extended-activity-testing-the-effect-of-music-on-relaxation" class="section level2 unnumbered">
<h2>Extended Activity: <em>Testing the Effect of Music on Relaxation</em><a class="anchor" aria-label="anchor" href="#extended-activity-testing-the-effect-of-music-on-relaxation"><i class="fas fa-link"></i></a>
</h2>
Data set: <code>Music</code>
<p>
</p>
</div>
<div id="the-bootstrap-distribution" class="section level2" number="1.8">
<h2>
<span class="header-section-number">1.8</span> <strong>The Bootstrap Distribution</strong><a class="anchor" aria-label="anchor" href="#the-bootstrap-distribution"><i class="fas fa-link"></i></a>
</h2>
<p>Bootstrapping is another simulation technique that is commonly used to develop confidence intervals and
hypothesis tests. Bootstrap techniques are useful because they generalize to situations where traditional methods
based on the normal distribution cannot be applied. For example, they can be used to create confidence intervals
and hypothesis tests for any parameter of interest, such as a median, ratio, or standard deviation. Bootstrap
methods differ from previously discussed techniques in that they sample <strong>with replacement</strong> (randomly draw
an observation from the original sample and put the observation back before drawing the next observation).
| Permutation tests, randomization tests, and bootstrapping are often called <strong>resampling techniques</strong>
because, instead of collecting many different samples from a population, we take repeated samples (called
resamples) from just one random sample.</p>
</div>
<div id="extended-activity-creating-a-sampling-distribution-and-a-bootstrap-distribution" class="section level2 unnumbered">
<h2>Extended Activity: <em>Creating a Sampling Distribution and a Bootstrap Distribution</em><a class="anchor" aria-label="anchor" href="#extended-activity-creating-a-sampling-distribution-and-a-bootstrap-distribution"><i class="fas fa-link"></i></a>
</h2>
Data set: <code>ChiSq</code>
<p>
</p>
<div class="line-block">In many real-world situations, the process used in Question 21 is not practical because collecting more</div>
<p>than one simple random sample is too expensive or time consuming. While the approach in Question 22 is
computer intensive, it is simple and convenient since it uses only one simple random sample. The key idea
behind bootstrap methods is the assumption that the original sample represents the population, so resamples
from the one simple random sample can be used to represent samples from the population, as is done in Question
22. Thus, the bootstrap distribution provides an approximation of the sampling distribution.</p>
<div class="line-block">Most traditional methods of statistical inference involve collecting one sample and calculating the sample</div>
<p>mean. Then, based on the central limit theorem, assumptions are made about the shape and spread of the
sampling distribution. In Question 22 we used one sample to calculate the sample mean and then used the
bootstrap distribution to estimate the shape and spread of the sampling distribution.</p>
<div class="line-block">The central limit theorem tells us about the shape and spread of the sample mean. A key advantage of</div>
<p>the bootstrap distribution is that it works for any parameter of interest. Thus, the bootstrap distribution can be
used to estimate the shape and spread for any sampling distribution of interest.</p>
<p>
</p>
<div class="line-block">Figure 1.5 shows the sampling distribution and the bootstrap distribution when a sample size of 10 is used to estimate the mean of the <code>ChiSq</code> data. Notice that the spreads for both histograms are</div>
<p>roughly equivalent. The central limit theorem tells us that the standard deviation of the sampling distribution (the distribution of <span class="math inline">\(\bar{x}\)</span> ) should be <span class="math inline">\(\sigma\)</span>/<span class="math inline">\(\sqrt{n}\)</span> = 1.3153/<span class="math inline">\(\sqrt{10}\)</span> = 0.4159. The standard deviation of the bootstrap distribution is 0.4541, which is a reasonable estimate of the standard deviation of the sampling distribution. In addition, both graphs have similar, right-skewed shapes. The strength of the bootstrap method is that it provides accurate estimates of the shape and spread of the sampling distribution. In general, histograms from the bootstrap distribution will have a similar shape and spread as histograms
from the sampling distribution.</p>
<pre><code>#&gt; [1] "1.465279886"</code></pre>
<p>[[[Fig1,5]]]</p>
<div class="line-block">The bootstrap method does not improve our estimate of the population mean. The mean of the sampling distribution in Question 21 will typically be very close to the population mean. But the mean of the bootstrap distribution in Question 22 typically will not be as accurate, because it is based on only one simple random sample. Ideally, we would like to know how close the statistic from our original sample is to the population parameter. A statistic is biased if it is not centered at the value of the population parameter. We can use the bootstrap distribution to estimate the bias of a statistic. The difference between the original sample mean and the bootstrap mean is called the <strong>bootstrap estimate of bias</strong>.</div>
<p>
</p>
</div>
<div id="using-bootstrap-methods-to-create-confidence-intervals" class="section level2" number="1.9">
<h2>
<span class="header-section-number">1.9</span> <strong>Using Bootstrap Methods to Create Confidence Intervals</strong><a class="anchor" aria-label="anchor" href="#using-bootstrap-methods-to-create-confidence-intervals"><i class="fas fa-link"></i></a>
</h2>
<p>A <strong>confidence interval</strong> gives a range of plausible values for some parameter. This is a range of values surrounding an observed estimate of the parameter—an estimate based on the data. To this range of values we attach a level of confidence that the true parameter lies in the range. An alpha-level, <span class="math inline">\(\alpha\)</span>, is often used to specify the level of confidence. For example, when <span class="math inline">\(\alpha\)</span> = 0.05, we have a 100(1 - <span class="math inline">\(\alpha\)</span>), = 95<span class="math inline">\(\%\)</span> confidence level. Thus, a 100(1 - <span class="math inline">\(\alpha\)</span>), confidence interval gives an estimate of where we think the parameter is and how precisely we have it pinned down.</p>
</div>
<div id="bootstrap-t-confidence-intervals-" class="section level2" number="1.10">
<h2>
<span class="header-section-number">1.10</span> *<strong>Bootstrap t Confidence Intervals</strong>{-}<a class="anchor" aria-label="anchor" href="#bootstrap-t-confidence-intervals-"><i class="fas fa-link"></i></a>
</h2>
<p>If the bootstrap distribution appears to be approximately normal, it is typically safe to assume that a
t-distribution can be used to calculate a 100(1 - <span class="math inline">\(\alpha\)</span>), confidence interval for <span class="math inline">\(\mu\)</span>, often called a bootstrap
t confidence interval:</p>
<p><span class="math display">\[\begin{equation}
  \bar{x} \pm t^*\left(S^*\right)
  \tag{1.1} \label{eq:1_1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(S^*\)</span> is the standard deviation of the bootstrap distribution and <span class="math inline">\(t^*\)</span> is the critical value of the t-distribution with n - 1 degrees of freedom.</p>
<div class="line-block">The one simple random sample of size n = 10 used to create the bootstrap distribution in Figure 1.5b has a mean of <span class="math inline">\(\bar{x}\)</span> = 1.238 and a standard deviation of s = 1.490. The bootstrap distribution in Figure 1.5b has a mean of <span class="math inline">\(\bar{x}^*\)</span> = 1.249 and a standard deviation of <span class="math inline">\(S^*\)</span> = 0.4541. Notice that Formula (1.1) uses the mean from the original sample but uses the bootstrap distribution to estimate the spread. If we <em>incorrectly assume</em> that the sampling distribution in Figure 1.5 is normal, a 95% bootstrap t confidence interval for <span class="math inline">\(\mu\)</span> is given by</div>
<p><span class="math display">\[\begin{equation}
  \bar{x} \pm t^*\left(S^*\right) = 1.238 \pm 2.262(0.4541)
\end{equation}\]</span></p>
<p>where <span class="math inline">\(t^*\)</span> = 2.262 is the critical value corresponding to the 97.5th percentile of the t-distribution with n - 1 = 9
degrees of freedom. Thus, the 95% confidence interval for <span class="math inline">\(\mu\)</span> is (0.211, 2.265).</p>
<p>
</p>
<p>With skewed data or small sample sizes (if the original data are not normally distributed), parametric
methods (which are based on the central limit theorem) are not appropriate. In Figure 1.5 we see that the
sampling distribution is skewed to the right. <em>Thus, with a sample size of 10, neither the traditional onesample
t confidence interval nor the bootstrap t confidence interval is reliable in this example</em>. However,
with a sample size of 40, the histograms in Questions 21 and 22 should tend to look somewhat normally
distributed.</p>
</div>
<div id="bootstrap-percentile-confidence-intervals" class="section level2 unnumbered">
<h2>Bootstrap Percentile Confidence Intervals<a class="anchor" aria-label="anchor" href="#bootstrap-percentile-confidence-intervals"><i class="fas fa-link"></i></a>
</h2>
<p>Bootstrap percentile confidence intervals are found by calculating the appropriate percentiles of the bootstrap distribution. To find a 100(1 - <span class="math inline">\(\alpha\)</span>) confidence interval, take the <span class="math inline">\(\alpha\)</span>/2 * 100 percentile of each tail of the bootstrap distribution. For example, to find a 95% confidence interval for <span class="math inline">\(\mu\)</span>, sort all the observations from the bootstrap distribution and find the values that represents the 2.5th and 97.5th percentiles of the bootstrap distribution. The
2.5th percentile of the bootstrap distribution in Figure 1.5b is 0.546, and the 97.5th percentile is 2.282. Thus,
a 95% confidence interval for <span class="math inline">\(\mu\)</span> is (0.546, 2.282).
Notice that the percentile confidence interval is not centered at the sample mean. Since the bootstrap
distribution is right skewed, the right side of the confidence interval (2.282 - 1.238 = 1.044) is wider than
the left side of the confidence interval (1.238 - 0.546 = 0.692). This lack of symmetry can influence the
accuracy of the confidence interval.</p>
<p>
</p>
</div>
<div id="when-to-use-bootstrap-confidence-intervals" class="section level2 unnumbered">
<h2>When to Use Bootstrap Confidence Intervals<a class="anchor" aria-label="anchor" href="#when-to-use-bootstrap-confidence-intervals"><i class="fas fa-link"></i></a>
</h2>
<p>Bootstrap methods are extremely useful when we cannot use theory, such as the central limit theorem, to
approximate the sampling distribution. Thus, bootstrap methods can be used to create confidence intervals
for essentially any parameter of interest, while the central limit theorem is limited to only a few parameters
(such as the population mean).<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Theoretical methods allow distributional tests for more than just the population mean. However, for purposes of this text it is sufficient to understand that distributional methods tend to be more complicated and are limited to testing only a few
parameters that could be of interest.&lt;/p&gt;"><sup>4</sup></a> However, bootstrap methods are not always reliable.</p>
<div class="line-block">Small sample sizes still produce problems for bootstrap methods. When the sample size is small, (1) the sample statistic may not accurately estimate the population parameter, (2) the distribution of sample means is less likely to be symmetric, and (3) the shape and spread of the bootstrap distribution may not accurately represent those of the true sampling distribution.</div>
<div class="line-block">In addition, bootstrap methods do not work equally well for all parameters. For example, the end-ofchapter</div>
<p>exercises show that bootstrapping often provides unreliable bootstrap distributions for median values because the median of a resample is likely to have only a few possible values. Thus, confidence intervals for medians should be used only with large (n <span class="math inline">\(\geq\)</span> 100) sample sizes.</p>
<div class="line-block">It is not easy to determine whether bootstrap methods provide appropriate confidence intervals. The bootstrap t and bootstrap percentile confidence intervals are often compared to each other. While the percentile confidence interval tends to be more accurate, neither of the two should be used if the intervals are not relatively</div>
<p>close. If the bootstrap distribution is skewed or biased, other methods should be used to find confidence intervals. More advanced bootstrap methods (such as BCa and tilting confidence intervals) are available that are generally accurate when bias or skewness exists in the bootstrap distribution.<span class="math inline">\(^5\)</span></p>
</div>
<div id="extended-activityestimating-salaries-of-medical-faculty" class="section level2 unnumbered">
<h2>Extended Activity:<em>Estimating Salaries of Medical Faculty</em><a class="anchor" aria-label="anchor" href="#extended-activityestimating-salaries-of-medical-faculty"><i class="fas fa-link"></i></a>
</h2>
Data set: <code>MedSalaries</code>.
The file <code>MedSalaries</code> is a random sample of n = 100 salaries of medical doctors who were teaching at United States universities in 2009.
</div>
<div id="relationship-between-the-randomization-test-and-the-two-sample-t-test" class="section level2" number="1.11">
<h2>
<span class="header-section-number">1.11</span> <strong>Relationship Between the Randomization Test and the Two-Sample t-Test</strong><a class="anchor" aria-label="anchor" href="#relationship-between-the-randomization-test-and-the-two-sample-t-test"><i class="fas fa-link"></i></a>
</h2>
<p>R.A. Fisher, perhaps the preeminent statistician of the 20th century, introduced the randomization test in the context of a two-group randomly allocated experiment in his famous 1935 book, <em>Design of Experiments</em>.<span class="math inline">\(^6\)</span> At that time he acknowledged that the randomization test was not practical because of the computational
intensity of the calculation. Clearly, 1935 predates modern computing. Indeed, Efron and Tibshirani describe the permutation test as “a computer-intensive statistical technique that predates computers.”<span class="math inline">\(^7\)</span> Fisher went on to assert that the classical two-sample t-test (for independent samples) approximates the randomization test very well. Ernst cites references to several approximations to the randomization tests using classical and computationally tractable methods that have been published over time.<span class="math inline">\(^8\)</span></p>
<div class="line-block">If you have seen two-sample tests previously, it is likely to have been in the context of what Ernst calls the population model, which he distinguishes from the randomization model. In a <strong>population model</strong>, units are selected at random from one or more populations. Most observational studies are population models. One simple case of a population model involves comparing two separate population means. In this case, we can take two independent simple random samples and use the classic two-sample t-test to make the comparison.</div>
<div class="line-block">In a *<strong>randomization model</strong>, a fixed number of experimental units are randomly allocated to treatments. Most experiments are randomization models. In randomization models such as the schistosomiasis example,</div>
<p>the two samples are formed from a collection of available experimental units that are randomly divided into two groups. Since there are a fixed number of units, the groups are not completely independent. For example,
if one of the 10 male mice had a natural resistance to schistosomiasis and was randomly placed in the treatment group, we would expect the control group to have a slightly higher worm count. Since the two groups are not completely independent, the assumptions of the classic two-sample t-test are violated. Even if the sample sizes in the schistosomiasis study were much larger, the randomization test would be a more appropriate test than the two-sample t-test. However, empirical evidence has shown that the two-sample t-test is a very good approximation to the randomization test when sample sizes are large enough. We are fortunate that, in this age of modern computing, we no longer have to routinely compromise by using the t-test to approximate the randomization test.</p>
<p>
</p>
</div>
<div id="wilcoxon-rank-sum-tests-for-two-independent-samples" class="section level2" number="1.12">
<h2>
<span class="header-section-number">1.12</span> <strong>Wilcoxon Rank Sum Tests for Two Independent Samples</strong><a class="anchor" aria-label="anchor" href="#wilcoxon-rank-sum-tests-for-two-independent-samples"><i class="fas fa-link"></i></a>
</h2>
<p>The <strong>Wilcoxon rank sum test</strong>, also called the two-sample <strong>Mann-Whitney test</strong>, makes inferences about the difference between two populations based on data from two independent random samples. This test ranks observations from two samples by arranging them in order from smallest to largest.</p>
<div class="line-block">Focusing on ranks instead of the actual observed values allows us to remove assumptions about the normal distribution. Rank-based tests have been used for many years. However, rank-based methods (discussed</div>
<p>in this section and the next section) are much less accurate than methods based on simulations. In general, randomization tests, permutation tests, or bootstrap methods should be used whenever possible.</p>
<div class="line-block">The following example examines whether pitchers and first basemen who play for National League baseball teams have the same salary distribution. The null and alternative hypotheses are written as,</div>
<p><span class="math inline">\(H_0\)</span>: the distribution of the salaries is the same for pitchers and first basemen
<span class="math inline">\(H_a\)</span>: the distribution of the salaries is different for pitchers and first basemen</p>
<div class="line-block">Table 1.2 shows the salaries of five pitchers and five first basemen who were randomly selected from all National League baseball players. Table 1.3 ranks each of the players based on 2005 salaries.</div>
<div class="line-block">Note that if two players had exactly the same salary, standard practice would be to average the ranks of the tied values.</div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:table2">Table 1.1: </span>Randomly selected pitchers and first baseman from 2005 National League baseball teams.</caption>
<thead><tr class="header">
<th align="left">Team</th>
<th align="left">Position</th>
<th align="left">Name</th>
<th align="right">Salary($)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Milwaukee Brewers</td>
<td align="left">Pitcher</td>
<td align="left">Obermueller, Wes</td>
<td align="right">342000</td>
</tr>
<tr class="even">
<td align="left">Houston Astros</td>
<td align="left">Pitcher</td>
<td align="left">Backe, Brandon</td>
<td align="right">350000</td>
</tr>
<tr class="odd">
<td align="left">Atlanta Braves</td>
<td align="left">Pitcher</td>
<td align="left">Sosa, Jorge</td>
<td align="right">650000</td>
</tr>
<tr class="even">
<td align="left">Atlanta Braves</td>
<td align="left">Pitcher</td>
<td align="left">Thomson, John</td>
<td align="right">4250000</td>
</tr>
<tr class="odd">
<td align="left">Cincinnati Reds</td>
<td align="left">First Baseman</td>
<td align="left">Casey, Sean</td>
<td align="right">7800000</td>
</tr>
<tr class="even">
<td align="left">Arizona Diamondbacks</td>
<td align="left">First Baseman</td>
<td align="left">Green, Shawn</td>
<td align="right">7833333</td>
</tr>
<tr class="odd">
<td align="left">San Diego Padres</td>
<td align="left">First Baseman</td>
<td align="left">Nevin, Phil</td>
<td align="right">9625000</td>
</tr>
<tr class="even">
<td align="left">New York Mets</td>
<td align="left">Pitcher</td>
<td align="left">Glavine, Tom</td>
<td align="right">10765608</td>
</tr>
<tr class="odd">
<td align="left">Colorado Rockies</td>
<td align="left">First Baseman</td>
<td align="left">Helton, Todd</td>
<td align="right">12600000</td>
</tr>
<tr class="even">
<td align="left">Philadelphia Phillies</td>
<td align="left">First Baseman</td>
<td align="left">Thome, Jim</td>
<td align="right">13166667</td>
</tr>
</tbody>
</table></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:table3">Table 1.2: </span>Ranking the 10 randomly selected 2005 National League baseball players.</caption>
<tbody>
<tr class="odd">
<td align="left">Position</td>
<td align="left">Pr</td>
<td align="left">Pr</td>
<td align="left">Pr</td>
<td align="left">Pr</td>
<td align="left">FB</td>
<td align="left">FB</td>
<td align="left">FB</td>
<td align="left">Pr</td>
<td align="left">FB</td>
<td align="left">FB</td>
</tr>
<tr class="even">
<td align="left">Salary</td>
<td align="left">342</td>
<td align="left">350</td>
<td align="left">650</td>
<td align="left">4250</td>
<td align="left">7800</td>
<td align="left">7833</td>
<td align="left">9625</td>
<td align="left">10766</td>
<td align="left">12600</td>
<td align="left">13167</td>
</tr>
<tr class="odd">
<td align="left">Rank</td>
<td align="left">1</td>
<td align="left">2</td>
<td align="left">3</td>
<td align="left">4</td>
<td align="left">5</td>
<td align="left">6</td>
<td align="left">7</td>
<td align="left">8</td>
<td align="left">9</td>
<td align="left">10</td>
</tr>
</tbody>
</table></div>
<p>For the Wilcoxon rank sum test, we define the following terms:</p>
<ul>
<li>
<span class="math inline">\(n_1\)</span> is the sample size for the first group (5 for the pitcher group in this example)</li>
<li>
<span class="math inline">\(n_2\)</span> is the sample size for the second group (5 for the first baseman group in this example)</li>
<li>N= <span class="math inline">\(n_1\)</span> + <span class="math inline">\(n_2\)</span>
</li>
<li>W, the Wilcoxon rank sum statistic, is the sum of the ranks in the first group
(1 + 2 + 3 + 4 + 8 = 18)</li>
</ul>
<p>If the two groups are from the same continuous distribution, then W has a mean,</p>
<p><span class="math display">\[\begin{equation}
  \mu_W = \frac{n_1(N+1)}{2} = \frac{5(11)}{2}=27.5
  \tag{1.3} \label{eq:1_3}
\end{equation}\]</span></p>
<p>and standard deviation<span class="math inline">\(^9\)</span></p>
<p><span class="math display">\[\begin{equation}
  \sigma_W = \sqrt{\frac{n_1n_2(N+1)}{12}} = \sqrt{\frac{(5)(5)(11)}{12}}= 4.787
  \tag{1.4} \label{eq:1_4}
\end{equation}\]</span></p>
<div class="line-block">If W is far from <span class="math inline">\(\mu_W\)</span>, then the Wilcoxon rank sum test rejects the hypothesis that the two populations have identical distributions—that is, rejects <span class="math inline">\(H_0\)</span> (no difference in distribution of salaries) in favor of <span class="math inline">\(H_a\)</span> (salary distributions are different based on position). The p-value is the probability of observing a sample statistic, W, at least as extreme as the one in our sample. Since 18 is less than the hypothesized mean, 27.5, the p-value for the two-sided test in this example is found by calculating 2 * P(W <span class="math inline">\(\leq\)</span> 18).</div>
<p>
</p>
</div>
<div id="extended-activity-wilcoxon-rank-sum-tests" class="section level2 unnumbered">
<h2>Extended Activity: <em>Wilcoxon Rank Sum Tests</em><a class="anchor" aria-label="anchor" href="#extended-activity-wilcoxon-rank-sum-tests"><i class="fas fa-link"></i></a>
</h2>
Data set: <code>NLBB Salaries</code>
<div class="line-block">At first it may seem somewhat surprising that first basemen tend to make more than pitchers. However, in 2005 there were 19 first basemen and 215 pitchers in the National League. Many pitchers did not play much and got paid a low salary, whereas all 19 first basemen were considered quite valuable to their teams.</div>
</div>
<div id="kruskal-wallis-test-for-two-or-more-independent-samples" class="section level2" number="1.13">
<h2>
<span class="header-section-number">1.13</span> <strong>Kruskal-Wallis Test for Two or More Independent Samples</strong><a class="anchor" aria-label="anchor" href="#kruskal-wallis-test-for-two-or-more-independent-samples"><i class="fas fa-link"></i></a>
</h2>
<p>The <strong><em>Kruskal-Wallis</em> test</strong> is another popular nonparametric test that is often used to compare two or more independent samples. Like ANOVA, a more common parametric test that will be discussed in later chapters, the Kruskal-Wallis test requires independent random samples from each population. When the data clearly deviate from the normal distribution, the Kruskal-Wallis test will be more likely than a one-way ANOVA to identify true differences in the population. The null and alternative hypotheses for the Kruskal-Wallis test are:</p>
<p><span class="math inline">\(H_0\)</span>: the distribution of the response variable is the same for all groups
<span class="math inline">\(H_a\)</span>: some responses are systematically higher in some groups than in others</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:table4">Table 1.3: </span>Randomly selected catchers from 2005 National League baseball teams.</caption>
<thead><tr class="header">
<th align="left">Team</th>
<th align="left">Position</th>
<th align="left">Name</th>
<th align="right">Salary($)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Pittsburgh Pirates</td>
<td align="left">Catcher</td>
<td align="left">Ross, David</td>
<td align="right">338500</td>
</tr>
<tr class="even">
<td align="left">Los Angeles Dodgers</td>
<td align="left">Catcher</td>
<td align="left">Phillips, Jason</td>
<td align="right">339000</td>
</tr>
<tr class="odd">
<td align="left">Atlanta Braves</td>
<td align="left">Catcher</td>
<td align="left">Perez, Eddie</td>
<td align="right">625000</td>
</tr>
<tr class="even">
<td align="left">Washington Nationals</td>
<td align="left">Catcher</td>
<td align="left">Bennett, Gary</td>
<td align="right">750000</td>
</tr>
<tr class="odd">
<td align="left">Pittsburgh Pirates</td>
<td align="left">Catcher</td>
<td align="left">Santiago, Benito</td>
<td align="right">2150000</td>
</tr>
</tbody>
</table></div>
<p>The Kruskal-Wallis test is also based on ranks. The ranks are summed for each group, and when these group sums are far apart, we have evidence that the groups are different. While the calculations for the Kruskal-Wallis test statistic are provided here, we suggest using statistical software to conduct this significance test. Continuing the baseball salaries example, Table 1.4 displays salaries of five randomly selected catchers from 2005 National League baseball teams.</p>
<div class="line-block">For the Kruskal-Wallis test, we define the following terms:</div>
<ul>
<li>
<span class="math inline">\(n_1\)</span> is the sample size for the first group (5 for the pitcher group)</li>
<li>
<span class="math inline">\(n_2\)</span> is the sample size for the second group (5 for the first baseman group)</li>
<li>
<span class="math inline">\(n_3\)</span> is the sample size for the third group (5 for the catcher group)</li>
<li>N = <span class="math inline">\(n_1\)</span> + <span class="math inline">\(n_2\)</span> + <span class="math inline">\(n_3\)</span>
</li>
<li>
<span class="math inline">\(R_i\)</span> is the sum of the ranks for the ith group (<span class="math inline">\(R_1\)</span> = 35, <span class="math inline">\(R_2\)</span> = 62, and <span class="math inline">\(R_3\)</span> = 23)</li>
</ul>
<p>The Kruskal-Wallis test statistic is calculated as,</p>
<p><span class="math display">\[\begin{equation}
  H = \frac{12}{N(N + 1)} \sum_{i=1}^k \frac{R_i^2}{n_i} - 3(N + 1) = \frac{12}{(15)(16)}(\frac{35^2}{5}+\frac{62^2}{5}+\frac{23^2}{5}) -3(16) =7.98
  \tag{1.5} \label{eq:1_5}
\end{equation}\]</span></p>
<div class="line-block">The exact distribution of H under the null hypothesis depends on each ni, so it is complex and time consuming to calculate. Even most statistical software packages use the chi-square approximation with I - 1 degrees of freedom to obtain p-values (where I is the number of groups).</div>
<p>
</p>
</div>
<div id="extended-activity-kruskal-wallis-test" class="section level2 unnumbered">
<h2>Extended Activity: <em>Kruskal-Wallis Test</em><a class="anchor" aria-label="anchor" href="#extended-activity-kruskal-wallis-test"><i class="fas fa-link"></i></a>
</h2>
<p>Data set: <code>NLBB Salaries</code></p>
<ol start="28" style="list-style-type: decimal">
<li>Using a software package, run the Kruskal-Wallis test (use all three groups with samples of size 5 per group) to determine if the distribution of salaries differs by position. Create an individual value plot of the data. Do the data look normally distributed in each group?</li>
</ol>
<p>
</p>
<div class="line-block">Nonparametric tests based on rank are usually less powerful (less likely to reject the null hypothesis) than the corresponding parametric tests. Thus, you are less likely to identify differences between groups when they really exist. If you are reasonably certain that the assumptions for the parametric procedure are satisfied, a parametric procedure should be used instead of a rank-based nonparametric procedure. Many introductory texts suggest that, in order to conduct a parametric test, you should have a sample size of 15 in each group and no skewed data or outliers.</div>
</div>
<div id="multiple-comparisons" class="section level2" number="1.14">
<h2>
<span class="header-section-number">1.14</span> <strong>Multiple Comparisons</strong><a class="anchor" aria-label="anchor" href="#multiple-comparisons"><i class="fas fa-link"></i></a>
</h2>
<div class="line-block">In introductory texts, statistical inference is often described in terms of drawing one random sample, performing one significance test, and then stating appropriate conclusions—analysis done, case closed. However, there are many situations where inference is not that simple. Performing multiple statistical tests on the same data set can create several problems.</div>
<div class="line-block">Using a significance level of <span class="math inline">\(\alpha\)</span> = 0.05 (i.e., rejecting <span class="math inline">\(H_0\)</span> in favor of the alternative when the p-value is less than or equal to 0.05) helps to ensure that we won’t make a wrong decision. In other words, one time out of 20 we expect to incorrectly reject the null hypothesis. But what if we want to do 20 or more tests on the</div>
<p>same data set? Does this mean that we’re sure to be wrong at least once? And if so, how can we tell which findings are incorrect? The following activities explore how researchers can protect themselves from drawing conclusions from statistical findings that could be the result of random chance.</p>
</div>
<div id="extended-activitycomparing-car-prices" class="section level2 unnumbered">
<h2>Extended Activity:<em>Comparing Car Prices</em><a class="anchor" aria-label="anchor" href="#extended-activitycomparing-car-prices"><i class="fas fa-link"></i></a>
</h2>

</div>
<div id="extended-activity-the-least-significant-differences-method-and-the-bonferroni-method" class="section level2 unnumbered">
<h2>Extended Activity: <em>The Least-Significant Differences Method and the Bonferroni Method</em><a class="anchor" aria-label="anchor" href="#extended-activity-the-least-significant-differences-method-and-the-bonferroni-method"><i class="fas fa-link"></i></a>
</h2>
<p>Data set: <code>Car1</code></p>
<p>When the significance level is controlled for each individual test, as was done in Question 29, the process is often called the <strong>least-significant differences method (LSD)</strong>. Notice that using a = 0.05 for all tests has some undesirable properties, especially when a large number of tests being conducted. If 100 independent tests were conducted to compare multiple groups (and there really were no differences), the probability of incorrectly rejecting at least one test would be 1 - 0.95<span class="math inline">\(^{100}\)</span> = 0.994. Thus, using <span class="math inline">\(\alpha\)</span>= 0.05 as a critical value for 100 comparisons will almost always lead us to incorrectly conclude that some results are significantly different.</p>
<div class="line-block">One technique that is commonly used to address the problem with multiple comparisons is called the *<strong>Bonferroni method</strong>. This technique protects against the probability of false rejection by using a cutoff value of <span class="math inline">\(\alpha\)</span>/K, where K is the number of comparisons. In Question 29, there are three comparisons (i.e.,three hypothesis tests). Thus, a cutoff value of 0.05/3 = 0.01667 should be used. In other words, when there are three comparisons as in Question 29, the Bonferroni method rejects the null hypothesis when the p-value is less than or equal to 0.01667. Using the least-significant differences method (<span class="math inline">\(\alpha\)</span> = 0.05), as was done in Question 29, we would conclude that the prices of Buicks and Chevrolets are significantly different, but using the Bonferroni method we would fail to reject in all three tests.</div>
<p>
</p>
</div>
<div id="choosing-a-critical-value" class="section level2 unnumbered">
<h2>
<strong>Choosing a Critical Value</strong><a class="anchor" aria-label="anchor" href="#choosing-a-critical-value"><i class="fas fa-link"></i></a>
</h2>
<p>The <span class="math inline">\(\alpha\)</span>-level represents the probability of a <strong>type I error</strong>. A type I error can be considered a false alarm: Our hypothesis test has led us to conclude that we have found a significant difference when one does not exist. However, it is important to recognize that it is also possible to make a <strong>type II error</strong>, which means our hypothesis
test failed to detect a significant difference when one exists. In essence, a type II error can be thought of as an alarm that failed to go off.</p>
<div class="line-block">Notice that if the Bonferroni method is used with all six tests, the critical value for each individual test is 0.05/6 = 0.00833. Thus, this method often fails to detect real differences between groups, leaving us open to a high rate of type II error while protecting us against type I errors.</div>
<div class="line-block">Neither the least-significant differences nor the Bonferroni method is ideal. Caution should be used with both techniques, and neither technique should be used with numerous comparisons. The key is to recognize the benefits and limitations of each technique and to properly interpret what the results of each technique tell us. Some researchers suggest limiting the number of tests, using both techniques, and letting the reader decide</div>
<p>which conclusions to draw. Both techniques are commonly used when there are fewer than 10 comparisons. However, a researcher should always decide which comparisons to test before looking at the data.</p>
<hr>
</div>
<div id="chapter-summary" class="section level2 unnumbered">
<h2>
<strong>Chapter Summary</strong><a class="anchor" aria-label="anchor" href="#chapter-summary"><i class="fas fa-link"></i></a>
</h2>
<p>This chapter described the basic concepts behind randomization tests, permutation tests, bootstrap methods, and rank-based nonparametric tests. <strong>Parametric tests</strong> (such as z-tests, t-tests or F-tests) assume that data follow a known a probability distribution or use the central limit theorem to make inferences about a population. *<strong>Nonparametric tests</strong> do not require assumptions about the distribution of the population or the central limit theorem in order to make inferences about a population.</p>
<div class="line-block">The *<strong>null hypothesis</strong>, denoted <span class="math inline">\(H_0\)</span>, states that in a study nothing is creating group differences except the random allocation process. The research hypothesis is called the <strong>alternative hypothesis</strong> and is denoted <span class="math inline">\(H_a\)</span> (or <span class="math inline">\(H_1\)</span>). The p-value is the likelihood of observing a statistic at least as extreme as the one observed</div>
<p>from the sample data when the null hypothesis is true. A threshold value, called a <strong>significance level</strong>, is denoted by the Greek letter alpha (<span class="math inline">\(\alpha\)</span>). When a study’s p-value is less than or equal to this significance level, we state that the results are <strong>statistically significant at level <span class="math inline">\(\alpha\)</span></strong>. Exact p-values are often difficult to calculate, but *<strong>empirical p-values</strong> can often be simulated through a randomization or permutation test. The empirical p-value will become more precise as the number of randomizations within a simulation study
increases.</p>
<div class="line-block">The steps in a <strong>randomization test</strong> are as follows:</div>
<ul>
<li>An experiment is conducted in which units are assigned to a treatment and an observed sample statistic is calculated (such as the difference between group means).</li>
<li>Software is used to simulate the random allocation process a number of times (N iterations).</li>
<li>For each iteration, the statistic of interest (difference between group means) is recorded, with X being
the number of times the statistic in the iteration exceeds or is the same as the observed statistic in the
actual experiment.</li>
<li>X/N is computed to find the p-value, the proportion of times the statistic exceeds or is the same as the
observed difference.</li>
</ul>
<p>A *<strong>permutation test</strong> is a more general form of the randomization test. The steps in both tests are identical, except that permutation tests do not require random allocation. Randomization tests and permutation tests can provide very accurate results. These tests are preferred over parametric methods when the sample size is small or when there are outliers in a data set. Since real data sets tend not to come from exactly normal populations, it is important to recognize that even p-values from parametric tests are approximate (but typically accurate as long as the sample sizes are large enough, the data are not skewed, there are no outliers, and the data are reasonably normal). A graph such as a boxplot or individual value plot should always be created to determine if parametric methods are appropriate. Randomization tests are gaining popularity because they require fewer assumptions and are just as powerful as parametric tests.</p>
<div class="line-block">Bootstrap methods take many (at least 1000) resamples with replacement of the original sample to create</div>
<p>a bootstrap distribution. If the bootstrap distribution is symmetric and unbiased, bootstrap t or bootstrap
percentile confidence intervals can be used to approximate 100(1 - <span class="math inline">\(\alpha\)</span>)%, confidence intervals.</p>
<p>The steps in creating <strong>bootstrap confidence intervals</strong> are as follows:</p>
<ul>
<li>One sample of size n is taken from a population and the statistic of interest is calculated.</li>
<li>Software is used to take resamples (with replacement) of size n from the original sample a number of
times (N iterations). For each iteration, the statistic of interest is calculated from the resample.</li>
<li>The <strong>bootstrap distribution</strong>, which is the distribution of all N resample statistics, is used to estimate
the shape and spread of the sampling distribution.</li>
<li>A <strong>bootstrap t confidence interval</strong> is found by calculating <span class="math inline">\(\bar{x}\)</span> <span class="math inline">\(\pm t^*(S^*)\)</span> where <span class="math inline">\(S^*\)</span> is the standard
deviation of the bootstrap distribution and <span class="math inline">\(t^*\)</span> is the critical value of the t(n - 1) distribution with
100(1 - <span class="math inline">\(\alpha\)</span>)%, of the area between - t* and t*.</li>
<li>A 100(1 - <span class="math inline">\(\alpha\)</span>), bootstrap percentile confidence interval is found by taking the <span class="math inline">\(\alpha\)</span> / 2 * 100
percentile of each tail of the bootstrap distribution.</li>
</ul>
<div class="line-block">Bootstrap confidence intervals based on small samples can be unreliable. The bootstrap t or percentile confidence interval may be used if,</div>
<ul>
<li>the bootstrap distribution does not appear to be biased,</li>
<li>the bootstrap distribution appears to be normal, and</li>
<li>the bootstrap t and percentile confidence intervals are similar.</li>
</ul>
<div class="line-block">Simulation studies can easily be extended to testing other terms, such as the median or variance, whereas most parametric tests described in introductory statistics classes (such as the z-test and t-test) are restricted to testing for the mean. Simulation studies are an extremely useful tool that can fairly easily be used to calculate accurate p-values for research hypotheses when other tests are not appropriate.</div>
<div class="line-block">Before computationally intensive techniques were easily available, rank-based nonparametric tests, such</div>
<p>as the <strong>Wilcoxon rank sum</strong> test and the <strong>Kruskal-Wallis test</strong>, were commonly used. These tests do not
require assumptions about distributions, but they tend to be less informative because ranks are used instead
of the actual data. Both the Mann-Whitney test and the Kruskal-Wallis test assume that sample data are
from independent random samples whose distributions have the same shape and scale. Each sample in the
Kruskal-Wallis test should consist of at least five measurements. Rank-based nonparametric tests tend to be
less powerful (less likely to identify differences between groups) than parametric tests (when assumptions do
hold) and resampling methods. When the sample sizes are small and there are reasons to doubt the normality
assumption, rank-based nonparametric tests are recommended over parametric tests. Randomization tests and
permutation tests are typically preferred over parametric and rank-based tests. Their p-values are often more
reliable, and they are more flexible in the choice of parameter tested.</p>
<div class="line-block">One final note of caution: Even though it is possible to analyze the same data with a variety of parametric</div>
<p>and nonparametric techniques, statisticians should never search around for a technique that provides the
results they are looking for. Conducting multiple tests on the same data and choosing the test that provides
the smallest p-value will cause the results to be unreliable. If possible, determine the type of analysis that will
be conducted before the data are collected.</p>
</div>
<div id="exercises" class="section level2 unnumbered">
<h2>
<strong>Exercises</strong><a class="anchor" aria-label="anchor" href="#exercises"><i class="fas fa-link"></i></a>
</h2>


</div>
</div>

  <div class="chapter-nav">
<div class="empty"></div>
<div class="empty"></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#an-introduction-to-nonparametric-methods-schistosomiasis"><span class="header-section-number">Chapter 1</span> An Introduction to Nonparametric Methods: Schistosomiasis</a></li>
<li><a class="nav-link" href="#investigation-can-a-new-drug-reduce-the-spread-of-schistosomiasis"><span class="header-section-number">1.1</span> Investigation: Can a New Drug Reduce the Spread of Schistosomiasis?</a></li>
<li><a class="nav-link" href="#activity-describing-the-data">Activity: Describing the Data</a></li>
<li><a class="nav-link" href="#statistical-inference-through-a-randomization-test"><span class="header-section-number">1.2</span> Statistical Inference Through a Randomization Test</a></li>
<li><a class="nav-link" href="#activity-conducting-a-randomization-test-by-hand">Activity: Conducting a Randomization Test by Hand</a></li>
<li><a class="nav-link" href="#performing-a-randomization-test-using-a-computer-simulation"><span class="header-section-number">1.3</span> Performing a Randomization Test Using a Computer Simulation</a></li>
<li><a class="nav-link" href="#activity-using-computer-simulations-to-conduct-a-hypothesis-test">Activity: Using Computer Simulations to Conduct a Hypothesis Test</a></li>
<li><a class="nav-link" href="#two-sided-tests"><span class="header-section-number">1.4</span> Two-Sided Tests</a></li>
<li><a class="nav-link" href="#activity-a-two-sided-hypothesis-test">Activity: A Two-Sided Hypothesis Test</a></li>
<li><a class="nav-link" href="#what-can-we-conclude-from-the-schistosomiasis-study"><span class="header-section-number">1.5</span> What Can We Conclude from the Schistosomiasis Study?</a></li>
<li><a class="nav-link" href="#a-closer-look-nonparametric-methods">A Closer Look: Nonparametric Methods</a></li>
<li><a class="nav-link" href="#permutation-tests-versus-randomization-tests"><span class="header-section-number">1.6</span> Permutation Tests versus Randomization Tests</a></li>
<li><a class="nav-link" href="#age-discrimination-study">Age Discrimination Study</a></li>
<li><a class="nav-link" href="#extended-activity-is-there-evidence-of-age-discrimination">Extended Activity: Is There Evidence of Age Discrimination?</a></li>
<li><a class="nav-link" href="#permutation-and-randomization-tests-for-matched-pairs-designs"><span class="header-section-number">1.7</span> Permutation and Randomization Tests for Matched Pairs Designs</a></li>
<li><a class="nav-link" href="#music-and-relaxation">Music and Relaxation</a></li>
<li><a class="nav-link" href="#extended-activity-testing-the-effect-of-music-on-relaxation">Extended Activity: Testing the Effect of Music on Relaxation</a></li>
<li><a class="nav-link" href="#the-bootstrap-distribution"><span class="header-section-number">1.8</span> The Bootstrap Distribution</a></li>
<li><a class="nav-link" href="#extended-activity-creating-a-sampling-distribution-and-a-bootstrap-distribution">Extended Activity: Creating a Sampling Distribution and a Bootstrap Distribution</a></li>
<li><a class="nav-link" href="#using-bootstrap-methods-to-create-confidence-intervals"><span class="header-section-number">1.9</span> Using Bootstrap Methods to Create Confidence Intervals</a></li>
<li><a class="nav-link" href="#bootstrap-t-confidence-intervals-"><span class="header-section-number">1.10</span> *Bootstrap t Confidence Intervals{-}</a></li>
<li><a class="nav-link" href="#bootstrap-percentile-confidence-intervals">Bootstrap Percentile Confidence Intervals</a></li>
<li><a class="nav-link" href="#when-to-use-bootstrap-confidence-intervals">When to Use Bootstrap Confidence Intervals</a></li>
<li><a class="nav-link" href="#extended-activityestimating-salaries-of-medical-faculty">Extended Activity:Estimating Salaries of Medical Faculty</a></li>
<li><a class="nav-link" href="#relationship-between-the-randomization-test-and-the-two-sample-t-test"><span class="header-section-number">1.11</span> Relationship Between the Randomization Test and the Two-Sample t-Test</a></li>
<li><a class="nav-link" href="#wilcoxon-rank-sum-tests-for-two-independent-samples"><span class="header-section-number">1.12</span> Wilcoxon Rank Sum Tests for Two Independent Samples</a></li>
<li><a class="nav-link" href="#extended-activity-wilcoxon-rank-sum-tests">Extended Activity: Wilcoxon Rank Sum Tests</a></li>
<li><a class="nav-link" href="#kruskal-wallis-test-for-two-or-more-independent-samples"><span class="header-section-number">1.13</span> Kruskal-Wallis Test for Two or More Independent Samples</a></li>
<li><a class="nav-link" href="#extended-activity-kruskal-wallis-test">Extended Activity: Kruskal-Wallis Test</a></li>
<li><a class="nav-link" href="#multiple-comparisons"><span class="header-section-number">1.14</span> Multiple Comparisons</a></li>
<li><a class="nav-link" href="#extended-activitycomparing-car-prices">Extended Activity:Comparing Car Prices</a></li>
<li><a class="nav-link" href="#extended-activity-the-least-significant-differences-method-and-the-bonferroni-method">Extended Activity: The Least-Significant Differences Method and the Bonferroni Method</a></li>
<li><a class="nav-link" href="#choosing-a-critical-value">Choosing a Critical Value</a></li>
<li><a class="nav-link" href="#chapter-summary">Chapter Summary</a></li>
<li><a class="nav-link" href="#exercises">Exercises</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Book Chapter Example</strong>" was written by Your Name. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
