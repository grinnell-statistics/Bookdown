---
title: "Ch7"
output:
  pdf_document: default
  keep_tex: true
  html_document: default
header-includes:
  \usepackage{float}
date: "2025-07-11"
---
---
output:
  pdf_document: default
  html_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(knitr)
library(kableExtra)
# switch ggplot text to Latin Modern Roman
theme_set(
  theme_minimal(base_family = "Latin Modern Roman") +
    theme(
      plot.title = element_text(size = 24, face = "bold"),
      axis.title = element_text(size = 12),
      plot.caption = element_text(size = 10, hjust = 0)
    )
)
```

# Logistic Regression: The Space Shuttle Challenger

*The best thing about being a statistician is that you get to play in everyone’s backyard.*  
—John Tukey$^1$

There are many investigations where a researcher is interested in developing a regression model when the response variable is dichotomous (has only two categories). Dichotomous responses can be represented with binary data (data with values of only zero or one). Logistic regression is used to examine the relationship between one or more explanatory variables and a binary response variable.

In this chapter, we will look at several studies, including O-ring failure data provided by the National Aeronautics and Space Administration (NASA) after the space shuttle Challenger disaster, in order to introduce the following logistic regression techniques:

* Calculating and interpreting the logistic regression model  
* Using the Wald statistic and likelihood ratio tests to determine the significance of individual explanatory variables  
* Calculating the log-odds function and maximum likelihood estimates  
* Conducting goodness-of-fit tests to evaluate model appropriateness  
* Assessing regression model performance by looking at a classification table, showing correct and incorrect classification of the response variable  
* Extending logistic regression to cases with multiple explanatory variables  

## **Investigation: Did Temperature Influence the Likelihood of an O-Ring Failure?**

On January 28, 1986, the NASA space shuttle program launched its 25th shuttle flight from Kennedy Space Center in Florida. Seventy-three seconds into the flight, the external fuel tank collapsed and spilled liquid oxygen and hydrogen. These chemicals ignited, destroying the shuttle and killing all seven crew members on board. Reports to President Reagan and videos of the event are available at the Kennedy Space Center website.*  

Investigations showed that an O-ring seal in the right solid rocket booster failed to isolate the fuel supply. Figure 7.1 shows the space shuttle Challenger just after ignition with the fuel tank and two 149.16-foot-long solid rocket boosters. Figure 7.2 shows a diagram of a solid rocket booster. Because of its size, the rocket boosters were built and shipped in separate sections. A forward, center and aft field joint connected the sections. Two O-rings (one primary and one secondary), which resemble giant rubber bands 0.28 inch thick but 37 feet in diameter, were used to seal the field joints between each of the sections.

An O-ring seal was used to stop the gases inside the solid rocket booster from escaping. However, the cold outside air temperature caused the O-rings to become brittle and fail to seal properly. Gases at 5800 °F escaped and burned a hole through the side of the rocket booster.

[[[Figure 7.1]]]
Picture of the space shuttle Challenger just after ignition. Each solid rocket booster had six O-rings, two at each field joint. The O-rings at the right aft field joint failed.

*The Report of the Presidential Commission on the Space Shuttle Challenger Accident*, also known as the Rogers’ Commission Report, states:

> ““O-ring resiliency is directly related to its temperature. . . . A warm O-ring that has been compressed
will return to its original shape much quicker than will a cold O-ring when compression is relieved.
. . . A compressed O-ring at 75 degrees Fahrenheit is five times more responsive in returning to its
uncompressed shape than a cold O-ring at 30 degrees Fahrenheit. . . . At the cold launch temperature
experienced, the O-ring would be very slow in returning to its normal rounded shape. . . . It would
remain in its compressed position in the O-ring channel and not provide a space between itself and the
upstream channel wall. Thus, it is probable the O-ring would not . . . seal the gap in time to preclude
joint failure due to blow-by and erosion from hot combustion gases. . . . Of 21 launches with ambient 
temperatures of 61 degrees Fahrenheit or greater, only four showed signs of O-ring thermal distress:
i.e., erosion or blow-by and soot. Each of the launches below 61 degrees Fahrenheit resulted in one
or more O-rings showing signs of thermal distress.”$^2$

[[[Figure 7.2]]]
Diargam of a solid rocket booster

A lamentable aspect of this disaster is that the problem with the O-rings was already understood by
some engineers prior to the Challenger launch. In February 1984, the Marshall Configuration Control
Board sent a memo about the O-ring erosion that occurred on STS 41-B (the 10th space shuttle flight and
the 4th mission for the Challenger shuttle). Messages continued to increase in intensity, as evidenced by
a 1985 internal memo from Thiokol Corporation, the company that designed the O-ring. Employees from
Thiokol wrote the following to their Vice President of Engineering: “This letter is written to ensure that
management is fully aware of the seriousness of the current O-Ring erosion problem in the SRM joints
from an engineering standpoint.”$^3$

With the temperature on January 28, 1986, expected to be 31°F, Thiokol Corporation recommended
against the Challenger launch. However, this flight was getting significant publicity because a high school
teacher, Christa McAuliffe, was on the flight. The flight had already been delayed several times, and there
was no quick solution to the O-ring concern. The engineers were overruled, and the decision was made to go
ahead with the launch. The eventual presidential investigation stated,
>“The decision to launch the Challenger was flawed. Those who made that decision were unaware of
the recent history of problems concerning the O-rings and the joint and were unaware of the initial
written recommendation of the contractor advising against the launch at temperatures below 53
degrees Fahrenheit and the continuing opposition of the engineers at Thiokol after the management
reversed its position. They did not have a clear understanding of Rockwell’s concern that it was not
safe to launch because of ice on the pad. If the decision makers had known all of the facts, it is highly
unlikely that they would have decided to launch 51-L on January 28, 1986.”$^4$

It seems that even though some engineers did comprehend the severity of the problem, they were unable
to properly communicate the results. Prior to the ill-fated Challenger flight, the solid rocket boosters for 24
shuttle launches were recovered and inspected for damage. Even though O-ring damage was present in some
of the flights, the O-rings were not damaged enough to allow any gas to escape. Since damage was very
minimal, all 24 prior flights were considered a success by NASA.

```{r tab7.1, echo=FALSE, results='asis'}
# Table 7.1: O‑ring damage on 24 space shuttle launches
df7.1 <- data.frame(
  `Flight Number` = 1:24,
  Date = c(
    "4/12/1981", "11/12/1981", "3/22/1982", "6/27/1982",
    "11/11/1982", "4/4/1983",  "6/18/1983", "8/30/1983",
    "11/28/1983","2/3/1984",  "4/6/1984",  "8/30/1984",
    "10/5/1984", "11/8/1984", "1/24/1985","4/12/1985",
    "4/29/1985","6/17/1985","7/29/1985","8/27/1985",
    "10/3/1985","10/30/1985","11/26/1985","1/12/1986"
  ),
  `Ambient Temperature (°F)` = c(
    66,70,69,80,68,67,72,73,70,57,63,70,78,67,53,67,75,70,81,76,79,75,76,58
  ),
  `Successful Launch` = c(
    1,0,1,"*",1,1,1,1,1,0,0,0,1,1,0,1,1,1,1,1,1,0,1,0
  ),
  check.names = FALSE,
  stringsAsFactors = FALSE
)
cat(
  kable(
    df7.1,
    format = "latex",
    booktabs = TRUE,
    escape = FALSE,
    caption  = "Table 7.1 O‑ring damage on 24 space shuttle launches.",
    align = c("c","c","c","c")
  ) %>%
  column_spec(1, width = "2cm")  %>%
  column_spec(3, width = "3cm")  %>%
  column_spec(4, width = "2cm")  %>%
  kable_styling(latex_options="hold_position"),
  "\n\n* Flight 4 is a missing data point because the rockets were lost at sea.\n"
)
```

Table 7.1 shows the temperature at the time of each launch and whether any damage was visible in any of the O‑rings. In this chapter, we will define a successful launch as one with no evidence of any O‑ring damage. In Table 7.1, **Successful Launch** is a categorical variable, with 0 representing a launch where O‑ring damage occurred and 1 indicating a successful launch with no O‑ring damage. Throughout the rest of this investigation, the relatively small data set in Table 7.1 will be used to demonstrate techniques that can be used to determine if the likelihood of O‑ring damage is related to temperature.

## Activity: Describing the Data{-}
>1. Based on the description of the Challenger disaster O‑ring concerns, identify which variable in the space shuttle data set in Table 7.1 should be the explanatory variable and which should be the response variable.  
2. Imagine you were an engineer working for Thiokol Corporation prior to January 1986. Create a few graphs of the data in Table 7.1. Is it obvious that temperature is related to the success of the O‑rings? Submit any charts or graphs you have created that show a potential relationship between temperature and O‑ring damage.  

In this chapter, we will develop a regression model using a binary response variable, Successful Launch. For
the space shuttle data set, y = 1 represents a successful flight with no O-ring damage and y = 0 represents a
flight with some O-ring damage. Binary response data occur in many fields; for example, we may want to know

* whether a disease is present or absent
* whether or not a person is a good credit risk for a loan
* whether or not a high school student should be admitted to a particular college
* whether or not an individual is involved in substance abuse

The next section describes why the least squares regression model is not appropriate when the response is
binary. Logistic regression is used to examine the relationship between one or more explanatory variables
and a binary response variable. Like other regression models, logistic regression models often have explana-
tory variables that are quantitative, but they can be categorical as well

## **Review of the Least Squares Regression Model**

In Chapters 2 and 3, you saw that the ordinary least squares regression model has the form
\begin{align}
y_i &= \beta_0 + \beta_1 x_i + \epsilon_i
\quad\text{for } i = 1, 2, 3, \dots, n
\tag{7.1}
\end{align}

where $n$ is the number of observations, $y_i$ is the $i$th value of a \textit{continuous response variable}, $\beta_0$ and $\beta_1$ are regression coefficients, $x_i$ is the $i$th value of the explanatory variable, and $\epsilon_i$ represents normally distributed errors with a constant variance. Equation (7.2) states that the mean response (the expected response at each particular $x_i$) is equal to the linear predictor $\beta_0 + \beta_1 x_i$ for each observed value $x_i$:

\begin{align}
E(Y_i \mid x_i) &= \beta_0 + \beta_1 x_i 
\quad\text{for } i = 1, 2, 3, \dots, n
\tag{7.2}
\end{align}

where $\beta_0$ and $\beta_1$ are parameters that can be estimated with sample data. In addition to assuming that the regression model has a linear predictor, we assume that the error terms in the least squares regression model are independent and follow the normal distribution with a zero mean and a fixed standard deviation:

\begin{align}
\epsilon_i &\overset{\mathrm{iid}}{\sim} N(0, \sigma^2) 
\quad\text{for } i = 1, 2, 3, \dots, n
\tag{7.3}
\end{align}

Equation (7.3) states that each independent and identically distributed error term follows a normal probability
distribution that is centered at zero and has a constant variance.

\large
\textbf{NOTE:} 
When there is only one explanatory variable, as in Equation (7.1), ordinary least squares regression is often called simple linear regression. As shown in Chapter 3, the model is called least squares regression because the line minimizes the sum of the squared residuals (the difference between an observed value and the expected response). Least squares estimates for $\beta_0$ and $\beta_1$ (represented as $b_0 = \hat\beta_0$ and $b_1 = \hat\beta_1$) can be calculated even when the normality and equal variance assumptions are violated. However, these assumptions about the error terms are needed to conduct hypothesis tests and construct confidence intervals for $\beta_0$ and $\beta_1$.
\normalsize

## Activity: Building a Least Squares Regression Model{-}
>3. Use the data in Table 7.1 to create a scatterplot with a least squares regression line for the space shuttle data. Calculate the predicted response values $\hat y = b_0 + b_1 x$ when the temperature is 60°F and when the temperature is 85°F.

## **The Logistic Regression Model**

When the response variable is binary, the response is typically defined as a probability of success, instead of 0 or 1. For example, in Question 3, when the temperature is 60°F, the least squares regression line estimates that the probability of a successful launch is 0.338. The expected response at each particular $x_i$ is defined as

\begin{align}
\pi_i &= P(Y_i = 1) = \text{probability that a launch has no O-ring damage at temperature } x_i \notag \\
      &= E(Y_i \mid x_i) \notag \\
      &= \beta_0 + \beta_1 x_i \quad \text{for } i = 1,2,3,\dots,n \tag{7.4}
\end{align}

While the linear model ($\beta_0 + \beta_1 x_i$) in Equation (7.4) is simple, it is not appropriate to use, since probabilities must be between 0 and 1. For example, with a temperature value $x_i = 50$, the least squares regression model in Question 3 would predict a probability of $-0.036$. In order to restrict the predictions to values between 0 and 1, an S-shaped function called the log-odds function will be used.

Logistic regression uses the following model to fit an S-shaped relationship between $\pi$ and $x$:

\begin{align}
\ln\bigl(\frac{\pi_i}{1 - \pi_i}\bigr) &= \beta_0 + \beta_1 x_i 
\tag{7.5}
\end{align}

where ln represents the natural log, $\beta_0$ and $\beta_1$ are regression parameters, and $\pi_i$ is the probability of a successful launch for a given temperature ($x_i$). The ratio $\pi/(1 - \pi)$ is called the odds, the probability of success over the probability of failure. Thus, the function ln[$\pi/(1 - \pi)$] is called the log-odds of $\pi$ or the logistic or logit transformation of $\pi$.* Figure 7.3 shows both the least squares regression model and the logistic regres-
sion model for the space shuttle data.

\large
\textbf{MATHEMATICAL NOTE:}
In Chapter 6, the odds of an outcome are defined as $\pi/(1 - \pi)$, the probability of a success (no O-ring damage) over the probability of a failure (O-ring damage). For example, if a computer randomly selects a day of the week, the odds of selecting Saturday (Saturday is considered a success) are 1 to 6, since

\begin{align}
\text{odds} = \frac{\pi}{1 - \pi} = \frac{1/7}{(1 - (1/7))} = \frac{1}{6}.
\notag
\end{align}

Similarly, the odds are 6 to 1 against Saturday being selected (any day but Saturday is a success).
\normalsize

*?Throughout this chapter, we will use terms such as \textit{log-odds} or \textit{log-likelihood}, but we actually use natural logs (ln) in
our calculations.

Equation (7.5) can be solved for $\pi_i$ to show that

\begin{align}
\pi_i &= \frac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}} \tag{7.6}
\end{align}

\large
\textbf{MATHEMATICAL NOTE:}
Binary logistic regression assumes that for each $x_i$ value, the response variable $Y_i$ follows a Bernoulli distribution (described in the extended activities). This means we assume that (1) each $Y_i$ is independent, (2) each $Y_i$ falls into exactly one of two categories represented by either a zero or a one, and (3) for each $x_i$, $P(Y_i = 1) = \pi_i$ and $P(Y_i = 0) = 1 - \pi_i$ (more specifically written as $P(Y_i = 1 \mid x_i) = \pi_i$ and $P(Y_i = 0 \mid x_i) = 1 - \pi_i$). This third assumption states that for any given explanatory variable (a specific temperature value), the probability of success (no O-ring failures) is constant.
\normalsize 

It is possible to use least squares regression techniques to estimate $\beta_0$ and $\beta_1$ in logistic regression models. However, the assumptions needed for hypothesis tests and confidence intervals using the ordinary least squares regression model are not met. Specifically, even after the log-odds transformation, Figure 7.3 demonstrates that the residuals are not normally distributed and the variability of the residuals depends on the explanatory variable.

Residuals (observed values minus expected values) are used to estimate the error terms. Visual inspection of residual plots is often used to check for normality. Recall from previous work in regression that if the residuals are normally distributed, the scatterplot of the residuals versus the explanatory variable should resemble a randomly scattered oval of points. For example, it should resemble the random scatter you would see if you happened to drop 23 coins (one for each residual value).

In logistic regression, the residuals are $y_i - \hat\pi_i$. If the observed response $y_i = 0$, then the residual value is $-\hat\pi_i$. If the observed response $y_i = 1$, then the residual value is $1 - \hat\pi_i$. This leads to the two curves shown in Figure 7.4. When the temperature is low in the space shuttle data (around 55°F, as seen in Figure 7.3), the observed responses tend to be zero and the predicted responses ($\hat\pi_i$’s) are small positive numbers. Thus, the residual values ($-\hat\pi_i$’s) are negative and close to zero. When the temperature is high, the observed responses tend to be one, the predicted responses are close to one, and the residual values are positive and close to zero.

\Large
\textbf{\textcolor{red}{Key Concept:}}
\color{red}
When the response in a regression model is binomial, $\pi_i = P(Y_i = 1)$ is the probability of a success (a launch has no O-ring damage at temperature $x_i$). In simple linear regression models with a binomial response,

\begin{align}
y_i &= \pi_i + \epsilon_i = \beta_0 + \beta_1 x_i + \epsilon_i \quad \text{for } i = 1,2,3,\dots,n \tag{7.7}
\end{align}

With the logit transformation, logistic regression models with a binomial response have the following form:

\begin{align}
y_i &= \pi_i + \epsilon_i = \frac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}} + \epsilon_i \quad \text{for } i = 1,2,3,\dots,n \tag{7.8}
\end{align}

While the logit transformation results in a nice S-shaped curve, the error terms in Equations (7.7) and (7.8) are not constant and are not normally distributed. Thus, hypothesis tests and confidence intervals cannot be calculated using least squares regression.
\color{black}
\normalsize

## **The Logistic Regression Model Using Maximum Likelihood Estimates**

Logistic regression is a special case of what is known as a generalized linear model. Generalized linear models expand linear regression models to cases where the normal assumptions do not hold. All generalized linear models have three components:

\begin{itemize}
  \item \textbf{A linear predictor:} In the shuttle example, there is only one explanatory variable, $x = \text{Temperature}$, so the linear predictor in Equation (7.5) is $\beta_0 + \beta_1 x$. However, just as in other multiple regression models, the linear predictor can include many explanatory variables, including indicator variables and interaction terms as well as other transformed variables.
  \item \textbf{A random component:} Each error term is assumed to be independent. However, in generalized linear models, the error terms are not required to follow a normal distribution. In addition, generalized linear models do not require that the variability of the error terms be constant.
  \item \textbf{A link function:} A link function is a function that fits the expected response value to a linear predictor. In Equation (7.5), the link function is the log-odds function, $\ln\bigl[\pi/(1 - \pi)\bigr]$. The link function depends on the distribution of the response variable. In logistic regression, the response variable is binary. Other link functions for binary response data do exist, but the log-odds function is the most common because it is the easiest to interpret.
\end{itemize}

Generalized linear models can also be used when the response variable follows other distributions. For example, $y$ may follow a Poisson or gamma distribution. Textbooks on generalized linear models derive link functions for each of these types of response variables. In least squares regression where the response has a normal distribution, as in Equation (7.1), the link function is simply the identity function. In other words, the response needs no transformation in simple linear regression models.

Clearly the logistic regression model in Figure 7.3 is nonlinear. So it may seem somewhat surprising to consider logistic regression as a generalized linear model. The reason we still call this model linear is that the link function, the log-odds transformation, is modeled with a linear predictor, $\beta_0 + \beta_1 x$.

Instead of using least squares estimates, generalized linear models use the method of maximum likelihood to estimate the coefficients $\beta_0$ and $\beta_1$. The extended activities provide more detail on calculating maximum likelihood estimates in logistic regression. In the space shuttle example, we will simply use a computer software package to find maximum likelihood estimates of $\beta_0$ and $\beta_1$.

\large
\textbf{NOTE:} In least squares regression, we often transform the response variable ($y$) so that the data fit model assumptions. In addition to linearizing data, transforming $y$ impacts the variability and the distribution of the error terms. In generalized linear models, the link function transforms the expected response ($\pi$) to fit a linear predictor. For those who have had calculus, link functions are also differentiable and invertible.
\normalsize

## Activity: Using Software to Calculate Maximum Likelihood Estimates {-}
>4. Solve Equation (7.5) for $\pi_i$ to show that Equation (7.6) is true.  
>5. Use Equation (7.6) to create six graphs. In each graph, plot the explanatory variable ($x$) versus the expected probability of success ($\pi$) using $\beta_0 = -10$ and $-5$ and $\beta_1 = 0.5$, $1$, and $1.5$. Repeat the process for $\beta_0 = 10$ and $5$ and $\beta_1 = -0.5$, $-1$, and $-1.5$.  
   a. Do not submit the graphs, but explain the impact of changing $\beta_0$ and $\beta_1$.  
   b. For all of these graphs, what value of $\pi$ appears to have the steepest slope?  
>6. Use statistical software to calculate the maximum likelihood estimates of $\beta_0$ and $\beta_1$. Compare the maximum likelihood estimates to the least squares estimates in Question 3.

Figure 7.3 shows a logistic regression model using maximum likelihood estimates of $\beta_0$ and $\beta_1$. Using Equation (7.6) and the maximum likelihood estimates from Question 6, we can estimate the probability that a launch has no O-ring damage at temperature $x_i$:

\begin{align}
\hat\pi_i &= \frac{e^{b_0 + b_1 x_i}}{1 + e^{b_0 + b_1 x_i}} 
           = \frac{e^{-15.043 + 0.232 x_i}}{1 + e^{-15.043 + 0.232 x_i}}
           \quad \text{for } i = 1,2,3,\dots,n \tag{7.9}
\end{align}

Notice that $\pi$ in Equation (7.6) has been replaced by $\hat\pi$ in Equation (7.9) because the parameters in the linear regression model ($\beta_0$ and $\beta_1$) have been estimated with our sample data; $b_0 = \hat\beta_0 = -15.043$ and $b_1 = \hat\beta_1 = 0.232$.

### Activity: Estimating the Probability of Success with Maximum Likelihood Estimates
>7. Use Equation (7.9) to predict the probability that a launch has no O-ring damage when the temperature is 31°F, 50°F, and 75°F.

At this point, it seems reasonable to question why the O-rings were not considered a higher risk at the time of the 1986 Challenger launch. After all, the odds of a successful launch (no O-ring damage) at the expected temperature of 31°F are about 1 to 2555 and the predicted odds change dramatically based on temperature. It is important to recognize that the previous launches did not result in the same disaster as the Challenger launch because the O-rings showed only “minor” damage. This wasn’t enough for gas to escape—only an indicator that the O-rings might not be as resilient as expected.

\large
\textbf{CAUTION:} Estimating a value for a temperature of 31°F is extrapolating beyond our data set. Just as in least squares regression, caution should be used when making predictions outside the range of explanatory variables that are available.
\normalsize

## **The Logistic Regression Model Using Maximum Likelihood Estimates**

Logistic regression is a special case of what is known as a generalized linear model. Generalized linear models expand linear regression models to cases where the normal assumptions do not hold. All generalized linear models have three components:

\begin{itemize}
  \item \textbf{A linear predictor:} In the shuttle example, there is only one explanatory variable, $x = \text{Temperature}$, so the linear predictor in Equation (7.5) is $\beta_0 + \beta_1 x$. However, just as in other multiple regression models, the linear predictor can include many explanatory variables, including indicator variables and interaction terms as well as other transformed variables.
  \item \textbf{A random component:} Each error term is assumed to be independent. However, in generalized linear models, the error terms are not required to follow a normal distribution. In addition, generalized linear models do not require that the variability of the error terms be constant.
  \item \textbf{A link function:} A link function is a function that fits the expected response value to a linear predictor. In Equation (7.5), the link function is the log-odds function, $\ln[\pi/(1 - \pi)]$. The link function depends on the distribution of the response variable. In logistic regression, the response variable is binary. Other link functions for binary response data do exist, but the log-odds function is the most common because it is the easiest to interpret.
\end{itemize}

Generalized linear models can also be used when the response variable follows other distributions. For example, $y$ may follow a Poisson or gamma distribution. Textbooks on generalized linear models derive link functions for each of these types of response variables. In least squares regression where the response has a normal distribution, as in Equation (7.1), the link function is simply the identity function. In other words, the response needs no transformation in simple linear regression models.

Clearly the logistic regression model in Figure 7.3 is nonlinear. So it may seem somewhat surprising to consider logistic regression as a generalized linear model. The reason we still call this model linear is that the link function, the log-odds transformation, is modeled with a linear predictor, $\beta_0 + \beta_1 x$.

Instead of using least squares estimates, generalized linear models use the method of maximum likelihood to estimate the coefficients $\beta_0$ and $\beta_1$. The extended activities provide more detail on calculating maximum likelihood estimates in logistic regression. In the space shuttle example, we will simply use a computer software package to find maximum likelihood estimates of $\beta_0$ and $\beta_1$.

\large
\textbf{NOTE:} In least squares regression, we often transform the response variable ($y$) so that the data fit model assumptions. In addition to linearizing data, transforming $y$ impacts the variability and the distribution of the error terms. In generalized linear models, the link function transforms the expected response ($\pi$) to fit a linear predictor. For those who have had calculus, link functions are also differentiable and invertible.
\normalsize

## Activity: Using Software to Calculate Maximum Likelihood Estimates {-}
>4. Solve Equation (7.5) for $\pi_i$ to show that Equation (7.6) is true.  
>5. Use Equation (7.6) to create six graphs. In each graph, plot the explanatory variable ($x$) versus the expected probability of success ($\pi$) using $\beta_0 = -10$ and $-5$ and $\beta_1 = 0.5$, $1$, and $1.5$. Repeat the process for $\beta_0 = 10$ and $5$ and $\beta_1 = -0.5$, $-1$, and $-1.5$.  
>   a. Do not submit the graphs, but explain the impact of changing $\beta_0$ and $\beta_1$.  
>   b. For all of these graphs, what value of $\pi$ appears to have the steepest slope?  
>6. Use statistical software to calculate the maximum likelihood estimates of $\beta_0$ and $\beta_1$. Compare the maximum likelihood estimates to the least squares estimates in Question 3.

Figure 7.3 shows a logistic regression model using maximum likelihood estimates of $\beta_0$ and $\beta_1$. Using Equation (7.6) and the maximum likelihood estimates from Question 6, we can estimate the probability that a launch has no O-ring damage at temperature $x_i$:

\begin{align}
\hat\pi_i 
&= \frac{e^{b_0 + b_1 x_i}}{1 + e^{b_0 + b_1 x_i}} \\
&= \frac{e^{-15.043 + 0.232 x_i}}{1 + e^{-15.043 + 0.232 x_i}} \\
&\quad \text{for } i = 1,2,3,\dots,n \tag{7.9}
\end{align}

Notice that $\pi$ in Equation (7.6) has been replaced by $\hat\pi$ in Equation (7.9) because the parameters in the linear regression model ($\beta_0$ and $\beta_1$) have been estimated with our sample data; $b_0 = \hat\beta_0 = -15.043$ and $b_1 = \hat\beta_1 = 0.232$.

## Activity: Estimating the Probability of Success with Maximum Likelihood Estimates
>7. Use Equation (7.9) to predict the probability that a launch has no O-ring damage when the temperature is 31°F, 50°F, and 75°F.

At this point, it seems reasonable to question why the O-rings were not considered a higher risk at the time of the 1986 Challenger launch. After all, the odds of a successful launch (no O-ring damage) at the expected temperature of 31°F are about 1 to 2555 and the predicted odds change dramatically based on temperature. It is important to recognize that the previous launches did not result in the same disaster as the Challenger launch because the O-rings showed only “minor” damage. This wasn’t enough for gas to escape—only an indicator that the O-rings might not be as resilient as expected.

\large
\textbf{CAUTION:} Estimating a value for a temperature of 31°F is extrapolating beyond our data set. Just as in least squares regression, caution should be used when making predictions outside the range of explanatory variables that are available.
\normalsize

## **Interpreting the Logistic Regression Model**

Interpretation of logistic regression models is often done in terms of the odds of success (odds of a launch with no O-ring damage). When the temperature is 59°F, the odds of a successful launch with no O-ring damage are $\hat\pi/(1 - \hat\pi) = 0.2066/(1 - 0.2066) = 0.2605 \approx 0.25 = 1/4$. Thus, at 59°F, we state that the odds of a successful launch are about 1 to 4. When the temperature is 60°F, the odds of a successful launch are $\hat\pi/(1 - \hat\pi) = 0.3285 \approx 0.333 \approx 1/3$. At 60°F, we state that the odds of a successful launch are about 1 to 3.

The slope is not as easy to interpret for a logistic regression model as for a simple linear regression model. While ordinary least squares regression focuses on $\beta_1$, logistic regression measures the change in the odds of success by the term $e^{b_1}$, which is called the odds ratio. If we increase $x_i$ by 1 unit in a logistic regression model, the predicted odds that $y = 1$ (i.e., the launch will not have any O-ring damage) will be multiplied by $e^{b_1}$. For example, when the temperature changes from 59°F to 60°F, the odds increase by a multiplicative factor of $e^{b_1} = e^{0.232} = 1.2613$. In other words,

\begin{align}
\text{odds of success at 59°F} \times e^{b_1} 
&= 0.2605(1.2613) \notag \\
&= 0.3285 \notag \\
&= \text{odds of success at 60°F} \notag
\end{align}

For any temperature value $x_i$, this relationship can also be stated as

\begin{align}
\text{odds ratio} = e^{b_1} &= \frac{\text{odds}(x_i + 1)}{\text{odds}(x_i)} \tag{7.10}
\end{align}

\large
\textbf{MATHEMATICAL NOTE:}
Taking the exponent of Equation (7.5), we can write the odds of success as
\begin{align}
\text{odds} = \biggl(\frac{\pi_i}{1 - \pi_i}\biggr) &= e^{\beta_0 + \beta_1 x_i} = e^{\beta_0}(e^{\beta_1})^{x_i} \tag{7.11}
\end{align}

Thus, as $x_i$ increases by 1,
\begin{align}
e^{\beta_0}(e^{\beta_1})^{x_i + 1} &= e^{\beta_0}(e^{\beta_1})^{x_i}(e^{\beta_1}) \tag{7.12}
\end{align}

\normalsize

\Large
\textbf{\textcolor{red}{Key Concept:}}
\color{red}
The slope in a logistic regression model is typically described in terms of the odds ratio $e^{b_1}$. If we increase $x_i$ by 1 unit, the predicted odds will be multiplied by $e^{b_1}$. In our example, if the temperature increases by one degree, we increase the odds of a successful launch by $e^{b_1} = e^{0.232} = 1.2613$ times. Similarly, if we decrease $x_i$ by 1 unit, the predicted odds will be multiplied by $e^{-b_1} = 1/e^{b_1}$.
\color{black}
\normalsize

## Activity: Interpreting a Logistic Regression Model
>8. Calculate the odds of a launch with no O-ring damage when the temperature is 60°F and when the temperature is 70°F.  
>9. When $x_i$ increases by 10, state in terms of $e^{b_1}$ how much you would expect the odds to change.  
>10. The difference between the odds of success at 60°F and 59°F is about 0.3285 – 0.2605 = 0.068. Would you expect the difference between the odds at 52°F and 51°F to also be about 0.068? Explain why or why not.  
>11. Create a plot of two logistic regression models. Plot temperature versus the estimated probability using maximum likelihood estimates from Question 6, and plot temperature versus the estimated probability using least squares estimates from Question 3.

Thus far, we have developed a model to estimate the odds of a successful launch with no O-ring failures. However, we have not yet discussed the variability of the estimates or how confident we can be of the results. In the next section, we will discuss two hypothesis tests that can be used to determine if the odds of a successful launch are related to temperature. In other words, can we conclude that the logistic regression coefficient $b_1$ is not equal to zero?

\Large
\textbf{\textcolor{red}{Key Concept:}}
\color{red}
The probability, the odds, and the log-odds are three closely related calculations. Even though any of
the three could be used to express the concepts of interest, the log-odds are often used to estimate the
coefficients, while interpretation of logistic regression models typically relies on expected probabilities
and odds because they are easier to interpret.
\color{black}
\normalsize

## **Inference for the Logistic Regression Model**

## Assumptions for Logistic Regression Models {-}

Inference for logistic regression uses statistical theory that is based on limits as the sample size approaches infinity. The techniques, based on what is called asymptotic theory, work well with large sample sizes, but are only approximate with outliers or small sample sizes. It is common for logistic regression models to be developed for data sets of any size, but savvy statisticians will always use caution when interpreting the results for data sets with small sample sizes (such as the space shuttle example).

## The Wald Statistic {-}

Wald’s test is often used to test the significance of logistic regression coefficients. Just as in least squares regression, we set up a hypothesis test to determine if there is a relationship between the explanatory and response variables:

\begin{align}
H_0: b_1 = 0 \quad \text{vs.}\quad H_a: b_1 \neq 0
\end{align}

Wald’s test is similar to the one-sample Z-test seen in introductory statistics courses. The Wald statistic is calculated as

\begin{align}
Z = \frac{b_1 - 0}{\text{se}(b_1)} = \frac{0.232}{0.108} = 2.14 \tag{7.13}
\end{align}

\large
\textbf{NOTE:}
Some texts use a chi-square statistic instead of the Z-statistic given in Equation (7.13). Most probability textbooks explain that the square of the test statistic in Equation (7.13) follows a chi-square distribution with 1 degree of freedom. Both techniques provide identical p-values.
\normalsize

where $b_1$ is the maximum likelihood estimate of $\beta_1$ and se($b_1$) is the standard error of $b_1$. The maximum likelihood estimate, $b_1$, is asymptotically normally distributed (i.e., $b_1$ is normally distributed when the sample size is large). Thus, the Z-statistic in Equation (7.13) will follow a standard normal distribution when the null hypothesis is true and the sample size is large. In this model, we see that the estimated slope coefficient $b_1 = 0.232$ has a p-value of $P(|Z| \geq 2.14) = 0.032$.

Wald confidence intervals can also be created. In logistic regression, the confidence interval is often discussed in terms of the odds ratio. For example, a 95% confidence interval for $\beta_1$ is given as

\begin{align}
(e^{b_1 - Z^* \text{se}(b_1)}, e^{b_1 + Z^* \text{se}(b_1)}) = (e^{0.232 - 1.96(0.108)}, e^{0.232 + 1.96(0.108)}) = (e^{0.02}, e^{0.44}) = (1.02, 1.56) \tag{7.14}
\end{align}

where 1.96 = Z^* represents a value corresponding to a 95% confidence interval for a normal distribution with mean of 0 and standard deviation of 1. When $b_1 = 0$, and thus the odds ratio $e^{b_1} = 1$, the odds of success for temperature $x_i$ are the same as the odds of success for any other temperature. Thus, $e^{b_1} = 1$ tells us that there is no association between the explanatory variable and the response.

\Large
\textbf{\textcolor{red}{Key Concept:}}
\color{red}
When a 95% Wald confidence interval for the odds ratio does not contain 1, we reject the null hypothesis $H_0: \beta_1 = 0$ (using an alpha-level of 0.05) and conclude that the odds of success do depend on the explanatory variable $x_i$. If the interval does contain 1, we fail to reject $H_0: \beta_1 = 0$.
\color{black}
\normalsize

The Minitab output in Figure 7.5 shows Wald’s test and the corresponding confidence interval for the
odds ratio. In the space shuttle example, the 95% confidence interval does not include $e^{\beta_1} = 1$; thus, we can
reject the null hypotheses and conclude that the odds of a successful launch do depend on the temperature.
Even though computer software provided a small p-value and a confidence interval that does not include 1, it
is important to note that there are only 23 observations in this study. While Wald’s test is reasonable with very
large sample sizes, with smaller sample sizes it is known to have a tendency to result in a type II error—failing
to reject the null hypothesis when it should be rejected.$^5$

We can also calculate the odds ratio of a successful launch between 60°F and 70°F. When $x_i$
increases by 10°F, the odds are multiplied by $(e^{b_1})10 = 1.2613^{10} = 10.19$. Thus, we have approximately
10 times higher odds of a successful launch when the temperature is 70°F than when it is 60°F. A 95%
confidence interval for the odds ratio of a successful launch between 60°F and 70°F can be given by
$(1.02^10, 1.56^10) = (1.22, 85.40)$. This 95% confidence interval has a very wide range; the odds of success
at 70°F could be just slightly larger than the odds at 60°F or 85 times as large as the odds at 60°F. This
large range suggests that our estimate of the odds ratio, 10.19, is highly variable. More data are needed to
better understand the true odds ratio.

## Activity: Wald Confidence Intervals and Hypothesis Tests {-}
>12. Calculate the odds ratio of a successful launch between 31°F and 60°F. Provide a confidence interval
for this odds ratio and interpret your results.
>13. The coefficients in Equation (7.9) were calculated when a successful launch was given a value of 1. Con-
duct a logistic regression analysis where 1 indicates an O-ring failure and 0 represents a successful launch.
a. Explain any relationships between the model shown in Equation (7.9) and this new model.
b. How did the regression coefficients change?
c. How did the odds ratio change?
d. Create a 95% Wald confidence interval for the new odds ratio and interpret the results.

[[[Figure 7.5]]]

?Recall that Z in Equation (7.13) is a statistic calculated from the sample data and Z* in Equation (7.11) is called a critical
value. Z* represents a value based on a desired level of confidence. Z* is known before the data are collected, while Z is
based on the sample data

## **The Likelihood Ratio Test**

The **likelihood ratio test (LRT)** is derived by calculating the difference between the adequacy of the full and restricted log-likelihood models. A **full model** (sometimes called an **unrestricted model**) includes all parameters under consideration in the model. In this example, there are only two parameters, $\beta_0$ and $\beta_1$, but the full model could include more parameters if more explanatory variables were in the model. The **restricted model** (also called a **reduced model**) is a model with fewer terms than the full model. In our example, only $\beta_0$ is in the restricted model (no explanatory variables are in this model). When no explanatory variables are in the restricted model, the restricted model is also called a **null model**. If the full model has a significantly better fit (the expected values are closer to the observed values) than the restricted model, we reject the null hypothesis $H_0: \beta_1 = 0$ and conclude that $H_a: \beta_1 \neq 0$.

The log-likelihood (restricted) function, described in the extended activities, is a measure of the fit of the model that includes only the intercept:

\begin{align}
\text{Restricted Model:} \quad \pi_i = \frac{e^{\beta_0}}{1 + e^{\beta_0}}
\notag
\end{align}

For the restricted model, the null hypothesis is true and $\pi_i$ is constant for any x-value. The log-likelihood (full) function measures the fit of the model that includes all of the parameters of interest:

\begin{align}
\text{Full Model:} \quad \pi_i = \frac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}}
\notag
\end{align}

When the null hypothesis $H_0: \beta_1 = 0$ is true, it can be shown that

\begin{align}
G = 2 \times \text{log-likelihood(full model)} - 2 \times \text{log-likelihood(restricted model)} \sim \chi^2_1 
\tag{7.15}
\end{align}

The G-statistic measures the difference between the fits of the restricted and full models. In essence, we
are measuring how much better the fit is when an explanatory variable (temperature) is added to the logistic
model. If the p-value corresponding to the G-statistic is small, the difference in fits is so large that it is unlikely
to occur by chance, and thus we conclude that Ha: b1 0 (the fit of the full model is significantly better than
that of the restricted model).

Degrees of freedom for the LRT equal the number of parameters in the full model minus the number of
parameters in the restricted model. In our case, this is 2 - 1 = 1. Different software packages will present
this test in slightly different ways. In the Minitab output in Figure 7.5, the log-likelihood of the full model
(-10.158) and the G-statistic (7.952) are provided.

Other statistics packages may not give the G-statistic, but they will give enough information so that
the LRT can be calculated. The R output shown in Figure 7.6 gives the null deviance [K - 2 * log-
likelihood (restricted model)] and the residual deviance [K - 2 * log-likelihood (full model)], where
K is a constant value.

Note that
\begin{align}
G &= 2 \times \text{log-likelihood(full model)} - 2 \times \text{log-likelihood(restricted model)} \notag \\
&= \text{null (restricted model) deviance} - \text{residual (full model) deviance} \notag \\
&= 7.952. \notag 
\end{align}

## Activity: The Likelihood Ratio Test {-}
>14. Use statistical software to calculate the LRT for the space shuttle data. Submit the p-value and state your conclusions.

The G-statistic is relatively large, indicating that we have some evidence to reject $H_0: beta_1 = 0$ and conclude that temperature is related to the odds of a successful launch with no O-ring damage. When sample sizes are large and there is only one explanatory variable in the model, the p-values for the LRT and Wald’s test will be approximately the same. In the space shuttle example, the LRT and Wald’s test have somewhat different p-values.

The likelihood ratio test is more reliable and is often preferred over Wald’s test for small sample sizes. However, unlike the LRT, Wald’s test can have one-sided alternative hypothesis tests as well as nonzero hypothesized values. It is difficult to determine the actual sample size needed for Wald’s test or the LRT to perform well. Some statisticians suggest a minimum sample size of 100 observations.$^7$ Thus, it is best to label each p-value as approximate when using these tests with a small sample size.

\Large
\textbf{\textcolor{red}{Key Concept:}}
\color{red}
With a large sample size, Wald’s test and the likelihood ratio test provide accurate tests for $H_0: \beta_1 = 0$ versus $H_a: \beta_1 \neq 0$. While the LRT test tends to be more reliable with smaller sample sizes, use caution when interpreting the results for data sets with small sample sizes (such as the space shuttle example).
\color{black}
\normalsize

## **What Can We Conclude from the Space Shuttle Study?**

The space shuttle example is an observational study, since the launches were not “randomly assigned” to the temperature groups, so we cannot conclude solely from this data set that low temperatures caused O-ring damage. Wald’s test and the likelihood ratio test both provided some evidence that the odds of a successful launch are related to temperature. However, a sample size of 23 is not large enough for us to be confident that the p-values are reliable. The logistic regression model provides some indication that the probability of a successful launch is related to temperature. Other information, such as scientists understanding that cold temperatures cause O-rings to be more brittle, also strengthens the conclusion.

## **Logistic Regression with Multiple Explanatory Variables**

Wolberg and Mangasarian developed a technique to accurately diagnose breast masses using only visual characteristics of the cells within the tumor. A sample is placed on a slide, and characteristics of the cellular nuclei within the tumor, such as size, shape, and texture, are examined under a microscope to determine whether the cancer cells are benign or malignant. Benign tumors are scar tissue or abnormal growths that do not spread and are typically harmless. Malignant (or invasive) cancer cells are cells that can travel, typically through the bloodstream or lymph nodes, and begin to replace normal cells in other parts of the body. If a tumor is malignant, it is essential to remove or destroy all cancerous cells in order to keep them from spreading. If a tumor is benign, surgery is typically not needed and the harmless tumor can remain.

In Chapter 6, we used contingency tables with only two variables, cell shape and type, to better understand how to analyze two categorical variables. This section will describe the process of variable selection in logistic regression, using the radius and the concavity of cell nuclei to estimate the probability that a tumor is malignant. In this data set, radius is actually the average radius (in micrometers, $\mu$m) of all visible cell nuclei from a slide, but we will refer to this variable simply as the cell radius for the tumor. The concavity of the cell nuclei is an indicator of whether the visible cell nuclei from the sample have the nice round shape of typical healthy cells or whether cells appear to have grown in such a way that the perimeters of the cell nuclei tend to have concave points.

## Extended Activity: Estimating the Probability of Malignancy in Cancer Cells {-}

Data set: Cancer2

15. Create a logistic regression model using Radius and Concave as explanatory variables to estimate the probability that a mass is malignant.  
   a. Using Radius as the first explanatory variable, $x_1$, and Concave as the second explanatory variable, $x_2$, submit the logistic regression model. In other words, find the coefficients for the model

\begin{align}
   y_i &= \frac{e^{\beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i}}}{1 + e^{\beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i}}} + \epsilon_i \quad \text{for } i = 1,2,\dots,n
\end{align}

   b. Submit the likelihood ratio test results, including the log-likelihood (or deviance) values.  
   c. Concave = 0 represents round cells and Concave = 1 represents concave cells. Calculate the event probability when Radius = 4 and the cells are concave. Also calculate the event probability when Radius = 4 and the cells are not concave.

16. Create a logistic regression model using only Radius as an explanatory variable to estimate the probability that a mass is malignant.  
   a. Submit the logistic regression model and the likelihood ratio test results, including the log-likelihood (or deviance) values.  
   b. Calculate the event probability when Radius = 4.

When there are multiple explanatory variables in a logistic regression model, such as in the model created in Question 15, the likelihood ratio test compares the full model to the null model, which excludes the radius and concavity terms. Thus, the null hypothesis is that the coefficient corresponding to each of the explanatory variables is zero. In Question 15, the LRT is testing

$$H_0: \beta_1 = \beta_2 = 0 \quad \text{vs.}\quad H_a: \text{at least one of the coefficients is not zero}$$

The G-statistic for this hypothesis test is 527.42 with 3 - 1 = 2 degrees of freedom (the number of parameters in the full model minus the number of parameters in the restricted model) and a corresponding p-value < 0.001. Thus, we can reject $H_0$ and conclude that at least one of the explanatory variables is significantly related to the probability that the cells are malignant.

The coefficients in multiple logistic regression models are discussed in terms of the odds of success. Any coefficient ($b_j$) indicates how the response will change corresponding to the $j$th explanatory variable, conditional on all other explanatory variables in the model. When the $j$th explanatory variable is increased by one unit, the odds of success will be multiplied by $e^{b_j}$. When an explanatory variable ($x_j$) is binary, as Concave is, $e^{b_j}$ represents the odds ratio between the two groups. However, just as in ordinary least squares regression with multiple explanatory variables, these coefficients are conditional on the other terms in the model.

## **The Drop-in-Deviance Test**

Figure 7.7 provides the expected probabilities for the model created in Question 15. The probability of a cell being malignant appears to depend on Radius. In addition, it appears that concavity is an important variable in the model, since concave cells tend to have a higher estimated probability of being malignant. In Chapter 3, variable selection is described as a process of determining which explanatory variables should be included in a regression model. Ideally, we would like the simplest model (i.e., the model with the fewest terms) that best explains the response (i.e., the model that has the smallest residuals). In this example, we want to determine if the model in Question 16 can estimate the probability of malignancy just as accurately as the slightly more complex model in Question 15. If the models have similar abilities to estimate the probability of malignancy, then we will prefer the simpler model.

The logic for determining whether additional terms should be in a logistic regression model is essentially the same as for the LRT discussed in connection with the space shuttle study. The log-likelihood (or deviance) can be used to measure how well any model fits the data. If a full model with two explanatory variables, $x_1$ and $x_2$, has a much better fit than a restricted model with just one explanatory variable, $x_1$, we can conclude that $x_2$ is significant and should be included in the model.

If the LRT shows no significant difference between the full and the restricted model, the coefficient of
the second variable, x2, can be set to zero and we can conclude that including the additional variable in the
model does not improve our ability to estimate the probability of success (in this case, success represents a
malignant cell).

In the cancer study, we can compare the full log-likelihood (or deviance) from the two-term model in
Question 15 to the restricted log-likelihood (or deviance) from the one-term model in Question 16.

Just as in the LRT described in connection with the shuttle example, degrees of freedom are calculated as the
number of parameters in the full model minus the number of parameters in the restricted model. Thus, we can
test whether Concave (x2) should be included in the model by finding the p-value, which is the percentage of
the x2 distribution with 3 - 2 = 1 degree of freedom that exceeds G. The p-value corresponding to Equation
(7.16) is less than 0.0001. We have strong evidence that the explanatory variable, Concave, is important in
the model when Radius is already included. Thus, the logistic regression model in Question 15 is preferred
over the model in Question 16.

\Large
\textbf{Key Concept:}
\color{red}
To use the drop-in-deviance test to determine if $x_i$ should be included in a model:

1. Calculate the deviance (or $-2 \times$ log-likelihood) for the full (i.e., unrestricted) model with all variables of interest.
2. Calculate the deviance (or $-2 \times$ log-likelihood) for the reduced (i.e., restricted) model (e.g., the model including all the variables in step 1 except for $x_i$).
3. Calculate $G = 2 \times$ log-likelihood (full model) $-2 \times$ log-likelihood (restricted model) = deviance (restricted model) $-$ deviance (full model).
4. Calculate the degrees of freedom, the number of parameters in the full model minus the number of parameters in the restricted model.
5. Find the $p$-value, the percentage of the $\chi^2$ distribution that exceeds $G$.
6. If the $p$-value is small, reject $H_0: \beta_i = 0$ and conclude that $x_i$ should be included in the model. If the $p$-value is large, the explanatory variable, $x_i$, can be eliminated from the model.
\color{black}
\normalsize
\color{black}

## **Measures of Association**

When sample sizes are small, a model may have a strong association (a clear pattern is visible) but not have enough evidence to show that the independent variable is significant. Conversely, if there were thousands of observations in a data set, a hypothesis test might conclude that an independent variable was significant even though there was only a very weak association. Thus, researchers typically report both significance tests and a measure of association when discussing results. While there is no widely accepted equivalent to $R^2$ in logistic regression, this section will describe calculations that can be used to measure the strength of association.

To measure the strength of association in the space shuttle logistic regression model, pair each observed success with every observed failure. In the shuttle example, there are 16 successes and 7 failures; thus, there are $16 \times 7 = 112$ pairs. For each pair, use the logistic regression model to estimate the probability of success for both the observed success and the observed failure. If the observation corresponding to a success has a higher estimated probability, the pair is called a concordant pair. If the observation corresponding to a success has a lower estimated probability, the pair is called a discordant pair. Tied pairs occur when the observed success has the same estimated probability as the observed failure.

\begin{itemize}
  \item To find Somers’ $D$, take the number of concordant pairs minus the number of discordant pairs and then divide by the total number of pairs.
  \item To find Goodman-Kruskal gamma (also called Goodman and Kruskal’s gamma), take the number of concordant pairs minus the number of discordant pairs and then divide by the total number of pairs excluding ties.
  \item To find Kendall’s tau-a, take the number of concordant pairs minus the number of discordant pairs and then divide by the total number of pairs of observations including pairs with the same response value.
\end{itemize}

If all possible pairs were concordant, then Somers’ $D$ would equal 1. If the model had no predictive power, we would expect half the pairs to be concordant and half to be discordant. This would correspond to Somers’ $D = 0$. Thus, a value of 0 for Somers’ $D$ (as well as Goodman and Kruskal’s gamma) indicates no effect of the explanatory variable on the response variable.

## Extended Activity: Measures of Association {-}

Data set: Shuttle

19. The first and eleventh launches form a pair, since at 63°F there was an O-ring failure and at 66°F there was a success (no O-ring failure). This is a concordant pair, since the probability of success is higher when there was an observed success. Estimate the probability of success for each temperature.
20. Calculate the expected probabilities of the first (66°F) and 22nd (75°F) observations. Is this a concordant or discordant pair?
21. Identify two launches in the space shuttle data that represent a tied pair.
22. Various statistical software packages tend to provide different measures of association. Use statistical software to calculate the Goodman-Kruskal gamma, Somers’ $D$, or Kendall’s tau-a for the space shuttle data.

## **Review of Means and Variances of Binary and Binomial Data**

If you have worked with discrete probability models, you will recognize that binary data follow a Bernoulli distribution if the following conditions are true:

\begin{itemize}
  \item Each observation, $y_i$, is independent.
  \item Each $y_i$ falls into exactly one of two categories represented by either a 0 or a 1.
  \item The probability of success, $P(Y_i = 1) = \pi_i$, is constant for each observation.
\end{itemize}

The Bernoulli distribution can be displayed as in Table 7.2. Table 7.2 is often represented with the following mathematical function to model the Bernoulli distribution:

\begin{align}
P(Y = k) &= \pi^k(1 - \pi)^{1-k} \quad \text{for } k = 0,1 \tag{7.17}
\end{align}

The expected value (mean) of $y$ is the average outcome:

\begin{align}
E(Y) &= 0 \times P(Y = 0) + 1 \times P(Y = 1) = 0 \times (1 - \pi) + 1 \times \pi = \pi \tag{7.18}
\end{align}

Note that each outcome is not equally likely. Thus, each outcome is weighted by its probability. The variance of $y$ is the average value of the squared deviation of the observed value, $y$, and the expected value, $\pi$:

\begin{align}
\mathrm{Var}(Y) &= E[(Y - \pi)^2] = (0 - \pi)^2 \times P(Y = 0) + (1 - \pi)^2 \times P(Y = 1) = \pi(1 - \pi) \tag{7.19}
\end{align}

In the above equations, $\pi$ is the same for every observational unit and the results for each observational unit are independent of each other. However, in regression we focus on the expected value of $y$ given $x$, $E(Y \mid x_i)$. This represents the expectation that the probability of success depends on an explanatory variable. In the space shuttle example, the expected probability of a successful launch will change depending on the temperature. For any given temperature value, $x_i$, the probability of success is constant, $\pi_i$, and the observations are independent. Thus, for a particular $x_i$, the corresponding mean and variance are:

\begin{align}
E(Y \mid x_i) &= \pi_i \\
\mathrm{Var}(Y \mid x_i) &= \pi_i(1 - \pi_i) \tag{7.20}
\end{align}

Thus, in logistic regression with Bernoulli response variables, the variance of $Y$ will depend on $x$. This violates the key assumption of constant variance in least squares regression models.

Logistic regression is also appropriate when the response is a count of the number of successes. A count follows a binomial distribution if the following conditions are true:

\begin{itemize}
  \item There are $n_i$ independent observations at a given level of $x$ ($x_i$).
  \item $\pi_i = P(Y_i = 1 \mid x_i)$ is the probability of success, and this probability is constant for any given $x_i$ ($0 \le \pi_i \le 1$).
  \item Each response has only two possible outcomes. However, instead of recording a 0 or a 1 value for each outcome, we typically record $Y$ as the count of successes (or proportion of successes) at a particular $x_i$ value.
\end{itemize}

Many introductory textbooks show that if the data follow a binomial distribution, the probability that there are $k$ successes in $n_i$ independent observations is:

\begin{align}
P(Y = k \mid x_i) &= \binom{n_i}{k} \pi^k(1 - \pi)^{n_i-k} \quad \text{for } k = 0,1,\dots,n_i \tag{7.21}
\end{align}

where $\binom{n_i}{k} = \frac{n_i!}{k!(n_i-k)!}$ is called the binomial coefficient and is read as “$n_i$ choose $k$. ”

Using a technique similar to that for the Bernoulli distribution, it can be shown that the conditional mean (probability of success) and variance of binomial response variables are:

\begin{align}
E(Y \mid x_i) &= n_i \times \pi_i \\
\mathrm{Var}(Y \mid x_i) &= n_i \times \pi_i(1 - \pi_i) \tag{7.22}
\end{align}















```{r tab7.2, echo=FALSE, results='asis'}
# Table 7.2: The Bernoulli model
tab <- data.frame(
  "Value of y" = "Probability",
  "0" = "$1 - \\pi$",
  "1" = "$\\pi$",
  check.names  = FALSE,
  stringsAsFactors = FALSE
)

cat(
  kable(
    tab,
    format  = "latex",
    booktabs = TRUE,
    escape = FALSE,
    caption = "Table 7.2 The Bernoulli model.",
    align = c("l", "c", "c")
  ) %>%
  kable_styling(latex_options = "hold_position")
)

```

```{r tab7.3, echo=FALSE, results='asis'}
# Table 7.3: Cancer cell data by radius group
df7.3 <- data.frame(
  i = c("1","2","3","4","5",""),
  `Median radius $(x_i)$` = c("2.5","3.5","4","4.5","6.5","Total"),
  `Benign $(n_i - y_i)$` = c("113","140","85","17","2","357"),
  `Malignant $(y_i)$`  = c("2","12","34","43","121","212"),
  `Total $(n_i)$` = c("115","152","119","60","123","569"),
  `Proportion Malignant` = c("0.017","0.079","0.286","0.717","0.984",""),
  check.names = FALSE,
  stringsAsFactors = FALSE
)

cat(
  kable(
    df7.3,
    format = "latex",
    booktabs = TRUE,
    escape = FALSE,
    caption = "Table 7.3 Cancer cell data classified into groups based on the size of the radius.",
    align = c("r","r","r","r","r","r"),
    row.names = FALSE
  ) %>%
  kable_styling(latex_options="hold_position")
)
```

```{r tab7.4, echo=FALSE, results='asis'}
# Table 7.4: Observed vs. expected under logistic regression
df7.4 <- data.frame(
  i                   = c("1","2","3","4","5",""),
  `Median radius`     = c("2.5","3.5","4","4.5","6.5","Total"),
  `Benign`   = c("113","140","85","17","2","357"),
  `Malignant`= c("2","12","34","43","121","212"),
  `Total`    = c("115","152","119","60","123","569"),
  `Benign`   = c(
    "115($1-\\hat\\pi_1$) = 113.967",
    "","", "","", "357"
  ),
  `Malignant`= c(
    "115($\\hat\\pi_1$) = 115(0.00898) = 1.033",
    "$152\\hat\\pi_2 = 16.13$", "", "", "", "212"
  ),
  `Total`    = c("115","152","119","60","123","569"),
  check.names     = FALSE,
  stringsAsFactors= FALSE
)

cat(
  kable(
    df7.4,
    format    = "latex",
    booktabs  = TRUE,
    escape    = FALSE,
    caption   = paste0(
      "Table 7.4 Observed and expected values using the logistic regression model ",
      "for the Cancercells data, where $\\hat\\pi_i$ is estimated from the ",
      "logistic regression model $\\hat\\pi_i = e^{b_0 + b_1 x_i}/(1 + e^{b_0 + b_1 x_i})$."
    ),
    align        = c(rep("c",8)),
    row.names    = FALSE
  ) %>%
  add_header_above(c(" "=2, "Observed"=3, "Expected"=3)) %>%
  kable_styling(latex_options="hold_position") %>%
  column_spec(2, width = "2cm") %>%
  column_spec(6, width = "2.5cm") %>%
  column_spec(7, width = "2.5cm")  
)
```


```{r fig7.3, echo=FALSE, fig.width=6, fig.height=4, message=FALSE, fig.cap="Figure 7.3 Space shuttle data with a simple linear regression model and a logistic regression model."}
shuttle <- read.csv("Shuttle.csv", stringsAsFactors = FALSE)
ggplot(shuttle, aes(x = Temperature, y = Success)) +
  geom_jitter(width = 0, height = 0.02, size = 2, shape = 21, fill = "black", color = "black") +
  # logistic fit
  stat_smooth(
    aes(color = "logistic model"),
    method = "glm",
    method.args = list(family = "binomial"),
    se = FALSE,
    size = 1
  ) +
  # simple linear fit
  stat_smooth(
    aes(color = "simple linear model"),
    method = "lm",
    se = FALSE,
    size = 1
  ) +
  # legend colors
  scale_color_manual(
    "",
    values = c(
      "logistic model" = "#00BFC4",
      "simple linear model" = "black"
    )
  ) +
  # labels and y‐axis limits
  labs(
    x = "Ambient Temperature",
    y = "Probability of a Successful Launch"
  ) +
  scale_y_continuous(limits = c(-0.05, 1.05), expand = c(0, 0)) +
  theme_minimal(base_family = "serif") +
  theme(
    # graph
    panel.background = element_rect(fill = "grey95", colour = "black", linewidth = 1),
    panel.border = element_rect(fill = NA, colour = "black", linewidth = 1),
    # background
    plot.background = element_rect(fill = "grey90", colour = "black", linewidth = 1),
    legend.position = c(0.2, 0.8),
    # legend & text
    legend.background = element_rect(fill = "white", colour = "black"),
    plot.title = element_text(size = 16, hjust = 0.5),
    plot.margin = margin(t = 20, r = 20, b = 20, l = 15, unit = "pt")
  )

```


```{r fig7.4, echo=FALSE, fig.width=6, fig.height=4, message=FALSE, fig.cap="Figure 7.4 A scatterplot of the residuals from the space shuttle logistic regression model and a sample of what a scatterplot of normally distributed residuals might look like."}
# install.packages("pactchwork")
library(patchwork)
# fit logistic model and compute residuals
shuttle <- read.csv("Shuttle.csv", stringsAsFactors = FALSE)
log_mod <- glm(Success ~ Temperature, data = shuttle, family = binomial)
shuttle$resid <- shuttle$Success - predict(log_mod, type = "response")
set.seed(42)
df_norm <- data.frame(
  Temperature = shuttle$Temperature,
  Resid       = rnorm(
    nrow(shuttle),
    mean = mean(shuttle$resid),
    sd   = sd(shuttle$resid)
  )
)
# theme
base_theme <- theme_minimal(base_family = "serif") +
  theme(
    panel.background = element_rect(fill = "white", colour = "black", linewidth = 1),
    panel.border = element_rect(fill = NA,      colour = "black", linewidth = 1),
    plot.background  = element_rect(fill = "grey90", colour = "black", linewidth = 1),
    plot.title = element_text(size = 16, hjust = 0.5),
    plot.margin = margin(t = 20, r = 20, b = 20, l = 15, unit = "pt")
  )

# 1. Residual plot for the logistic model
p1 <- ggplot(shuttle, aes(x = Temperature, y = resid)) +
  geom_point(size = 2) +
  geom_hline(yintercept = 0) +
  labs(
    title = "Residual Plot for the\nLogistic Model",
    x = "Ambient Temperature",
    y = "Residuals"
  ) +
  scale_y_continuous(limits = c(-1, 1), expand = c(0, 0)) +
  base_theme

# 2. Normally distributed residuals 
p2 <- ggplot(df_norm, aes(x = Temperature, y = Resid)) +
  geom_point(size = 2) +
  geom_hline(yintercept = 0) +
  labs(
    title = "Normally Distributed\nResiduals",
    x = "Ambient Temperature",
    y = "Residuals"
  ) +
  scale_y_continuous(limits = c(-2, 2), expand = c(0, 0)) +
  base_theme

# combine side by side
p1 + p2 + plot_layout(ncol = 2)
```

```{r fig7.7, echo=FALSE, fig.width=6, fig.height=4, message=FALSE, fig.cap="Figure 7.7 A scatterplot of the observed data and estimated probabilities for both round cells (Concave= 0) and concave cells (Concave= 1)."}
library(tidyr)
# fut two logistic models
df <- read.csv("Cancer2.csv", stringsAsFactors = FALSE)
mod_round  <- glm(Malignant ~ radius,
                  data   = df,
                  family = binomial,
                  subset = (concavity == 0))
mod_concav <- glm(Malignant ~ radius,
                  data   = df,
                  family = binomial,
                  subset = (concavity == 1))
grid <- data.frame(radius = seq(min(df$radius), max(df$radius), length.out = 200))
grid$`Round Cells`  <- predict(mod_round, newdata = grid, type = "response")
grid$`Concave Cells`<- predict(mod_concav, newdata = grid, type = "response")
pred <- pivot_longer(
  grid,
  cols      = c("Round Cells", "Concave Cells"),
  names_to  = "Type",
  values_to = "Probability"
)

# plot
ggplot() +
  # two logistic curves
  geom_line(
    data = pred,
    aes(x = radius, y = Probability, color = Type),
    size = 1
  ) +
  # observed Malignant (0/1) points
  geom_point(
    data  = df,
    aes(x = radius, y = Malignant, color = "Observations"),
    shape = 23, size = 3, fill = "black"
  ) +
  # legend
  scale_color_manual(
    "",
    breaks = c("Round Cells", "Concave Cells", "Observations"),
    values = c(
      "Round Cells"    = "lightblue",
      "Concave Cells"  = "#00BFC4",
      "Observations"   = "black"
    ),
    guide = guide_legend(
      override.aes = list(
        linetype = c(1, 1, 0),  # lines for the two curves, not for points
        shape = c(NA, NA, 23) # the diamond for observations
      )
    )
  ) +
  # axis labels
  labs(
    x = "Radius",
    y = "Estimated Probabilities"
  ) +
  # theme
  theme_minimal(base_family = "serif") +
  theme(
    panel.background  = element_rect(fill = "white",  colour = "black", linewidth = 1),
    panel.border = element_rect(fill = NA,       colour = "black", linewidth = 1),
    plot.background  = element_rect(fill = "grey90", colour = "black", linewidth = 1),
    legend.background = element_rect(fill = "white",  colour = "black"),
    legend.position = c(0.75, 0.25), 
    plot.margin = margin(t = 20, r = 20, b = 15, l = 15, unit = "pt")
  )
```


```{r fig7.9, echo=FALSE, fig.width=6, fig.height=4, message=FALSE, fig.cap="Figure 7.9 A logistic regression model, with $\\hat \\pi_i$ estimated from Equation (7.8) using maximum likelihood estimates, plotted with the observed probability of malignancy for the grouped data in Table 7.3."}
# fit model
df <- read.csv("CancercellsTable.csv", stringsAsFactors = FALSE)
mod <- glm(cbind(Malignant, Benign) ~ Median.Radius,
           data = df,
           family = binomial)
radius_grid <- seq(min(df$Median.Radius), max(df$Median.Radius), length = 200)
pred_df <- data.frame(Median.Radius = radius_grid)
pred_df$predicted <- predict(mod, newdata = pred_df, type = "response")

# plot
ggplot() +
  # logistic curve
  geom_line(
    data = pred_df,
    aes(x = Median.Radius, y = predicted),
    color = "#00BFC4",
    linetype = "dashed",
    size = 1
  ) +
  # observed group proportions
  geom_point(
    data = df,
    aes(x = Median.Radius, y = Proportion.Malignant),
    size = 3
  ) +
  # labels
  labs(
    title = "Observed Cancer Cell Probabilities\nwith a Logistic Regression Model",
    x = "Radius",
    y = "Probability of Malignancy"
  ) +
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  scale_x_continuous(expand = c(0, 0)) +
  # theme
  theme_minimal(base_family = "serif") +
  theme(
    plot.title       = element_text(size = 16, face = "bold", hjust = 0.5, lineheight = 1.1),
    panel.background = element_rect(fill = "white", colour = "black", linewidth = 1),
    panel.border     = element_rect(fill = NA,  colour = "black", linewidth = 1),
    plot.background  = element_rect(fill = "grey90",  colour = "black", linewidth = 1),
    axis.title       = element_text(face = "bold"),
    legend.position  = "none"
  )
```


```{r fig7.11, echo=FALSE, fig.width=6, fig.height=4, message=FALSE, fig.cap="Figure 7.11 Scatterplots of delta deviance, delta chi-square, and delta beta values versus the expected probabilities from the space shuttle data. Circled values represent launches at 75°F."}
library(patchwork)

# fit model
df <- read.csv("Shuttle.csv", stringsAsFactors = FALSE)
mod_full <- glm(Success ~ Temperature, data = df, family = binomial)

# compute values
dev_full <- mod_full$deviance
chisq_full <- sum(residuals(mod_full, type = "pearson")^2)
beta_full  <- coef(mod_full)["Temperature"]
temps <- sort(unique(df$Temperature))
diag_list <- lapply(temps, function(t) {
  df2  <- subset(df, Temperature != t)
  m2 <- glm(Success ~ Temperature, data = df2, family = binomial)
  dev2 <- m2$deviance
  ch2 <- sum(residuals(m2, type = "pearson")^2)
  b2 <- coef(m2)["Temperature"]
  pi_t <- predict(mod_full,
                  newdata = data.frame(Temperature = t),
                  type    = "response")
  data.frame(
    Temperature = t,
    prob  = pi_t,
    delta_dev   = abs(dev_full   - dev2),
    delta_chisq = abs(chisq_full - ch2),
    delta_beta  = abs(beta_full  - b2)
  )
})
diag_df <- do.call(rbind, diag_list)

# find max pt
highlight_devmax <- diag_df[which.max(diag_df$delta_dev),   , drop = FALSE]
highlight_chisqmax <- diag_df[which.max(diag_df$delta_chisq), , drop = FALSE]
highlight_betamax  <- diag_df[which.max(diag_df$delta_beta),  , drop = FALSE]

# theme
my_theme <- theme_minimal(base_family = "serif") +
  theme(
    panel.border = element_rect(fill = NA,       colour = "black", linewidth = 1),
    plot.margin = margin(10, 10, 10, 10)
  )

# plot 1: Delta Deviance vs Probability
p1 <- ggplot(diag_df, aes(x = prob, y = delta_dev)) +
  geom_point(size = 2) +
  geom_point(
    data = highlight_devmax,
    aes(x = prob, y = delta_dev),
    shape  = 21, fill = NA, color = "#00BFC4",
    size = 4, stroke = 1.2
  ) +
  labs(x = "Probability", y = "Delta Deviance") +
  my_theme

# plot 2: Delta Chi‑Square vs Probability
p2 <- ggplot(diag_df, aes(x = prob, y = delta_chisq)) +
  geom_point(size = 2) +
  geom_point(
    data = highlight_chisqmax,
    aes(x = prob, y = delta_chisq),
    shape = 21, fill = NA, color = "#00BFC4",
    size = 4, stroke = 1.2
  ) +
  labs(x = "Probability", y = "Delta Chi‑Square") +
  my_theme

# plot 3: Delta Beta vs Probability
p3 <- ggplot(diag_df, aes(x = prob, y = delta_beta)) +
  geom_point(size = 2) +
  geom_point(
    data = highlight_betamax,
    aes(x = prob, y = delta_beta),
    shape = 21, fill = NA, color = "#00BFC4",
    size = 4, stroke = 1.2
  ) +
  labs(x = "Probability", y = "Delta Beta") +
  my_theme

# combine to grid
(p1 | p2) /
(p3 | plot_spacer())
```


```{r fig7.12, echo=FALSE, fig.width=6, fig.height=4, message=FALSE, fig.cap="Figure 7.12 Scatterplots of delta chi-square and delta beta values versus leverage from the space shuttle data. The extreme value along the y-axis represents the launches at 75°F. There do not appear to be any extreme leverage values."}
library(ggplot2)
library(patchwork)
# fit regression
df <- read.csv("Shuttle.csv", stringsAsFactors = FALSE)
mod <- glm(Success ~ Temperature, data = df, family = binomial)
df$hat           <- hatvalues(mod)
df$delta_chisq   <- residuals(mod, type = "deviance")^2
df$delta_beta    <- abs(dfbeta(mod)[, "Temperature"])

# theme
my_theme <- theme_minimal(base_family = "serif") +
  theme(
    panel.border     = element_rect(fill = NA,       colour = "black", linewidth = 1),
    panel.background = element_blank(),
    plot.background  = element_blank(),
    panel.grid.major = element_line(color = "grey80"),
    panel.grid.minor = element_blank(),
    axis.title       = element_text(face = "bold"),
    plot.margin      = margin(10, 10, 10, 10)
  )

# plot 1: Delta Chi‑Square vs Leverage
p1 <- ggplot(df, aes(x = hat, y = delta_chisq)) +
  geom_point(size = 2) +
  labs(x = "Leverage", y = "Delta Chi‑Square") +
  my_theme

# plot 2: Delta Beta vs Leverage
p2 <- ggplot(df, aes(x = hat, y = delta_beta)) +
  geom_point(size = 2) +
  labs(x = "Leverage", y = "Delta Beta") +
  my_theme

# combine to grid
p1 | p2

```
